
=== Training started at 2025-12-10 03:18:59 ===
Step 10: {'loss': 4.3209, 'grad_norm': 8.168756484985352, 'learning_rate': 4.986666666666667e-05, 'epoch': 0.008888888888888889}
Step 20: {'loss': 3.5372, 'grad_norm': 7.066354751586914, 'learning_rate': 4.971851851851852e-05, 'epoch': 0.017777777777777778}
Step 30: {'loss': 3.0172, 'grad_norm': 7.59269380569458, 'learning_rate': 4.9570370370370374e-05, 'epoch': 0.02666666666666667}
Step 40: {'loss': 2.6486, 'grad_norm': 8.644094467163086, 'learning_rate': 4.942222222222223e-05, 'epoch': 0.035555555555555556}
Step 50: {'loss': 2.3366, 'grad_norm': 6.687088489532471, 'learning_rate': 4.927407407407408e-05, 'epoch': 0.044444444444444446}
Step 60: {'loss': 2.1551, 'grad_norm': 7.500741481781006, 'learning_rate': 4.912592592592593e-05, 'epoch': 0.05333333333333334}
Step 70: {'loss': 1.9127, 'grad_norm': 6.007360935211182, 'learning_rate': 4.897777777777778e-05, 'epoch': 0.06222222222222222}
Step 80: {'loss': 1.9569, 'grad_norm': 6.606240272521973, 'learning_rate': 4.882962962962963e-05, 'epoch': 0.07111111111111111}
Step 90: {'loss': 1.7164, 'grad_norm': 5.983922958374023, 'learning_rate': 4.8681481481481485e-05, 'epoch': 0.08}
Step 100: {'loss': 1.6873, 'grad_norm': 6.693104267120361, 'learning_rate': 4.853333333333334e-05, 'epoch': 0.08888888888888889}
Step 110: {'loss': 1.7177, 'grad_norm': 5.794921875, 'learning_rate': 4.838518518518519e-05, 'epoch': 0.09777777777777778}
Step 120: {'loss': 1.6681, 'grad_norm': 5.548788070678711, 'learning_rate': 4.8237037037037045e-05, 'epoch': 0.10666666666666667}
Step 130: {'loss': 1.5335, 'grad_norm': 4.899167537689209, 'learning_rate': 4.808888888888889e-05, 'epoch': 0.11555555555555555}
Step 140: {'loss': 1.6776, 'grad_norm': 5.207950115203857, 'learning_rate': 4.7940740740740744e-05, 'epoch': 0.12444444444444444}
Step 150: {'loss': 1.53, 'grad_norm': 5.745246410369873, 'learning_rate': 4.77925925925926e-05, 'epoch': 0.13333333333333333}
Step 160: {'loss': 1.5858, 'grad_norm': 4.961906433105469, 'learning_rate': 4.764444444444445e-05, 'epoch': 0.14222222222222222}
Step 170: {'loss': 1.6522, 'grad_norm': 5.421715259552002, 'learning_rate': 4.74962962962963e-05, 'epoch': 0.1511111111111111}
Step 180: {'loss': 1.5799, 'grad_norm': 6.1207990646362305, 'learning_rate': 4.7348148148148156e-05, 'epoch': 0.16}
Step 190: {'loss': 1.5833, 'grad_norm': 4.971269607543945, 'learning_rate': 4.72e-05, 'epoch': 0.1688888888888889}
Step 200: {'loss': 1.5617, 'grad_norm': 4.7288618087768555, 'learning_rate': 4.7051851851851855e-05, 'epoch': 0.17777777777777778}
Step 210: {'loss': 1.4843, 'grad_norm': 4.478967666625977, 'learning_rate': 4.690370370370371e-05, 'epoch': 0.18666666666666668}
Step 220: {'loss': 1.4393, 'grad_norm': 4.959149360656738, 'learning_rate': 4.675555555555556e-05, 'epoch': 0.19555555555555557}
Step 230: {'loss': 1.4342, 'grad_norm': 3.8592092990875244, 'learning_rate': 4.660740740740741e-05, 'epoch': 0.20444444444444446}
Step 240: {'loss': 1.3895, 'grad_norm': 4.141768932342529, 'learning_rate': 4.645925925925926e-05, 'epoch': 0.21333333333333335}
Step 250: {'loss': 1.4519, 'grad_norm': 4.3325934410095215, 'learning_rate': 4.6311111111111113e-05, 'epoch': 0.2222222222222222}
Step 260: {'loss': 1.5375, 'grad_norm': 3.5599629878997803, 'learning_rate': 4.6162962962962966e-05, 'epoch': 0.2311111111111111}
Step 270: {'loss': 1.4129, 'grad_norm': 4.952556610107422, 'learning_rate': 4.601481481481482e-05, 'epoch': 0.24}
Step 280: {'loss': 1.5278, 'grad_norm': 3.9160537719726562, 'learning_rate': 4.5866666666666666e-05, 'epoch': 0.24888888888888888}
Step 290: {'loss': 1.5079, 'grad_norm': 3.569981813430786, 'learning_rate': 4.571851851851852e-05, 'epoch': 0.2577777777777778}
Step 300: {'loss': 1.5598, 'grad_norm': 3.702634572982788, 'learning_rate': 4.557037037037037e-05, 'epoch': 0.26666666666666666}
Step 310: {'loss': 1.4944, 'grad_norm': 3.5649850368499756, 'learning_rate': 4.5422222222222225e-05, 'epoch': 0.27555555555555555}
Step 320: {'loss': 1.4497, 'grad_norm': 3.7301387786865234, 'learning_rate': 4.527407407407407e-05, 'epoch': 0.28444444444444444}
Step 330: {'loss': 1.4811, 'grad_norm': 3.8482089042663574, 'learning_rate': 4.5125925925925924e-05, 'epoch': 0.29333333333333333}
Step 340: {'loss': 1.445, 'grad_norm': 5.010837078094482, 'learning_rate': 4.497777777777778e-05, 'epoch': 0.3022222222222222}
Step 350: {'loss': 1.4821, 'grad_norm': 3.2066688537597656, 'learning_rate': 4.482962962962963e-05, 'epoch': 0.3111111111111111}
Step 360: {'loss': 1.4526, 'grad_norm': 3.3071720600128174, 'learning_rate': 4.468148148148148e-05, 'epoch': 0.32}
Step 370: {'loss': 1.4091, 'grad_norm': 3.80605149269104, 'learning_rate': 4.4533333333333336e-05, 'epoch': 0.3288888888888889}
Step 380: {'loss': 1.4419, 'grad_norm': 3.063589334487915, 'learning_rate': 4.438518518518518e-05, 'epoch': 0.3377777777777778}
Step 390: {'loss': 1.3616, 'grad_norm': 3.2311553955078125, 'learning_rate': 4.4237037037037035e-05, 'epoch': 0.3466666666666667}
Step 400: {'loss': 1.3924, 'grad_norm': 3.8137645721435547, 'learning_rate': 4.408888888888889e-05, 'epoch': 0.35555555555555557}
Step 410: {'loss': 1.4862, 'grad_norm': 3.191770076751709, 'learning_rate': 4.394074074074074e-05, 'epoch': 0.36444444444444446}
Step 420: {'loss': 1.5395, 'grad_norm': 2.816014528274536, 'learning_rate': 4.3792592592592594e-05, 'epoch': 0.37333333333333335}
Step 430: {'loss': 1.4941, 'grad_norm': 2.7329041957855225, 'learning_rate': 4.364444444444445e-05, 'epoch': 0.38222222222222224}
Step 440: {'loss': 1.5163, 'grad_norm': 3.1436350345611572, 'learning_rate': 4.3496296296296294e-05, 'epoch': 0.39111111111111113}
Step 450: {'loss': 1.4335, 'grad_norm': 3.1150529384613037, 'learning_rate': 4.334814814814815e-05, 'epoch': 0.4}
Step 460: {'loss': 1.2619, 'grad_norm': 3.371835947036743, 'learning_rate': 4.32e-05, 'epoch': 0.4088888888888889}
Step 470: {'loss': 1.4274, 'grad_norm': 2.6231119632720947, 'learning_rate': 4.305185185185185e-05, 'epoch': 0.4177777777777778}
Step 480: {'loss': 1.3658, 'grad_norm': 3.6087799072265625, 'learning_rate': 4.2903703703703706e-05, 'epoch': 0.4266666666666667}
Step 490: {'loss': 1.3926, 'grad_norm': 3.06591534614563, 'learning_rate': 4.275555555555556e-05, 'epoch': 0.43555555555555553}
Step 500: {'loss': 1.3455, 'grad_norm': 3.1010172367095947, 'learning_rate': 4.2607407407407405e-05, 'epoch': 0.4444444444444444}
Step 510: {'loss': 1.3679, 'grad_norm': 2.7748193740844727, 'learning_rate': 4.245925925925926e-05, 'epoch': 0.4533333333333333}
Step 520: {'loss': 1.4835, 'grad_norm': 2.3336479663848877, 'learning_rate': 4.231111111111111e-05, 'epoch': 0.4622222222222222}
Step 530: {'loss': 1.3694, 'grad_norm': 2.999316930770874, 'learning_rate': 4.2162962962962964e-05, 'epoch': 0.4711111111111111}
Step 540: {'loss': 1.4026, 'grad_norm': 3.0633656978607178, 'learning_rate': 4.201481481481482e-05, 'epoch': 0.48}
Step 550: {'loss': 1.2966, 'grad_norm': 2.502673625946045, 'learning_rate': 4.186666666666667e-05, 'epoch': 0.4888888888888889}
Step 560: {'loss': 1.3753, 'grad_norm': 2.6080167293548584, 'learning_rate': 4.1718518518518516e-05, 'epoch': 0.49777777777777776}
Step 570: {'loss': 1.3327, 'grad_norm': 3.0802297592163086, 'learning_rate': 4.157037037037037e-05, 'epoch': 0.5066666666666667}
Step 580: {'loss': 1.3601, 'grad_norm': 2.4749507904052734, 'learning_rate': 4.142222222222222e-05, 'epoch': 0.5155555555555555}
Step 590: {'loss': 1.3639, 'grad_norm': 2.7114977836608887, 'learning_rate': 4.1274074074074075e-05, 'epoch': 0.5244444444444445}
Step 600: {'loss': 1.4293, 'grad_norm': 2.763674259185791, 'learning_rate': 4.112592592592593e-05, 'epoch': 0.5333333333333333}
Step 610: {'loss': 1.372, 'grad_norm': 2.739217758178711, 'learning_rate': 4.097777777777778e-05, 'epoch': 0.5422222222222223}
Step 620: {'loss': 1.4083, 'grad_norm': 2.7012939453125, 'learning_rate': 4.0829629629629634e-05, 'epoch': 0.5511111111111111}
Step 630: {'loss': 1.3377, 'grad_norm': 2.6133060455322266, 'learning_rate': 4.068148148148148e-05, 'epoch': 0.56}
Step 640: {'loss': 1.394, 'grad_norm': 2.792140007019043, 'learning_rate': 4.0533333333333334e-05, 'epoch': 0.5688888888888889}
Step 650: {'loss': 1.3192, 'grad_norm': 2.621814250946045, 'learning_rate': 4.038518518518519e-05, 'epoch': 0.5777777777777777}
Step 660: {'loss': 1.4293, 'grad_norm': 3.50982928276062, 'learning_rate': 4.023703703703704e-05, 'epoch': 0.5866666666666667}
Step 670: {'loss': 1.4949, 'grad_norm': 2.6948082447052, 'learning_rate': 4.008888888888889e-05, 'epoch': 0.5955555555555555}
Step 680: {'loss': 1.3659, 'grad_norm': 2.3496804237365723, 'learning_rate': 3.9940740740740746e-05, 'epoch': 0.6044444444444445}
Step 690: {'loss': 1.2604, 'grad_norm': 2.757422685623169, 'learning_rate': 3.979259259259259e-05, 'epoch': 0.6133333333333333}
Step 700: {'loss': 1.2523, 'grad_norm': 3.292153835296631, 'learning_rate': 3.9644444444444445e-05, 'epoch': 0.6222222222222222}
Step 710: {'loss': 1.3577, 'grad_norm': 2.9097468852996826, 'learning_rate': 3.94962962962963e-05, 'epoch': 0.6311111111111111}
Step 720: {'loss': 1.3946, 'grad_norm': 2.6997969150543213, 'learning_rate': 3.934814814814815e-05, 'epoch': 0.64}
Step 730: {'loss': 1.4034, 'grad_norm': 3.0118463039398193, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.6488888888888888}
Step 740: {'loss': 1.4508, 'grad_norm': 3.053490400314331, 'learning_rate': 3.905185185185186e-05, 'epoch': 0.6577777777777778}
Step 750: {'loss': 1.2795, 'grad_norm': 3.6816012859344482, 'learning_rate': 3.8903703703703703e-05, 'epoch': 0.6666666666666666}
Step 760: {'loss': 1.2385, 'grad_norm': 2.621530294418335, 'learning_rate': 3.8755555555555556e-05, 'epoch': 0.6755555555555556}
Step 770: {'loss': 1.4504, 'grad_norm': 3.11110520362854, 'learning_rate': 3.860740740740741e-05, 'epoch': 0.6844444444444444}
Step 780: {'loss': 1.3637, 'grad_norm': 2.4073312282562256, 'learning_rate': 3.845925925925926e-05, 'epoch': 0.6933333333333334}
Step 790: {'loss': 1.4171, 'grad_norm': 2.4151322841644287, 'learning_rate': 3.8311111111111115e-05, 'epoch': 0.7022222222222222}
Step 800: {'loss': 1.3082, 'grad_norm': 2.9977030754089355, 'learning_rate': 3.816296296296297e-05, 'epoch': 0.7111111111111111}
Step 810: {'loss': 1.3237, 'grad_norm': 2.790888786315918, 'learning_rate': 3.8014814814814815e-05, 'epoch': 0.72}
Step 820: {'loss': 1.3209, 'grad_norm': 2.6245133876800537, 'learning_rate': 3.786666666666667e-05, 'epoch': 0.7288888888888889}
Step 830: {'loss': 1.3683, 'grad_norm': 2.2177813053131104, 'learning_rate': 3.771851851851852e-05, 'epoch': 0.7377777777777778}
Step 840: {'loss': 1.4662, 'grad_norm': 2.7940011024475098, 'learning_rate': 3.7570370370370374e-05, 'epoch': 0.7466666666666667}
Step 850: {'loss': 1.3635, 'grad_norm': 2.70283842086792, 'learning_rate': 3.742222222222223e-05, 'epoch': 0.7555555555555555}
Step 860: {'loss': 1.3409, 'grad_norm': 2.617501735687256, 'learning_rate': 3.727407407407408e-05, 'epoch': 0.7644444444444445}
Step 870: {'loss': 1.3927, 'grad_norm': 2.620600700378418, 'learning_rate': 3.7125925925925926e-05, 'epoch': 0.7733333333333333}
Step 880: {'loss': 1.4435, 'grad_norm': 2.471173048019409, 'learning_rate': 3.697777777777778e-05, 'epoch': 0.7822222222222223}
Step 890: {'loss': 1.3455, 'grad_norm': 2.620372772216797, 'learning_rate': 3.682962962962963e-05, 'epoch': 0.7911111111111111}
Step 900: {'loss': 1.3329, 'grad_norm': 2.1635990142822266, 'learning_rate': 3.6681481481481485e-05, 'epoch': 0.8}
Step 910: {'loss': 1.3024, 'grad_norm': 2.297250270843506, 'learning_rate': 3.653333333333334e-05, 'epoch': 0.8088888888888889}
Step 920: {'loss': 1.4244, 'grad_norm': 2.313279628753662, 'learning_rate': 3.638518518518519e-05, 'epoch': 0.8177777777777778}
Step 930: {'loss': 1.3763, 'grad_norm': 2.536586046218872, 'learning_rate': 3.623703703703704e-05, 'epoch': 0.8266666666666667}
Step 940: {'loss': 1.2543, 'grad_norm': 2.8560328483581543, 'learning_rate': 3.608888888888889e-05, 'epoch': 0.8355555555555556}
Step 950: {'loss': 1.3123, 'grad_norm': 3.1657888889312744, 'learning_rate': 3.5940740740740743e-05, 'epoch': 0.8444444444444444}
Step 960: {'loss': 1.2696, 'grad_norm': 2.0820748805999756, 'learning_rate': 3.5792592592592596e-05, 'epoch': 0.8533333333333334}
Step 970: {'loss': 1.3352, 'grad_norm': 2.6209301948547363, 'learning_rate': 3.564444444444445e-05, 'epoch': 0.8622222222222222}
Step 980: {'loss': 1.3563, 'grad_norm': 2.7952208518981934, 'learning_rate': 3.54962962962963e-05, 'epoch': 0.8711111111111111}
Step 990: {'loss': 1.4063, 'grad_norm': 2.931384801864624, 'learning_rate': 3.5348148148148156e-05, 'epoch': 0.88}
Step 1000: {'loss': 1.3457, 'grad_norm': 2.3480191230773926, 'learning_rate': 3.52e-05, 'epoch': 0.8888888888888888}
Step 1010: {'loss': 1.2906, 'grad_norm': 2.1978840827941895, 'learning_rate': 3.5051851851851855e-05, 'epoch': 0.8977777777777778}
Step 1020: {'loss': 1.3017, 'grad_norm': 2.34641432762146, 'learning_rate': 3.490370370370371e-05, 'epoch': 0.9066666666666666}
Step 1030: {'loss': 1.2974, 'grad_norm': 2.5470001697540283, 'learning_rate': 3.475555555555556e-05, 'epoch': 0.9155555555555556}
Step 1040: {'loss': 1.3223, 'grad_norm': 2.7510769367218018, 'learning_rate': 3.4607407407407414e-05, 'epoch': 0.9244444444444444}
Step 1050: {'loss': 1.3181, 'grad_norm': 3.037320852279663, 'learning_rate': 3.445925925925926e-05, 'epoch': 0.9333333333333333}
Step 1060: {'loss': 1.363, 'grad_norm': 2.4247469902038574, 'learning_rate': 3.431111111111111e-05, 'epoch': 0.9422222222222222}
Step 1070: {'loss': 1.4306, 'grad_norm': 2.7107133865356445, 'learning_rate': 3.4162962962962966e-05, 'epoch': 0.9511111111111111}
Step 1080: {'loss': 1.2503, 'grad_norm': 2.5284194946289062, 'learning_rate': 3.401481481481482e-05, 'epoch': 0.96}
Step 1090: {'loss': 1.3085, 'grad_norm': 2.6469202041625977, 'learning_rate': 3.3866666666666665e-05, 'epoch': 0.9688888888888889}
Step 1100: {'loss': 1.3659, 'grad_norm': 3.413059711456299, 'learning_rate': 3.371851851851852e-05, 'epoch': 0.9777777777777777}
Step 1110: {'loss': 1.2255, 'grad_norm': 2.6923835277557373, 'learning_rate': 3.357037037037037e-05, 'epoch': 0.9866666666666667}
Step 1120: {'loss': 1.4319, 'grad_norm': 2.5287675857543945, 'learning_rate': 3.3422222222222224e-05, 'epoch': 0.9955555555555555}
Epoch 1.0 completed in 773.90s
Step 1125: {'eval_loss': 1.277658462524414, 'eval_runtime': 21.9804, 'eval_samples_per_second': 22.747, 'eval_steps_per_second': 5.687, 'epoch': 1.0}
Step 1130: {'loss': 1.3248, 'grad_norm': 2.2680342197418213, 'learning_rate': 3.327407407407408e-05, 'epoch': 1.0044444444444445}
Step 1140: {'loss': 1.1553, 'grad_norm': 2.697929859161377, 'learning_rate': 3.3125925925925924e-05, 'epoch': 1.0133333333333334}
Step 1150: {'loss': 1.2369, 'grad_norm': 2.4405674934387207, 'learning_rate': 3.297777777777778e-05, 'epoch': 1.0222222222222221}
Step 1160: {'loss': 1.2964, 'grad_norm': 3.0213801860809326, 'learning_rate': 3.282962962962963e-05, 'epoch': 1.031111111111111}
Step 1170: {'loss': 1.2817, 'grad_norm': 3.2092878818511963, 'learning_rate': 3.268148148148148e-05, 'epoch': 1.04}
Step 1180: {'loss': 1.2474, 'grad_norm': 2.4614365100860596, 'learning_rate': 3.253333333333333e-05, 'epoch': 1.048888888888889}
Step 1190: {'loss': 1.3845, 'grad_norm': 2.305387496948242, 'learning_rate': 3.238518518518518e-05, 'epoch': 1.0577777777777777}
Step 1200: {'loss': 1.2104, 'grad_norm': 3.079881429672241, 'learning_rate': 3.2237037037037035e-05, 'epoch': 1.0666666666666667}
Step 1210: {'loss': 1.2356, 'grad_norm': 3.936598300933838, 'learning_rate': 3.208888888888889e-05, 'epoch': 1.0755555555555556}
Step 1220: {'loss': 1.1377, 'grad_norm': 2.5700724124908447, 'learning_rate': 3.194074074074074e-05, 'epoch': 1.0844444444444445}
Step 1230: {'loss': 1.2137, 'grad_norm': 2.9981651306152344, 'learning_rate': 3.1792592592592594e-05, 'epoch': 1.0933333333333333}
Step 1240: {'loss': 1.1668, 'grad_norm': 2.698788642883301, 'learning_rate': 3.164444444444444e-05, 'epoch': 1.1022222222222222}
Step 1250: {'loss': 1.2636, 'grad_norm': 3.746009111404419, 'learning_rate': 3.1496296296296293e-05, 'epoch': 1.1111111111111112}
Step 1260: {'loss': 1.1907, 'grad_norm': 2.4505064487457275, 'learning_rate': 3.1348148148148146e-05, 'epoch': 1.12}
Step 1270: {'loss': 1.1621, 'grad_norm': 2.717649221420288, 'learning_rate': 3.12e-05, 'epoch': 1.1288888888888888}
Step 1280: {'loss': 1.2328, 'grad_norm': 2.4319214820861816, 'learning_rate': 3.105185185185185e-05, 'epoch': 1.1377777777777778}
Step 1290: {'loss': 1.1542, 'grad_norm': 2.6488635540008545, 'learning_rate': 3.0903703703703705e-05, 'epoch': 1.1466666666666667}
Step 1300: {'loss': 1.2031, 'grad_norm': 2.4564125537872314, 'learning_rate': 3.075555555555556e-05, 'epoch': 1.1555555555555554}
Step 1310: {'loss': 1.1369, 'grad_norm': 3.13739013671875, 'learning_rate': 3.0607407407407405e-05, 'epoch': 1.1644444444444444}
Step 1320: {'loss': 1.126, 'grad_norm': 2.6439595222473145, 'learning_rate': 3.045925925925926e-05, 'epoch': 1.1733333333333333}
Step 1330: {'loss': 1.3165, 'grad_norm': 2.1998138427734375, 'learning_rate': 3.031111111111111e-05, 'epoch': 1.1822222222222223}
Step 1340: {'loss': 1.2571, 'grad_norm': 2.3383500576019287, 'learning_rate': 3.0162962962962964e-05, 'epoch': 1.1911111111111112}
Step 1350: {'loss': 1.2877, 'grad_norm': 2.357140064239502, 'learning_rate': 3.0014814814814817e-05, 'epoch': 1.2}
Step 1360: {'loss': 1.3163, 'grad_norm': 2.9434006214141846, 'learning_rate': 2.986666666666667e-05, 'epoch': 1.208888888888889}
Step 1370: {'loss': 1.2155, 'grad_norm': 2.2200331687927246, 'learning_rate': 2.9718518518518516e-05, 'epoch': 1.2177777777777778}
Step 1380: {'loss': 1.3488, 'grad_norm': 2.6576039791107178, 'learning_rate': 2.957037037037037e-05, 'epoch': 1.2266666666666666}
Step 1390: {'loss': 1.2606, 'grad_norm': 2.423510789871216, 'learning_rate': 2.9422222222222222e-05, 'epoch': 1.2355555555555555}
Step 1400: {'loss': 1.2144, 'grad_norm': 2.7012431621551514, 'learning_rate': 2.9274074074074075e-05, 'epoch': 1.2444444444444445}
Step 1410: {'loss': 1.2154, 'grad_norm': 2.3655149936676025, 'learning_rate': 2.9125925925925928e-05, 'epoch': 1.2533333333333334}
Step 1420: {'loss': 1.1943, 'grad_norm': 2.4659626483917236, 'learning_rate': 2.897777777777778e-05, 'epoch': 1.2622222222222224}
Step 1430: {'loss': 1.3192, 'grad_norm': 2.385026216506958, 'learning_rate': 2.8829629629629627e-05, 'epoch': 1.271111111111111}
Step 1440: {'loss': 1.2432, 'grad_norm': 3.094872236251831, 'learning_rate': 2.868148148148148e-05, 'epoch': 1.28}
Step 1450: {'loss': 1.3181, 'grad_norm': 2.284024953842163, 'learning_rate': 2.8533333333333333e-05, 'epoch': 1.2888888888888888}
Step 1460: {'loss': 1.2859, 'grad_norm': 2.6766550540924072, 'learning_rate': 2.8385185185185186e-05, 'epoch': 1.2977777777777777}
Step 1470: {'loss': 1.2478, 'grad_norm': 4.969729900360107, 'learning_rate': 2.823703703703704e-05, 'epoch': 1.3066666666666666}
Step 1480: {'loss': 1.1569, 'grad_norm': 2.7310657501220703, 'learning_rate': 2.8088888888888893e-05, 'epoch': 1.3155555555555556}
Step 1490: {'loss': 1.2601, 'grad_norm': 2.3039934635162354, 'learning_rate': 2.794074074074074e-05, 'epoch': 1.3244444444444445}
Step 1500: {'loss': 1.1501, 'grad_norm': 2.6296379566192627, 'learning_rate': 2.7792592592592592e-05, 'epoch': 1.3333333333333333}
Step 1510: {'loss': 1.2093, 'grad_norm': 2.5525331497192383, 'learning_rate': 2.7644444444444445e-05, 'epoch': 1.3422222222222222}
Step 1520: {'loss': 1.2401, 'grad_norm': 2.5514273643493652, 'learning_rate': 2.7496296296296298e-05, 'epoch': 1.3511111111111112}
Step 1530: {'loss': 1.1944, 'grad_norm': 2.455003261566162, 'learning_rate': 2.734814814814815e-05, 'epoch': 1.3599999999999999}
Step 1540: {'loss': 1.2293, 'grad_norm': 3.070502519607544, 'learning_rate': 2.7200000000000004e-05, 'epoch': 1.3688888888888888}
Step 1550: {'loss': 1.2346, 'grad_norm': 2.5611610412597656, 'learning_rate': 2.705185185185185e-05, 'epoch': 1.3777777777777778}
Step 1560: {'loss': 1.1751, 'grad_norm': 11.295082092285156, 'learning_rate': 2.6903703703703703e-05, 'epoch': 1.3866666666666667}
Step 1570: {'loss': 1.1286, 'grad_norm': 2.6142282485961914, 'learning_rate': 2.6755555555555556e-05, 'epoch': 1.3955555555555557}
Step 1580: {'loss': 1.2787, 'grad_norm': 2.362347364425659, 'learning_rate': 2.660740740740741e-05, 'epoch': 1.4044444444444444}
Step 1590: {'loss': 1.4383, 'grad_norm': 2.8074533939361572, 'learning_rate': 2.6459259259259262e-05, 'epoch': 1.4133333333333333}
Step 1600: {'loss': 1.1316, 'grad_norm': 2.4446420669555664, 'learning_rate': 2.6311111111111115e-05, 'epoch': 1.4222222222222223}
Step 1610: {'loss': 1.1988, 'grad_norm': 2.469449520111084, 'learning_rate': 2.6162962962962968e-05, 'epoch': 1.431111111111111}
Step 1620: {'loss': 1.3335, 'grad_norm': 3.7136471271514893, 'learning_rate': 2.6014814814814814e-05, 'epoch': 1.44}
Step 1630: {'loss': 1.1501, 'grad_norm': 2.689944267272949, 'learning_rate': 2.5866666666666667e-05, 'epoch': 1.448888888888889}
Step 1640: {'loss': 1.2505, 'grad_norm': 2.5126092433929443, 'learning_rate': 2.571851851851852e-05, 'epoch': 1.4577777777777778}
Step 1650: {'loss': 1.2119, 'grad_norm': 2.3298280239105225, 'learning_rate': 2.5570370370370374e-05, 'epoch': 1.4666666666666668}
Step 1660: {'loss': 1.2192, 'grad_norm': 2.2360336780548096, 'learning_rate': 2.5422222222222227e-05, 'epoch': 1.4755555555555555}
Step 1670: {'loss': 1.2598, 'grad_norm': 2.1827712059020996, 'learning_rate': 2.5274074074074076e-05, 'epoch': 1.4844444444444445}
Step 1680: {'loss': 1.2459, 'grad_norm': 2.386781692504883, 'learning_rate': 2.5125925925925926e-05, 'epoch': 1.4933333333333334}
Step 1690: {'loss': 1.3031, 'grad_norm': 2.776189088821411, 'learning_rate': 2.497777777777778e-05, 'epoch': 1.5022222222222221}
Step 1700: {'loss': 1.2055, 'grad_norm': 2.422116279602051, 'learning_rate': 2.4829629629629632e-05, 'epoch': 1.511111111111111}
Step 1710: {'loss': 1.199, 'grad_norm': 2.4142465591430664, 'learning_rate': 2.4681481481481485e-05, 'epoch': 1.52}
Step 1720: {'loss': 1.3179, 'grad_norm': 2.692185878753662, 'learning_rate': 2.4533333333333334e-05, 'epoch': 1.528888888888889}
Step 1730: {'loss': 1.215, 'grad_norm': 2.446969747543335, 'learning_rate': 2.4385185185185188e-05, 'epoch': 1.537777777777778}
Step 1740: {'loss': 1.3378, 'grad_norm': 2.3670246601104736, 'learning_rate': 2.4237037037037037e-05, 'epoch': 1.5466666666666666}
Step 1750: {'loss': 1.1618, 'grad_norm': 2.9113869667053223, 'learning_rate': 2.408888888888889e-05, 'epoch': 1.5555555555555556}
Step 1760: {'loss': 1.2382, 'grad_norm': 2.2854294776916504, 'learning_rate': 2.394074074074074e-05, 'epoch': 1.5644444444444443}
Step 1770: {'loss': 1.3173, 'grad_norm': 2.893031358718872, 'learning_rate': 2.3792592592592593e-05, 'epoch': 1.5733333333333333}
Step 1780: {'loss': 1.1847, 'grad_norm': 2.315094232559204, 'learning_rate': 2.3644444444444446e-05, 'epoch': 1.5822222222222222}
Step 1790: {'loss': 1.2355, 'grad_norm': 2.7543976306915283, 'learning_rate': 2.3496296296296295e-05, 'epoch': 1.5911111111111111}
Step 1800: {'loss': 1.2477, 'grad_norm': 2.1989657878875732, 'learning_rate': 2.334814814814815e-05, 'epoch': 1.6}
Step 1810: {'loss': 1.2181, 'grad_norm': 2.2861032485961914, 'learning_rate': 2.32e-05, 'epoch': 1.608888888888889}
Step 1820: {'loss': 1.259, 'grad_norm': 2.550361156463623, 'learning_rate': 2.305185185185185e-05, 'epoch': 1.6177777777777778}
Step 1830: {'loss': 1.1577, 'grad_norm': 2.129443645477295, 'learning_rate': 2.2903703703703704e-05, 'epoch': 1.6266666666666667}
Step 1840: {'loss': 1.1349, 'grad_norm': 2.64703631401062, 'learning_rate': 2.2755555555555557e-05, 'epoch': 1.6355555555555554}
Step 1850: {'loss': 1.275, 'grad_norm': 3.0710551738739014, 'learning_rate': 2.2607407407407407e-05, 'epoch': 1.6444444444444444}
Step 1860: {'loss': 1.2529, 'grad_norm': 2.567139148712158, 'learning_rate': 2.245925925925926e-05, 'epoch': 1.6533333333333333}
Step 1870: {'loss': 1.2729, 'grad_norm': 5.148467540740967, 'learning_rate': 2.2311111111111113e-05, 'epoch': 1.6622222222222223}
Step 1880: {'loss': 1.4217, 'grad_norm': 2.5336081981658936, 'learning_rate': 2.2162962962962962e-05, 'epoch': 1.6711111111111112}
Step 1890: {'loss': 1.2638, 'grad_norm': 2.448253870010376, 'learning_rate': 2.2014814814814815e-05, 'epoch': 1.6800000000000002}
Step 1900: {'loss': 1.2736, 'grad_norm': 2.021275281906128, 'learning_rate': 2.186666666666667e-05, 'epoch': 1.6888888888888889}
Step 1910: {'loss': 1.1632, 'grad_norm': 2.3740909099578857, 'learning_rate': 2.1718518518518518e-05, 'epoch': 1.6977777777777778}
Step 1920: {'loss': 1.1913, 'grad_norm': 2.4286704063415527, 'learning_rate': 2.157037037037037e-05, 'epoch': 1.7066666666666666}
Step 1930: {'loss': 1.2102, 'grad_norm': 2.7633261680603027, 'learning_rate': 2.1422222222222224e-05, 'epoch': 1.7155555555555555}
Step 1940: {'loss': 1.1479, 'grad_norm': 2.390687942504883, 'learning_rate': 2.1274074074074074e-05, 'epoch': 1.7244444444444444}
Step 1950: {'loss': 1.2592, 'grad_norm': 2.855391502380371, 'learning_rate': 2.1125925925925927e-05, 'epoch': 1.7333333333333334}
Step 1960: {'loss': 1.306, 'grad_norm': 2.4891271591186523, 'learning_rate': 2.097777777777778e-05, 'epoch': 1.7422222222222223}
Step 1970: {'loss': 1.1467, 'grad_norm': 3.217768907546997, 'learning_rate': 2.082962962962963e-05, 'epoch': 1.751111111111111}
Step 1980: {'loss': 1.1881, 'grad_norm': 2.5176966190338135, 'learning_rate': 2.0681481481481482e-05, 'epoch': 1.76}
Step 1990: {'loss': 1.1841, 'grad_norm': 2.4476897716522217, 'learning_rate': 2.0533333333333336e-05, 'epoch': 1.7688888888888887}
Step 2000: {'loss': 1.3485, 'grad_norm': 2.3463776111602783, 'learning_rate': 2.0385185185185185e-05, 'epoch': 1.7777777777777777}
Step 2010: {'loss': 1.2483, 'grad_norm': 1.9372631311416626, 'learning_rate': 2.0237037037037038e-05, 'epoch': 1.7866666666666666}
Step 2020: {'loss': 1.2284, 'grad_norm': 2.6406304836273193, 'learning_rate': 2.008888888888889e-05, 'epoch': 1.7955555555555556}
Step 2030: {'loss': 1.247, 'grad_norm': 2.7250189781188965, 'learning_rate': 1.994074074074074e-05, 'epoch': 1.8044444444444445}
Step 2040: {'loss': 1.1943, 'grad_norm': 2.6415669918060303, 'learning_rate': 1.9792592592592594e-05, 'epoch': 1.8133333333333335}
Step 2050: {'loss': 1.2599, 'grad_norm': 2.607361316680908, 'learning_rate': 1.9644444444444447e-05, 'epoch': 1.8222222222222222}
Step 2060: {'loss': 1.2036, 'grad_norm': 2.3909571170806885, 'learning_rate': 1.94962962962963e-05, 'epoch': 1.8311111111111111}
Step 2070: {'loss': 1.1702, 'grad_norm': 2.3150174617767334, 'learning_rate': 1.934814814814815e-05, 'epoch': 1.8399999999999999}
Step 2080: {'loss': 1.1665, 'grad_norm': 3.2357614040374756, 'learning_rate': 1.9200000000000003e-05, 'epoch': 1.8488888888888888}
Step 2090: {'loss': 1.1504, 'grad_norm': 2.3921213150024414, 'learning_rate': 1.9051851851851856e-05, 'epoch': 1.8577777777777778}
Step 2100: {'loss': 1.1508, 'grad_norm': 2.919834613800049, 'learning_rate': 1.8903703703703705e-05, 'epoch': 1.8666666666666667}
Step 2110: {'loss': 1.155, 'grad_norm': 2.6699624061584473, 'learning_rate': 1.8755555555555558e-05, 'epoch': 1.8755555555555556}
Step 2120: {'loss': 1.2218, 'grad_norm': 3.469477415084839, 'learning_rate': 1.8607407407407408e-05, 'epoch': 1.8844444444444446}
Step 2130: {'loss': 1.1737, 'grad_norm': 2.64369797706604, 'learning_rate': 1.845925925925926e-05, 'epoch': 1.8933333333333333}
Step 2140: {'loss': 1.2283, 'grad_norm': 2.6894214153289795, 'learning_rate': 1.8311111111111114e-05, 'epoch': 1.9022222222222223}
Step 2150: {'loss': 1.2096, 'grad_norm': 2.9467315673828125, 'learning_rate': 1.8162962962962963e-05, 'epoch': 1.911111111111111}
Step 2160: {'loss': 1.2381, 'grad_norm': 3.507237672805786, 'learning_rate': 1.8014814814814817e-05, 'epoch': 1.92}
Step 2170: {'loss': 1.292, 'grad_norm': 2.5177576541900635, 'learning_rate': 1.7866666666666666e-05, 'epoch': 1.9288888888888889}
Step 2180: {'loss': 1.208, 'grad_norm': 2.634122133255005, 'learning_rate': 1.771851851851852e-05, 'epoch': 1.9377777777777778}
Step 2190: {'loss': 1.195, 'grad_norm': 2.3821122646331787, 'learning_rate': 1.757037037037037e-05, 'epoch': 1.9466666666666668}
Step 2200: {'loss': 1.0598, 'grad_norm': 2.8153910636901855, 'learning_rate': 1.7422222222222222e-05, 'epoch': 1.9555555555555557}
Step 2210: {'loss': 1.2908, 'grad_norm': 2.886732816696167, 'learning_rate': 1.7274074074074075e-05, 'epoch': 1.9644444444444444}
Step 2220: {'loss': 1.2982, 'grad_norm': 2.8466663360595703, 'learning_rate': 1.7125925925925924e-05, 'epoch': 1.9733333333333334}
Step 2230: {'loss': 1.2755, 'grad_norm': 2.7448856830596924, 'learning_rate': 1.6977777777777777e-05, 'epoch': 1.982222222222222}
Step 2240: {'loss': 1.1809, 'grad_norm': 2.550189256668091, 'learning_rate': 1.682962962962963e-05, 'epoch': 1.991111111111111}
Step 2250: {'loss': 1.1748, 'grad_norm': 2.6081349849700928, 'learning_rate': 1.668148148148148e-05, 'epoch': 2.0}
Epoch 2.0 completed in 1567.73s
Step 2250: {'eval_loss': 1.2407914400100708, 'eval_runtime': 21.9473, 'eval_samples_per_second': 22.782, 'eval_steps_per_second': 5.695, 'epoch': 2.0}
Step 2260: {'loss': 1.1963, 'grad_norm': 2.3769021034240723, 'learning_rate': 1.6533333333333333e-05, 'epoch': 2.008888888888889}
Step 2270: {'loss': 1.2214, 'grad_norm': 2.608886241912842, 'learning_rate': 1.6385185185185186e-05, 'epoch': 2.017777777777778}
Step 2280: {'loss': 1.2644, 'grad_norm': 2.470837354660034, 'learning_rate': 1.6237037037037036e-05, 'epoch': 2.026666666666667}
Step 2290: {'loss': 1.2118, 'grad_norm': 2.368497371673584, 'learning_rate': 1.608888888888889e-05, 'epoch': 2.0355555555555553}
Step 2300: {'loss': 1.1177, 'grad_norm': 2.8253073692321777, 'learning_rate': 1.5940740740740742e-05, 'epoch': 2.0444444444444443}
Step 2310: {'loss': 1.2186, 'grad_norm': 2.6171085834503174, 'learning_rate': 1.579259259259259e-05, 'epoch': 2.0533333333333332}
Step 2320: {'loss': 1.1817, 'grad_norm': 2.9857473373413086, 'learning_rate': 1.5644444444444444e-05, 'epoch': 2.062222222222222}
Step 2330: {'loss': 1.1973, 'grad_norm': 2.559441089630127, 'learning_rate': 1.5496296296296298e-05, 'epoch': 2.071111111111111}
Step 2340: {'loss': 1.2669, 'grad_norm': 2.6700973510742188, 'learning_rate': 1.5348148148148147e-05, 'epoch': 2.08}
Step 2350: {'loss': 1.355, 'grad_norm': 2.6339876651763916, 'learning_rate': 1.52e-05, 'epoch': 2.088888888888889}
Step 2360: {'loss': 1.1195, 'grad_norm': 2.3254876136779785, 'learning_rate': 1.5051851851851853e-05, 'epoch': 2.097777777777778}
Step 2370: {'loss': 1.1107, 'grad_norm': 2.583319902420044, 'learning_rate': 1.4903703703703703e-05, 'epoch': 2.1066666666666665}
Step 2380: {'loss': 1.1367, 'grad_norm': 3.0792298316955566, 'learning_rate': 1.4755555555555556e-05, 'epoch': 2.1155555555555554}
Step 2390: {'loss': 1.127, 'grad_norm': 2.7151358127593994, 'learning_rate': 1.4607407407407409e-05, 'epoch': 2.1244444444444444}
Step 2400: {'loss': 1.2019, 'grad_norm': 2.4461452960968018, 'learning_rate': 1.4459259259259262e-05, 'epoch': 2.1333333333333333}
Step 2410: {'loss': 1.132, 'grad_norm': 1.9653291702270508, 'learning_rate': 1.4311111111111111e-05, 'epoch': 2.1422222222222222}
Step 2420: {'loss': 1.227, 'grad_norm': 2.436861515045166, 'learning_rate': 1.4162962962962965e-05, 'epoch': 2.151111111111111}
Step 2430: {'loss': 1.1855, 'grad_norm': 2.0699453353881836, 'learning_rate': 1.4014814814814818e-05, 'epoch': 2.16}
Step 2440: {'loss': 1.1191, 'grad_norm': 2.9794232845306396, 'learning_rate': 1.3866666666666667e-05, 'epoch': 2.168888888888889}
Step 2450: {'loss': 1.236, 'grad_norm': 2.717014789581299, 'learning_rate': 1.371851851851852e-05, 'epoch': 2.1777777777777776}
Step 2460: {'loss': 1.1369, 'grad_norm': 2.446322202682495, 'learning_rate': 1.3570370370370372e-05, 'epoch': 2.1866666666666665}
Step 2470: {'loss': 1.1612, 'grad_norm': 2.5665063858032227, 'learning_rate': 1.3422222222222223e-05, 'epoch': 2.1955555555555555}
Step 2480: {'loss': 1.1426, 'grad_norm': 2.40069580078125, 'learning_rate': 1.3274074074074074e-05, 'epoch': 2.2044444444444444}
Step 2490: {'loss': 1.1782, 'grad_norm': 2.5349862575531006, 'learning_rate': 1.3125925925925927e-05, 'epoch': 2.2133333333333334}
Step 2500: {'loss': 1.2029, 'grad_norm': 2.5365407466888428, 'learning_rate': 1.2977777777777777e-05, 'epoch': 2.2222222222222223}
Step 2510: {'loss': 1.1518, 'grad_norm': 4.116152763366699, 'learning_rate': 1.282962962962963e-05, 'epoch': 2.2311111111111113}
Step 2520: {'loss': 1.2884, 'grad_norm': 2.8133931159973145, 'learning_rate': 1.2681481481481483e-05, 'epoch': 2.24}
Step 2530: {'loss': 1.2014, 'grad_norm': 2.327822208404541, 'learning_rate': 1.2533333333333332e-05, 'epoch': 2.2488888888888887}
Step 2540: {'loss': 1.2163, 'grad_norm': 2.307954788208008, 'learning_rate': 1.2385185185185186e-05, 'epoch': 2.2577777777777777}
Step 2550: {'loss': 1.2313, 'grad_norm': 2.633415460586548, 'learning_rate': 1.2237037037037037e-05, 'epoch': 2.2666666666666666}
Step 2560: {'loss': 1.1875, 'grad_norm': 2.4359912872314453, 'learning_rate': 1.208888888888889e-05, 'epoch': 2.2755555555555556}
Step 2570: {'loss': 1.1066, 'grad_norm': 2.4769747257232666, 'learning_rate': 1.1940740740740741e-05, 'epoch': 2.2844444444444445}
Step 2580: {'loss': 1.1514, 'grad_norm': 3.3723933696746826, 'learning_rate': 1.1792592592592592e-05, 'epoch': 2.2933333333333334}
Step 2590: {'loss': 1.0777, 'grad_norm': 2.644425392150879, 'learning_rate': 1.1644444444444446e-05, 'epoch': 2.3022222222222224}
Step 2600: {'loss': 1.1523, 'grad_norm': 2.3127715587615967, 'learning_rate': 1.1496296296296297e-05, 'epoch': 2.311111111111111}
Step 2610: {'loss': 1.1628, 'grad_norm': 2.983527421951294, 'learning_rate': 1.1348148148148148e-05, 'epoch': 2.32}
Step 2620: {'loss': 1.1644, 'grad_norm': 2.663302183151245, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.328888888888889}
Step 2630: {'loss': 1.2093, 'grad_norm': 2.6214637756347656, 'learning_rate': 1.1051851851851853e-05, 'epoch': 2.3377777777777777}
Step 2640: {'loss': 1.121, 'grad_norm': 2.530034303665161, 'learning_rate': 1.0903703703703706e-05, 'epoch': 2.3466666666666667}
Step 2650: {'loss': 1.147, 'grad_norm': 2.3552517890930176, 'learning_rate': 1.0755555555555557e-05, 'epoch': 2.3555555555555556}
Step 2660: {'loss': 1.107, 'grad_norm': 2.450638771057129, 'learning_rate': 1.0607407407407408e-05, 'epoch': 2.3644444444444446}
Step 2670: {'loss': 1.2685, 'grad_norm': 3.578855514526367, 'learning_rate': 1.045925925925926e-05, 'epoch': 2.3733333333333335}
Step 2680: {'loss': 1.1763, 'grad_norm': 2.42592716217041, 'learning_rate': 1.031111111111111e-05, 'epoch': 2.3822222222222225}
Step 2690: {'loss': 1.1586, 'grad_norm': 2.513144016265869, 'learning_rate': 1.0162962962962964e-05, 'epoch': 2.391111111111111}
Step 2700: {'loss': 1.2026, 'grad_norm': 3.2369167804718018, 'learning_rate': 1.0014814814814815e-05, 'epoch': 2.4}
Step 2710: {'loss': 1.0694, 'grad_norm': 2.3403844833374023, 'learning_rate': 9.866666666666667e-06, 'epoch': 2.408888888888889}
Step 2720: {'loss': 1.0758, 'grad_norm': 2.257206916809082, 'learning_rate': 9.718518518518518e-06, 'epoch': 2.417777777777778}
Step 2730: {'loss': 1.2051, 'grad_norm': 2.896235942840576, 'learning_rate': 9.570370370370371e-06, 'epoch': 2.4266666666666667}
Step 2740: {'loss': 1.2069, 'grad_norm': 2.819835901260376, 'learning_rate': 9.422222222222222e-06, 'epoch': 2.4355555555555557}
Step 2750: {'loss': 1.1141, 'grad_norm': 2.3414053916931152, 'learning_rate': 9.274074074074073e-06, 'epoch': 2.4444444444444446}
Step 2760: {'loss': 1.1553, 'grad_norm': 2.4706733226776123, 'learning_rate': 9.125925925925927e-06, 'epoch': 2.453333333333333}
Step 2770: {'loss': 1.1504, 'grad_norm': 2.514575481414795, 'learning_rate': 8.977777777777778e-06, 'epoch': 2.462222222222222}
Step 2780: {'loss': 1.1773, 'grad_norm': 2.5332629680633545, 'learning_rate': 8.82962962962963e-06, 'epoch': 2.471111111111111}
Step 2790: {'loss': 1.2044, 'grad_norm': 2.459078311920166, 'learning_rate': 8.681481481481482e-06, 'epoch': 2.48}
Step 2800: {'loss': 1.121, 'grad_norm': 2.802992582321167, 'learning_rate': 8.533333333333334e-06, 'epoch': 2.488888888888889}
Step 2810: {'loss': 1.1488, 'grad_norm': 2.686697006225586, 'learning_rate': 8.385185185185187e-06, 'epoch': 2.497777777777778}
Step 2820: {'loss': 1.1888, 'grad_norm': 2.4127047061920166, 'learning_rate': 8.237037037037038e-06, 'epoch': 2.506666666666667}
Step 2830: {'loss': 1.1122, 'grad_norm': 2.4316751956939697, 'learning_rate': 8.08888888888889e-06, 'epoch': 2.5155555555555553}
Step 2840: {'loss': 1.1664, 'grad_norm': 3.456876277923584, 'learning_rate': 7.940740740740742e-06, 'epoch': 2.5244444444444447}
Step 2850: {'loss': 1.1325, 'grad_norm': 2.776276111602783, 'learning_rate': 7.792592592592594e-06, 'epoch': 2.533333333333333}
Step 2860: {'loss': 1.2472, 'grad_norm': 2.5324594974517822, 'learning_rate': 7.644444444444445e-06, 'epoch': 2.542222222222222}
Step 2870: {'loss': 1.1381, 'grad_norm': 2.4975802898406982, 'learning_rate': 7.496296296296297e-06, 'epoch': 2.551111111111111}
Step 2880: {'loss': 1.1567, 'grad_norm': 2.4718799591064453, 'learning_rate': 7.348148148148148e-06, 'epoch': 2.56}
Step 2890: {'loss': 1.1971, 'grad_norm': 2.6934423446655273, 'learning_rate': 7.2e-06, 'epoch': 2.568888888888889}
Step 2900: {'loss': 1.2003, 'grad_norm': 2.697016477584839, 'learning_rate': 7.051851851851853e-06, 'epoch': 2.5777777777777775}
Step 2910: {'loss': 1.1449, 'grad_norm': 2.557438611984253, 'learning_rate': 6.903703703703704e-06, 'epoch': 2.586666666666667}
Step 2920: {'loss': 1.2312, 'grad_norm': 2.644131898880005, 'learning_rate': 6.755555555555555e-06, 'epoch': 2.5955555555555554}
Step 2930: {'loss': 1.1111, 'grad_norm': 2.975111484527588, 'learning_rate': 6.607407407407408e-06, 'epoch': 2.6044444444444443}
Step 2940: {'loss': 1.1056, 'grad_norm': 2.5637755393981934, 'learning_rate': 6.45925925925926e-06, 'epoch': 2.6133333333333333}
Step 2950: {'loss': 1.1061, 'grad_norm': 2.9336676597595215, 'learning_rate': 6.311111111111112e-06, 'epoch': 2.6222222222222222}
Step 2960: {'loss': 1.1473, 'grad_norm': 2.4044747352600098, 'learning_rate': 6.162962962962963e-06, 'epoch': 2.631111111111111}
Step 2970: {'loss': 1.1736, 'grad_norm': 2.697967529296875, 'learning_rate': 6.0148148148148145e-06, 'epoch': 2.64}
Step 2980: {'loss': 1.1598, 'grad_norm': 2.3005785942077637, 'learning_rate': 5.866666666666667e-06, 'epoch': 2.648888888888889}
Step 2990: {'loss': 1.1874, 'grad_norm': 2.852179527282715, 'learning_rate': 5.718518518518519e-06, 'epoch': 2.6577777777777776}
Step 3000: {'loss': 1.1485, 'grad_norm': 2.492623805999756, 'learning_rate': 5.570370370370371e-06, 'epoch': 2.6666666666666665}
Step 3010: {'loss': 1.1635, 'grad_norm': 2.6673357486724854, 'learning_rate': 5.422222222222222e-06, 'epoch': 2.6755555555555555}
Step 3020: {'loss': 1.056, 'grad_norm': 2.945003032684326, 'learning_rate': 5.2740740740740745e-06, 'epoch': 2.6844444444444444}
Step 3030: {'loss': 1.1637, 'grad_norm': 2.3446106910705566, 'learning_rate': 5.125925925925927e-06, 'epoch': 2.6933333333333334}
Step 3040: {'loss': 1.0211, 'grad_norm': 2.3074824810028076, 'learning_rate': 4.977777777777778e-06, 'epoch': 2.7022222222222223}
Step 3050: {'loss': 1.1648, 'grad_norm': 2.5173137187957764, 'learning_rate': 4.82962962962963e-06, 'epoch': 2.7111111111111112}
Step 3060: {'loss': 1.1599, 'grad_norm': 2.4728565216064453, 'learning_rate': 4.6814814814814815e-06, 'epoch': 2.7199999999999998}
Step 3070: {'loss': 1.1136, 'grad_norm': 2.4257943630218506, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.728888888888889}
Step 3080: {'loss': 1.2114, 'grad_norm': 2.5584468841552734, 'learning_rate': 4.385185185185185e-06, 'epoch': 2.7377777777777776}
Step 3090: {'loss': 1.0345, 'grad_norm': 2.455724000930786, 'learning_rate': 4.237037037037037e-06, 'epoch': 2.7466666666666666}
Step 3100: {'loss': 1.1388, 'grad_norm': 3.510089159011841, 'learning_rate': 4.088888888888889e-06, 'epoch': 2.7555555555555555}
Step 3110: {'loss': 1.0895, 'grad_norm': 2.9891810417175293, 'learning_rate': 3.940740740740741e-06, 'epoch': 2.7644444444444445}
Step 3120: {'loss': 1.0844, 'grad_norm': 3.2375171184539795, 'learning_rate': 3.792592592592593e-06, 'epoch': 2.7733333333333334}
Step 3130: {'loss': 1.1253, 'grad_norm': 3.2823333740234375, 'learning_rate': 3.6444444444444446e-06, 'epoch': 2.7822222222222224}
Step 3140: {'loss': 1.1536, 'grad_norm': 2.3807411193847656, 'learning_rate': 3.496296296296296e-06, 'epoch': 2.7911111111111113}
Step 3150: {'loss': 1.1131, 'grad_norm': 2.223222255706787, 'learning_rate': 3.348148148148148e-06, 'epoch': 2.8}
Step 3160: {'loss': 1.1629, 'grad_norm': 2.7680394649505615, 'learning_rate': 3.2000000000000003e-06, 'epoch': 2.8088888888888888}
Step 3170: {'loss': 1.3096, 'grad_norm': 3.095956563949585, 'learning_rate': 3.051851851851852e-06, 'epoch': 2.8177777777777777}
Step 3180: {'loss': 1.1047, 'grad_norm': 2.4472055435180664, 'learning_rate': 2.9037037037037038e-06, 'epoch': 2.8266666666666667}
Step 3190: {'loss': 1.1982, 'grad_norm': 2.6407580375671387, 'learning_rate': 2.7555555555555555e-06, 'epoch': 2.8355555555555556}
Step 3200: {'loss': 1.1395, 'grad_norm': 2.562191963195801, 'learning_rate': 2.6074074074074073e-06, 'epoch': 2.8444444444444446}
Step 3210: {'loss': 1.1579, 'grad_norm': 2.8076488971710205, 'learning_rate': 2.4592592592592594e-06, 'epoch': 2.8533333333333335}
Step 3220: {'loss': 1.1183, 'grad_norm': 2.56386137008667, 'learning_rate': 2.311111111111111e-06, 'epoch': 2.862222222222222}
Step 3230: {'loss': 1.1145, 'grad_norm': 3.0846118927001953, 'learning_rate': 2.1629629629629634e-06, 'epoch': 2.871111111111111}
Step 3240: {'loss': 1.1324, 'grad_norm': 3.325263500213623, 'learning_rate': 2.0148148148148147e-06, 'epoch': 2.88}
Step 3250: {'loss': 1.2147, 'grad_norm': 2.5574898719787598, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.888888888888889}
Step 3260: {'loss': 1.1128, 'grad_norm': 2.412485122680664, 'learning_rate': 1.7185185185185186e-06, 'epoch': 2.897777777777778}
Step 3270: {'loss': 1.0815, 'grad_norm': 2.3380212783813477, 'learning_rate': 1.5703703703703704e-06, 'epoch': 2.9066666666666667}
Step 3280: {'loss': 1.1085, 'grad_norm': 2.5902087688446045, 'learning_rate': 1.4222222222222223e-06, 'epoch': 2.9155555555555557}
Step 3290: {'loss': 1.1376, 'grad_norm': 3.1416454315185547, 'learning_rate': 1.274074074074074e-06, 'epoch': 2.924444444444444}
Step 3300: {'loss': 1.134, 'grad_norm': 2.3994505405426025, 'learning_rate': 1.125925925925926e-06, 'epoch': 2.9333333333333336}
Step 3310: {'loss': 1.0832, 'grad_norm': 2.874718427658081, 'learning_rate': 9.777777777777778e-07, 'epoch': 2.942222222222222}
Step 3320: {'loss': 1.015, 'grad_norm': 2.3697245121002197, 'learning_rate': 8.296296296296296e-07, 'epoch': 2.951111111111111}
Step 3330: {'loss': 1.1454, 'grad_norm': 2.5561912059783936, 'learning_rate': 6.814814814814816e-07, 'epoch': 2.96}
Step 3340: {'loss': 1.0824, 'grad_norm': 2.617145538330078, 'learning_rate': 5.333333333333333e-07, 'epoch': 2.968888888888889}
Step 3350: {'loss': 1.2564, 'grad_norm': 2.718031644821167, 'learning_rate': 3.851851851851852e-07, 'epoch': 2.977777777777778}
Step 3360: {'loss': 1.2682, 'grad_norm': 3.2732906341552734, 'learning_rate': 2.3703703703703705e-07, 'epoch': 2.986666666666667}
Step 3370: {'loss': 1.2345, 'grad_norm': 2.8774545192718506, 'learning_rate': 8.88888888888889e-08, 'epoch': 2.9955555555555557}
Epoch 3.0 completed in 2363.41s
Step 3375: {'eval_loss': 1.23152756690979, 'eval_runtime': 22.0915, 'eval_samples_per_second': 22.633, 'eval_steps_per_second': 5.658, 'epoch': 3.0}
Step 3375: {'train_runtime': 2386.3739, 'train_samples_per_second': 5.657, 'train_steps_per_second': 1.414, 'total_flos': 440938266624000.0, 'train_loss': 1.2986941788284867, 'epoch': 3.0}
=== Training completed in 2386.38s ===

Step 3375: {'eval_loss': 1.23152756690979, 'eval_runtime': 22.0527, 'eval_samples_per_second': 22.673, 'eval_steps_per_second': 5.668, 'epoch': 3.0}
