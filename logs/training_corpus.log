
=== Training started at 2025-12-10 01:14:54 ===
Step 10: {'loss': 5.3048, 'grad_norm': 9.720057487487793, 'learning_rate': 4.9957216200798634e-05, 'epoch': 0.002852253280091272}

=== Training started at 2025-12-10 01:15:45 ===
Step 10: {'loss': 5.3048, 'grad_norm': 9.720057487487793, 'learning_rate': 4.9957216200798634e-05, 'epoch': 0.002852253280091272}
Step 20: {'loss': 4.4002, 'grad_norm': 10.34430980682373, 'learning_rate': 4.990967864613045e-05, 'epoch': 0.005704506560182544}
Step 30: {'loss': 3.9295, 'grad_norm': 9.274236679077148, 'learning_rate': 4.9862141091462255e-05, 'epoch': 0.008556759840273816}
Step 40: {'loss': 3.7157, 'grad_norm': 12.06432056427002, 'learning_rate': 4.981460353679407e-05, 'epoch': 0.011409013120365089}
Step 50: {'loss': 3.3689, 'grad_norm': 10.41235065460205, 'learning_rate': 4.9767065982125884e-05, 'epoch': 0.01426126640045636}
Step 60: {'loss': 3.2408, 'grad_norm': 10.935225486755371, 'learning_rate': 4.97195284274577e-05, 'epoch': 0.017113519680547633}
Step 70: {'loss': 2.7475, 'grad_norm': 9.975006103515625, 'learning_rate': 4.9671990872789506e-05, 'epoch': 0.019965772960638905}
Step 80: {'loss': 2.7608, 'grad_norm': 10.138603210449219, 'learning_rate': 4.962445331812132e-05, 'epoch': 0.022818026240730177}
Step 90: {'loss': 2.4556, 'grad_norm': 11.077003479003906, 'learning_rate': 4.957691576345313e-05, 'epoch': 0.02567027952082145}
Step 100: {'loss': 2.587, 'grad_norm': 11.022920608520508, 'learning_rate': 4.952937820878494e-05, 'epoch': 0.02852253280091272}
Step 110: {'loss': 2.4909, 'grad_norm': 11.322982788085938, 'learning_rate': 4.9481840654116756e-05, 'epoch': 0.03137478608100399}
Step 120: {'loss': 2.2387, 'grad_norm': 10.677231788635254, 'learning_rate': 4.943430309944856e-05, 'epoch': 0.034227039361095266}
Step 130: {'loss': 2.1785, 'grad_norm': 8.329137802124023, 'learning_rate': 4.938676554478038e-05, 'epoch': 0.037079292641186534}
Step 140: {'loss': 2.1369, 'grad_norm': 10.272608757019043, 'learning_rate': 4.933922799011219e-05, 'epoch': 0.03993154592127781}
Step 150: {'loss': 2.0779, 'grad_norm': 7.105381011962891, 'learning_rate': 4.9291690435444006e-05, 'epoch': 0.04278379920136908}
Step 160: {'loss': 2.0773, 'grad_norm': 9.463215827941895, 'learning_rate': 4.924415288077582e-05, 'epoch': 0.045636052481460354}
Step 170: {'loss': 1.9757, 'grad_norm': 9.692168235778809, 'learning_rate': 4.919661532610763e-05, 'epoch': 0.04848830576155162}
Step 180: {'loss': 2.0156, 'grad_norm': 6.224415302276611, 'learning_rate': 4.914907777143944e-05, 'epoch': 0.0513405590416429}
Step 190: {'loss': 1.9532, 'grad_norm': 7.6790995597839355, 'learning_rate': 4.910154021677125e-05, 'epoch': 0.05419281232173417}
Step 200: {'loss': 1.8406, 'grad_norm': 8.481208801269531, 'learning_rate': 4.905400266210306e-05, 'epoch': 0.05704506560182544}
Step 210: {'loss': 1.8165, 'grad_norm': 8.474637031555176, 'learning_rate': 4.900646510743487e-05, 'epoch': 0.05989731888191671}
Step 220: {'loss': 1.8873, 'grad_norm': 7.113288879394531, 'learning_rate': 4.8958927552766685e-05, 'epoch': 0.06274957216200798}
Step 230: {'loss': 1.8202, 'grad_norm': 6.608084201812744, 'learning_rate': 4.89113899980985e-05, 'epoch': 0.06560182544209926}
Step 240: {'loss': 1.844, 'grad_norm': 5.808560848236084, 'learning_rate': 4.886385244343031e-05, 'epoch': 0.06845407872219053}
Step 250: {'loss': 1.757, 'grad_norm': 8.01767349243164, 'learning_rate': 4.881631488876213e-05, 'epoch': 0.0713063320022818}
Step 260: {'loss': 1.6987, 'grad_norm': 6.432114601135254, 'learning_rate': 4.8768777334093935e-05, 'epoch': 0.07415858528237307}
Step 270: {'loss': 1.7439, 'grad_norm': 6.548396110534668, 'learning_rate': 4.872123977942575e-05, 'epoch': 0.07701083856246435}
Step 280: {'loss': 1.6066, 'grad_norm': 6.487468719482422, 'learning_rate': 4.867370222475756e-05, 'epoch': 0.07986309184255562}
Step 290: {'loss': 1.8399, 'grad_norm': 5.935137748718262, 'learning_rate': 4.862616467008938e-05, 'epoch': 0.08271534512264689}
Step 300: {'loss': 1.7048, 'grad_norm': 5.928245544433594, 'learning_rate': 4.8578627115421185e-05, 'epoch': 0.08556759840273816}
Step 310: {'loss': 1.7141, 'grad_norm': 6.541923522949219, 'learning_rate': 4.8531089560753e-05, 'epoch': 0.08841985168282944}
Step 320: {'loss': 1.6668, 'grad_norm': 6.367659091949463, 'learning_rate': 4.848355200608481e-05, 'epoch': 0.09127210496292071}
Step 330: {'loss': 1.6829, 'grad_norm': 5.123973369598389, 'learning_rate': 4.843601445141662e-05, 'epoch': 0.09412435824301198}
Step 340: {'loss': 1.8003, 'grad_norm': 5.504822731018066, 'learning_rate': 4.8388476896748435e-05, 'epoch': 0.09697661152310325}
Step 350: {'loss': 1.6679, 'grad_norm': 5.4794535636901855, 'learning_rate': 4.834093934208024e-05, 'epoch': 0.09982886480319453}
Step 360: {'loss': 1.5484, 'grad_norm': 6.020420074462891, 'learning_rate': 4.829340178741206e-05, 'epoch': 0.1026811180832858}
Step 370: {'loss': 1.7358, 'grad_norm': 5.229526519775391, 'learning_rate': 4.824586423274387e-05, 'epoch': 0.10553337136337707}
Step 380: {'loss': 1.6708, 'grad_norm': 4.909204959869385, 'learning_rate': 4.8198326678075685e-05, 'epoch': 0.10838562464346833}
Step 390: {'loss': 1.6471, 'grad_norm': 5.217482566833496, 'learning_rate': 4.815078912340749e-05, 'epoch': 0.11123787792355962}
Step 400: {'loss': 1.6428, 'grad_norm': 5.7389817237854, 'learning_rate': 4.810325156873931e-05, 'epoch': 0.11409013120365089}
Step 410: {'loss': 1.7611, 'grad_norm': 5.8240861892700195, 'learning_rate': 4.805571401407112e-05, 'epoch': 0.11694238448374215}
Step 420: {'loss': 1.6547, 'grad_norm': 4.62546443939209, 'learning_rate': 4.800817645940293e-05, 'epoch': 0.11979463776383342}
Step 430: {'loss': 1.6905, 'grad_norm': 5.0878190994262695, 'learning_rate': 4.796063890473474e-05, 'epoch': 0.1226468910439247}
Step 440: {'loss': 1.5302, 'grad_norm': 4.3788886070251465, 'learning_rate': 4.791310135006655e-05, 'epoch': 0.12549914432401596}
Step 450: {'loss': 1.6376, 'grad_norm': 5.049925327301025, 'learning_rate': 4.7865563795398364e-05, 'epoch': 0.12835139760410724}
Step 460: {'loss': 1.5633, 'grad_norm': 4.588894367218018, 'learning_rate': 4.781802624073018e-05, 'epoch': 0.13120365088419852}
Step 470: {'loss': 1.7045, 'grad_norm': 3.938211679458618, 'learning_rate': 4.777048868606199e-05, 'epoch': 0.13405590416428978}
Step 480: {'loss': 1.7306, 'grad_norm': 4.105890274047852, 'learning_rate': 4.772295113139381e-05, 'epoch': 0.13690815744438106}
Step 490: {'loss': 1.5031, 'grad_norm': 4.656177520751953, 'learning_rate': 4.7675413576725614e-05, 'epoch': 0.13976041072447234}
Step 500: {'loss': 1.6253, 'grad_norm': 4.370143890380859, 'learning_rate': 4.762787602205743e-05, 'epoch': 0.1426126640045636}
Step 510: {'loss': 1.5187, 'grad_norm': 4.496453285217285, 'learning_rate': 4.758033846738924e-05, 'epoch': 0.14546491728465488}
Step 520: {'loss': 1.7094, 'grad_norm': 4.424911975860596, 'learning_rate': 4.753280091272106e-05, 'epoch': 0.14831717056474614}
Step 530: {'loss': 1.5104, 'grad_norm': 5.703064918518066, 'learning_rate': 4.7485263358052864e-05, 'epoch': 0.15116942384483742}
Step 540: {'loss': 1.6349, 'grad_norm': 4.556964874267578, 'learning_rate': 4.743772580338467e-05, 'epoch': 0.1540216771249287}
Step 550: {'loss': 1.6469, 'grad_norm': 4.176896572113037, 'learning_rate': 4.7390188248716486e-05, 'epoch': 0.15687393040501996}
Step 560: {'loss': 1.613, 'grad_norm': 4.487843990325928, 'learning_rate': 4.73426506940483e-05, 'epoch': 0.15972618368511124}
Step 570: {'loss': 1.5615, 'grad_norm': 4.45355749130249, 'learning_rate': 4.7295113139380115e-05, 'epoch': 0.16257843696520252}
Step 580: {'loss': 1.6438, 'grad_norm': 3.971508741378784, 'learning_rate': 4.724757558471192e-05, 'epoch': 0.16543069024529378}
Step 590: {'loss': 1.5988, 'grad_norm': 3.9413535594940186, 'learning_rate': 4.7200038030043736e-05, 'epoch': 0.16828294352538506}
Step 600: {'loss': 1.7077, 'grad_norm': 4.584709167480469, 'learning_rate': 4.715250047537555e-05, 'epoch': 0.17113519680547631}
Step 610: {'loss': 1.611, 'grad_norm': 4.4282121658325195, 'learning_rate': 4.7104962920707365e-05, 'epoch': 0.1739874500855676}
Step 620: {'loss': 1.5415, 'grad_norm': 4.3826799392700195, 'learning_rate': 4.705742536603917e-05, 'epoch': 0.17683970336565888}
Step 630: {'loss': 1.6612, 'grad_norm': 3.34346079826355, 'learning_rate': 4.7009887811370986e-05, 'epoch': 0.17969195664575013}
Step 640: {'loss': 1.583, 'grad_norm': 3.9408464431762695, 'learning_rate': 4.69623502567028e-05, 'epoch': 0.18254420992584142}
Step 650: {'loss': 1.4639, 'grad_norm': 3.497763156890869, 'learning_rate': 4.691481270203461e-05, 'epoch': 0.1853964632059327}
Step 660: {'loss': 1.6517, 'grad_norm': 4.608572959899902, 'learning_rate': 4.686727514736642e-05, 'epoch': 0.18824871648602395}
Step 670: {'loss': 1.5612, 'grad_norm': 4.397219181060791, 'learning_rate': 4.681973759269823e-05, 'epoch': 0.19110096976611524}
Step 680: {'loss': 1.7643, 'grad_norm': 3.7320728302001953, 'learning_rate': 4.6772200038030044e-05, 'epoch': 0.1939532230462065}
Step 690: {'loss': 1.5825, 'grad_norm': 3.896219253540039, 'learning_rate': 4.672466248336186e-05, 'epoch': 0.19680547632629777}
Step 700: {'loss': 1.55, 'grad_norm': 3.6952736377716064, 'learning_rate': 4.667712492869367e-05, 'epoch': 0.19965772960638906}
Step 710: {'loss': 1.5196, 'grad_norm': 3.3095662593841553, 'learning_rate': 4.662958737402548e-05, 'epoch': 0.2025099828864803}
Step 720: {'loss': 1.5915, 'grad_norm': 3.4304072856903076, 'learning_rate': 4.6582049819357294e-05, 'epoch': 0.2053622361665716}
Step 730: {'loss': 1.6647, 'grad_norm': 3.7197089195251465, 'learning_rate': 4.653451226468911e-05, 'epoch': 0.20821448944666288}
Step 740: {'loss': 1.6694, 'grad_norm': 4.249790668487549, 'learning_rate': 4.648697471002092e-05, 'epoch': 0.21106674272675413}
Step 750: {'loss': 1.6057, 'grad_norm': 3.161055326461792, 'learning_rate': 4.643943715535273e-05, 'epoch': 0.2139189960068454}
Step 760: {'loss': 1.53, 'grad_norm': 3.8296310901641846, 'learning_rate': 4.6391899600684544e-05, 'epoch': 0.21677124928693667}
Step 770: {'loss': 1.4669, 'grad_norm': 3.7572641372680664, 'learning_rate': 4.634436204601635e-05, 'epoch': 0.21962350256702795}
Step 780: {'loss': 1.5928, 'grad_norm': 3.918558120727539, 'learning_rate': 4.6296824491348166e-05, 'epoch': 0.22247575584711923}
Step 790: {'loss': 1.456, 'grad_norm': 3.713012456893921, 'learning_rate': 4.624928693667998e-05, 'epoch': 0.2253280091272105}
Step 800: {'loss': 1.5915, 'grad_norm': 4.126245498657227, 'learning_rate': 4.6201749382011794e-05, 'epoch': 0.22818026240730177}
Step 810: {'loss': 1.6263, 'grad_norm': 5.035412311553955, 'learning_rate': 4.61542118273436e-05, 'epoch': 0.23103251568739305}
Step 820: {'loss': 1.599, 'grad_norm': 3.864370107650757, 'learning_rate': 4.6106674272675416e-05, 'epoch': 0.2338847689674843}
Step 830: {'loss': 1.5282, 'grad_norm': 3.9779229164123535, 'learning_rate': 4.605913671800723e-05, 'epoch': 0.2367370222475756}
Step 840: {'loss': 1.5575, 'grad_norm': 3.964641809463501, 'learning_rate': 4.6011599163339044e-05, 'epoch': 0.23958927552766685}
Step 850: {'loss': 1.5886, 'grad_norm': 3.5377395153045654, 'learning_rate': 4.596406160867085e-05, 'epoch': 0.24244152880775813}
Step 860: {'loss': 1.618, 'grad_norm': 3.2636377811431885, 'learning_rate': 4.5916524054002666e-05, 'epoch': 0.2452937820878494}
Step 870: {'loss': 1.5615, 'grad_norm': 3.9446678161621094, 'learning_rate': 4.586898649933447e-05, 'epoch': 0.24814603536794066}
Step 880: {'loss': 1.4997, 'grad_norm': 3.3598151206970215, 'learning_rate': 4.582144894466629e-05, 'epoch': 0.2509982886480319}
Step 890: {'loss': 1.5593, 'grad_norm': 3.744845390319824, 'learning_rate': 4.57739113899981e-05, 'epoch': 0.25385054192812323}
Step 900: {'loss': 1.5535, 'grad_norm': 3.5994908809661865, 'learning_rate': 4.572637383532991e-05, 'epoch': 0.2567027952082145}
Step 910: {'loss': 1.56, 'grad_norm': 3.8663861751556396, 'learning_rate': 4.567883628066172e-05, 'epoch': 0.25955504848830574}
Step 920: {'loss': 1.6237, 'grad_norm': 3.796011209487915, 'learning_rate': 4.563129872599354e-05, 'epoch': 0.26240730176839705}
Step 930: {'loss': 1.4065, 'grad_norm': 4.069364547729492, 'learning_rate': 4.558376117132535e-05, 'epoch': 0.2652595550484883}
Step 940: {'loss': 1.4706, 'grad_norm': 3.6470320224761963, 'learning_rate': 4.553622361665716e-05, 'epoch': 0.26811180832857956}
Step 950: {'loss': 1.5297, 'grad_norm': 3.2799739837646484, 'learning_rate': 4.548868606198897e-05, 'epoch': 0.27096406160867087}
Step 960: {'loss': 1.6197, 'grad_norm': 4.647640705108643, 'learning_rate': 4.544114850732079e-05, 'epoch': 0.2738163148887621}
Step 970: {'loss': 1.5572, 'grad_norm': 2.647657632827759, 'learning_rate': 4.53936109526526e-05, 'epoch': 0.2766685681688534}
Step 980: {'loss': 1.5183, 'grad_norm': 4.004868507385254, 'learning_rate': 4.534607339798441e-05, 'epoch': 0.2795208214489447}
Step 990: {'loss': 1.7397, 'grad_norm': 3.1895933151245117, 'learning_rate': 4.529853584331622e-05, 'epoch': 0.28237307472903594}
Step 1000: {'loss': 1.5366, 'grad_norm': 3.1571576595306396, 'learning_rate': 4.525099828864803e-05, 'epoch': 0.2852253280091272}
Step 1010: {'loss': 1.5146, 'grad_norm': 2.9976022243499756, 'learning_rate': 4.5203460733979845e-05, 'epoch': 0.2880775812892185}
Step 1020: {'loss': 1.4802, 'grad_norm': 2.8933019638061523, 'learning_rate': 4.515592317931166e-05, 'epoch': 0.29092983456930976}
Step 1030: {'loss': 1.5194, 'grad_norm': 3.5244243144989014, 'learning_rate': 4.510838562464347e-05, 'epoch': 0.293782087849401}
Step 1040: {'loss': 1.5402, 'grad_norm': 5.111469268798828, 'learning_rate': 4.506084806997528e-05, 'epoch': 0.2966343411294923}
Step 1050: {'loss': 1.623, 'grad_norm': 3.5081703662872314, 'learning_rate': 4.5013310515307095e-05, 'epoch': 0.2994865944095836}
Step 1060: {'loss': 1.481, 'grad_norm': 3.302669048309326, 'learning_rate': 4.496577296063891e-05, 'epoch': 0.30233884768967484}
Step 1070: {'loss': 1.5371, 'grad_norm': 3.9506325721740723, 'learning_rate': 4.4918235405970724e-05, 'epoch': 0.3051911009697661}
Step 1080: {'loss': 1.5038, 'grad_norm': 3.487846851348877, 'learning_rate': 4.487069785130253e-05, 'epoch': 0.3080433542498574}
Step 1090: {'loss': 1.5358, 'grad_norm': 3.227931022644043, 'learning_rate': 4.4823160296634345e-05, 'epoch': 0.31089560752994866}
Step 1100: {'loss': 1.6156, 'grad_norm': 4.195546627044678, 'learning_rate': 4.477562274196615e-05, 'epoch': 0.3137478608100399}
Step 1110: {'loss': 1.5043, 'grad_norm': 2.8401641845703125, 'learning_rate': 4.472808518729797e-05, 'epoch': 0.3166001140901312}
Step 1120: {'loss': 1.472, 'grad_norm': 3.6947686672210693, 'learning_rate': 4.468054763262978e-05, 'epoch': 0.3194523673702225}
Step 1130: {'loss': 1.5636, 'grad_norm': 3.1030678749084473, 'learning_rate': 4.463301007796159e-05, 'epoch': 0.32230462065031373}
Step 1140: {'loss': 1.5372, 'grad_norm': 3.622791051864624, 'learning_rate': 4.45854725232934e-05, 'epoch': 0.32515687393040504}
Step 1150: {'loss': 1.6211, 'grad_norm': 3.544302463531494, 'learning_rate': 4.453793496862522e-05, 'epoch': 0.3280091272104963}
Step 1160: {'loss': 1.6444, 'grad_norm': 3.3103959560394287, 'learning_rate': 4.449039741395703e-05, 'epoch': 0.33086138049058755}
Step 1170: {'loss': 1.579, 'grad_norm': 3.7664976119995117, 'learning_rate': 4.444285985928884e-05, 'epoch': 0.33371363377067886}
Step 1180: {'loss': 1.4187, 'grad_norm': 3.5674381256103516, 'learning_rate': 4.439532230462065e-05, 'epoch': 0.3365658870507701}
Step 1190: {'loss': 1.5067, 'grad_norm': 3.433037757873535, 'learning_rate': 4.434778474995247e-05, 'epoch': 0.3394181403308614}
Step 1200: {'loss': 1.438, 'grad_norm': 3.090195655822754, 'learning_rate': 4.430024719528428e-05, 'epoch': 0.34227039361095263}
Step 1210: {'loss': 1.5446, 'grad_norm': 3.5797877311706543, 'learning_rate': 4.425270964061609e-05, 'epoch': 0.34512264689104394}
Step 1220: {'loss': 1.4919, 'grad_norm': 3.7171616554260254, 'learning_rate': 4.4205172085947896e-05, 'epoch': 0.3479749001711352}
Step 1230: {'loss': 1.4573, 'grad_norm': 3.550909996032715, 'learning_rate': 4.415763453127971e-05, 'epoch': 0.35082715345122645}
Step 1240: {'loss': 1.5422, 'grad_norm': 3.517561912536621, 'learning_rate': 4.4110096976611525e-05, 'epoch': 0.35367940673131776}
Step 1250: {'loss': 1.7793, 'grad_norm': 3.995004653930664, 'learning_rate': 4.406255942194334e-05, 'epoch': 0.356531660011409}
Step 1260: {'loss': 1.566, 'grad_norm': 3.200976610183716, 'learning_rate': 4.4015021867275146e-05, 'epoch': 0.35938391329150027}
Step 1270: {'loss': 1.5223, 'grad_norm': 3.1917388439178467, 'learning_rate': 4.396748431260696e-05, 'epoch': 0.3622361665715916}
Step 1280: {'loss': 1.4957, 'grad_norm': 3.5232930183410645, 'learning_rate': 4.3919946757938775e-05, 'epoch': 0.36508841985168283}
Step 1290: {'loss': 1.6554, 'grad_norm': 2.8667984008789062, 'learning_rate': 4.387240920327059e-05, 'epoch': 0.3679406731317741}
Step 1300: {'loss': 1.3128, 'grad_norm': 2.674590826034546, 'learning_rate': 4.38248716486024e-05, 'epoch': 0.3707929264118654}
Step 1310: {'loss': 1.4117, 'grad_norm': 3.1747779846191406, 'learning_rate': 4.377733409393421e-05, 'epoch': 0.37364517969195665}
Step 1320: {'loss': 1.5065, 'grad_norm': 2.9955697059631348, 'learning_rate': 4.3729796539266025e-05, 'epoch': 0.3764974329720479}
Step 1330: {'loss': 1.5436, 'grad_norm': 3.5346500873565674, 'learning_rate': 4.368225898459783e-05, 'epoch': 0.3793496862521392}
Step 1340: {'loss': 1.2907, 'grad_norm': 3.3176186084747314, 'learning_rate': 4.3634721429929646e-05, 'epoch': 0.3822019395322305}
Step 1350: {'loss': 1.4265, 'grad_norm': 2.8712985515594482, 'learning_rate': 4.358718387526146e-05, 'epoch': 0.3850541928123217}
Step 1360: {'loss': 1.424, 'grad_norm': 2.822844982147217, 'learning_rate': 4.353964632059327e-05, 'epoch': 0.387906446092413}
Step 1370: {'loss': 1.467, 'grad_norm': 3.2125396728515625, 'learning_rate': 4.349210876592508e-05, 'epoch': 0.3907586993725043}
Step 1380: {'loss': 1.429, 'grad_norm': 3.5059852600097656, 'learning_rate': 4.3444571211256896e-05, 'epoch': 0.39361095265259555}
Step 1390: {'loss': 1.5304, 'grad_norm': 3.3947718143463135, 'learning_rate': 4.339703365658871e-05, 'epoch': 0.3964632059326868}
Step 1400: {'loss': 1.4893, 'grad_norm': 3.0452041625976562, 'learning_rate': 4.334949610192052e-05, 'epoch': 0.3993154592127781}
Step 1410: {'loss': 1.577, 'grad_norm': 3.116162061691284, 'learning_rate': 4.330195854725233e-05, 'epoch': 0.40216771249286937}
Step 1420: {'loss': 1.5129, 'grad_norm': 3.2224314212799072, 'learning_rate': 4.3254420992584146e-05, 'epoch': 0.4050199657729606}
Step 1430: {'loss': 1.5797, 'grad_norm': 3.7240469455718994, 'learning_rate': 4.3206883437915954e-05, 'epoch': 0.40787221905305193}
Step 1440: {'loss': 1.4617, 'grad_norm': 3.3452370166778564, 'learning_rate': 4.315934588324777e-05, 'epoch': 0.4107244723331432}
Step 1450: {'loss': 1.4414, 'grad_norm': 3.1948347091674805, 'learning_rate': 4.3111808328579576e-05, 'epoch': 0.41357672561323444}
Step 1460: {'loss': 1.5756, 'grad_norm': 2.968855857849121, 'learning_rate': 4.306427077391139e-05, 'epoch': 0.41642897889332575}
Step 1470: {'loss': 1.5494, 'grad_norm': 3.5961806774139404, 'learning_rate': 4.3016733219243204e-05, 'epoch': 0.419281232173417}
Step 1480: {'loss': 1.4467, 'grad_norm': 3.0777766704559326, 'learning_rate': 4.296919566457502e-05, 'epoch': 0.42213348545350826}
Step 1490: {'loss': 1.5102, 'grad_norm': 3.4135162830352783, 'learning_rate': 4.2921658109906826e-05, 'epoch': 0.4249857387335995}
Step 1500: {'loss': 1.4601, 'grad_norm': 2.9093210697174072, 'learning_rate': 4.287412055523864e-05, 'epoch': 0.4278379920136908}
Step 1510: {'loss': 1.5178, 'grad_norm': 3.5768020153045654, 'learning_rate': 4.2826583000570454e-05, 'epoch': 0.4306902452937821}
Step 1520: {'loss': 1.4162, 'grad_norm': 3.1636784076690674, 'learning_rate': 4.277904544590227e-05, 'epoch': 0.43354249857387334}
Step 1530: {'loss': 1.4637, 'grad_norm': 2.822054862976074, 'learning_rate': 4.273150789123408e-05, 'epoch': 0.43639475185396465}
Step 1540: {'loss': 1.4706, 'grad_norm': 2.869387149810791, 'learning_rate': 4.268397033656589e-05, 'epoch': 0.4392470051340559}
Step 1550: {'loss': 1.4961, 'grad_norm': 3.199962615966797, 'learning_rate': 4.26364327818977e-05, 'epoch': 0.44209925841414716}
Step 1560: {'loss': 1.4947, 'grad_norm': 2.5929648876190186, 'learning_rate': 4.258889522722951e-05, 'epoch': 0.44495151169423847}
Step 1570: {'loss': 1.4247, 'grad_norm': 3.193101406097412, 'learning_rate': 4.2541357672561326e-05, 'epoch': 0.4478037649743297}
Step 1580: {'loss': 1.3894, 'grad_norm': 3.027848720550537, 'learning_rate': 4.249382011789313e-05, 'epoch': 0.450656018254421}
Step 1590: {'loss': 1.3893, 'grad_norm': 3.3291192054748535, 'learning_rate': 4.244628256322495e-05, 'epoch': 0.4535082715345123}
Step 1600: {'loss': 1.5393, 'grad_norm': 3.5112059116363525, 'learning_rate': 4.239874500855676e-05, 'epoch': 0.45636052481460354}
Step 1610: {'loss': 1.4847, 'grad_norm': 2.963153839111328, 'learning_rate': 4.2351207453888576e-05, 'epoch': 0.4592127780946948}
Step 1620: {'loss': 1.5834, 'grad_norm': 3.231019973754883, 'learning_rate': 4.230366989922039e-05, 'epoch': 0.4620650313747861}
Step 1630: {'loss': 1.3347, 'grad_norm': 2.598029375076294, 'learning_rate': 4.22561323445522e-05, 'epoch': 0.46491728465487736}
Step 1640: {'loss': 1.4938, 'grad_norm': 3.509351968765259, 'learning_rate': 4.220859478988401e-05, 'epoch': 0.4677695379349686}
Step 1650: {'loss': 1.395, 'grad_norm': 2.7106783390045166, 'learning_rate': 4.2161057235215826e-05, 'epoch': 0.47062179121505987}
Step 1660: {'loss': 1.5682, 'grad_norm': 2.5471699237823486, 'learning_rate': 4.211351968054763e-05, 'epoch': 0.4734740444951512}
Step 1670: {'loss': 1.4704, 'grad_norm': 3.1137633323669434, 'learning_rate': 4.206598212587945e-05, 'epoch': 0.47632629777524244}
Step 1680: {'loss': 1.5192, 'grad_norm': 2.524843215942383, 'learning_rate': 4.2018444571211255e-05, 'epoch': 0.4791785510553337}
Step 1690: {'loss': 1.4183, 'grad_norm': 3.2460107803344727, 'learning_rate': 4.197090701654307e-05, 'epoch': 0.482030804335425}
Step 1700: {'loss': 1.3399, 'grad_norm': 3.2165420055389404, 'learning_rate': 4.1923369461874883e-05, 'epoch': 0.48488305761551626}
Step 1710: {'loss': 1.3471, 'grad_norm': 3.435469627380371, 'learning_rate': 4.18758319072067e-05, 'epoch': 0.4877353108956075}
Step 1720: {'loss': 1.4813, 'grad_norm': 3.005552053451538, 'learning_rate': 4.1828294352538505e-05, 'epoch': 0.4905875641756988}
Step 1730: {'loss': 1.4789, 'grad_norm': 2.6993956565856934, 'learning_rate': 4.178075679787032e-05, 'epoch': 0.4934398174557901}
Step 1740: {'loss': 1.4561, 'grad_norm': 3.3968684673309326, 'learning_rate': 4.1733219243202134e-05, 'epoch': 0.49629207073588133}
Step 1750: {'loss': 1.4596, 'grad_norm': 2.682495594024658, 'learning_rate': 4.168568168853395e-05, 'epoch': 0.49914432401597264}
Step 1760: {'loss': 1.453, 'grad_norm': 3.17767071723938, 'learning_rate': 4.1638144133865755e-05, 'epoch': 0.5019965772960638}
Step 1770: {'loss': 1.5617, 'grad_norm': 3.3262548446655273, 'learning_rate': 4.159060657919757e-05, 'epoch': 0.5048488305761551}
Step 1780: {'loss': 1.3709, 'grad_norm': 3.0560078620910645, 'learning_rate': 4.154306902452938e-05, 'epoch': 0.5077010838562465}
Step 1790: {'loss': 1.3811, 'grad_norm': 3.176085948944092, 'learning_rate': 4.149553146986119e-05, 'epoch': 0.5105533371363377}
Step 1800: {'loss': 1.3526, 'grad_norm': 3.5604448318481445, 'learning_rate': 4.1447993915193005e-05, 'epoch': 0.513405590416429}
Step 1810: {'loss': 1.4127, 'grad_norm': 3.068840742111206, 'learning_rate': 4.140045636052481e-05, 'epoch': 0.5162578436965203}
Step 1820: {'loss': 1.4231, 'grad_norm': 2.651155710220337, 'learning_rate': 4.135291880585663e-05, 'epoch': 0.5191100969766115}
Step 1830: {'loss': 1.5808, 'grad_norm': 3.420074701309204, 'learning_rate': 4.130538125118844e-05, 'epoch': 0.5219623502567028}
Step 1840: {'loss': 1.4638, 'grad_norm': 2.9953436851501465, 'learning_rate': 4.1257843696520255e-05, 'epoch': 0.5248146035367941}
Step 1850: {'loss': 1.579, 'grad_norm': 3.7431693077087402, 'learning_rate': 4.121030614185207e-05, 'epoch': 0.5276668568168853}
Step 1860: {'loss': 1.5163, 'grad_norm': 2.1826019287109375, 'learning_rate': 4.116276858718388e-05, 'epoch': 0.5305191100969766}
Step 1870: {'loss': 1.5384, 'grad_norm': 2.541097402572632, 'learning_rate': 4.111523103251569e-05, 'epoch': 0.5333713633770679}
Step 1880: {'loss': 1.4082, 'grad_norm': 2.9577200412750244, 'learning_rate': 4.10676934778475e-05, 'epoch': 0.5362236166571591}
Step 1890: {'loss': 1.5229, 'grad_norm': 2.6873855590820312, 'learning_rate': 4.102015592317931e-05, 'epoch': 0.5390758699372504}
Step 1900: {'loss': 1.4558, 'grad_norm': 2.382124185562134, 'learning_rate': 4.097261836851112e-05, 'epoch': 0.5419281232173417}
Step 1910: {'loss': 1.5122, 'grad_norm': 3.1960904598236084, 'learning_rate': 4.0925080813842934e-05, 'epoch': 0.5447803764974329}
Step 1920: {'loss': 1.4442, 'grad_norm': 2.783588171005249, 'learning_rate': 4.087754325917475e-05, 'epoch': 0.5476326297775242}
Step 1930: {'loss': 1.5945, 'grad_norm': 2.9916815757751465, 'learning_rate': 4.083000570450656e-05, 'epoch': 0.5504848830576156}
Step 1940: {'loss': 1.4002, 'grad_norm': 3.0172064304351807, 'learning_rate': 4.078246814983838e-05, 'epoch': 0.5533371363377068}
Step 1950: {'loss': 1.4537, 'grad_norm': 2.9190964698791504, 'learning_rate': 4.0734930595170185e-05, 'epoch': 0.5561893896177981}
Step 1960: {'loss': 1.4065, 'grad_norm': 3.2745108604431152, 'learning_rate': 4.0687393040502e-05, 'epoch': 0.5590416428978894}
Step 1970: {'loss': 1.4284, 'grad_norm': 2.7083699703216553, 'learning_rate': 4.063985548583381e-05, 'epoch': 0.5618938961779806}
Step 1980: {'loss': 1.5537, 'grad_norm': 2.9894912242889404, 'learning_rate': 4.059231793116563e-05, 'epoch': 0.5647461494580719}
Step 1990: {'loss': 1.4927, 'grad_norm': 2.7267322540283203, 'learning_rate': 4.0544780376497435e-05, 'epoch': 0.5675984027381632}
Step 2000: {'loss': 1.6044, 'grad_norm': 3.2560839653015137, 'learning_rate': 4.049724282182925e-05, 'epoch': 0.5704506560182544}
Step 2010: {'loss': 1.3873, 'grad_norm': 2.6926381587982178, 'learning_rate': 4.0449705267161056e-05, 'epoch': 0.5733029092983457}
Step 2020: {'loss': 1.4081, 'grad_norm': 2.5726146697998047, 'learning_rate': 4.040216771249287e-05, 'epoch': 0.576155162578437}
Step 2030: {'loss': 1.301, 'grad_norm': 3.271136522293091, 'learning_rate': 4.0354630157824685e-05, 'epoch': 0.5790074158585282}
Step 2040: {'loss': 1.5469, 'grad_norm': 2.9638824462890625, 'learning_rate': 4.030709260315649e-05, 'epoch': 0.5818596691386195}
Step 2050: {'loss': 1.3616, 'grad_norm': 2.628509044647217, 'learning_rate': 4.0259555048488306e-05, 'epoch': 0.5847119224187107}
Step 2060: {'loss': 1.4601, 'grad_norm': 2.534963369369507, 'learning_rate': 4.021201749382012e-05, 'epoch': 0.587564175698802}
Step 2070: {'loss': 1.4739, 'grad_norm': 3.1115360260009766, 'learning_rate': 4.0164479939151935e-05, 'epoch': 0.5904164289788933}
Step 2080: {'loss': 1.5007, 'grad_norm': 2.8114938735961914, 'learning_rate': 4.011694238448375e-05, 'epoch': 0.5932686822589845}
Step 2090: {'loss': 1.2984, 'grad_norm': 3.2987606525421143, 'learning_rate': 4.0069404829815556e-05, 'epoch': 0.5961209355390759}
Step 2100: {'loss': 1.4643, 'grad_norm': 2.3217360973358154, 'learning_rate': 4.002186727514737e-05, 'epoch': 0.5989731888191672}
Step 2110: {'loss': 1.4675, 'grad_norm': 3.0565154552459717, 'learning_rate': 3.997432972047918e-05, 'epoch': 0.6018254420992584}
Step 2120: {'loss': 1.4947, 'grad_norm': 3.072110414505005, 'learning_rate': 3.992679216581099e-05, 'epoch': 0.6046776953793497}
Step 2130: {'loss': 1.4084, 'grad_norm': 3.3590033054351807, 'learning_rate': 3.98792546111428e-05, 'epoch': 0.607529948659441}
Step 2140: {'loss': 1.3893, 'grad_norm': 3.072309970855713, 'learning_rate': 3.9831717056474614e-05, 'epoch': 0.6103822019395322}
Step 2150: {'loss': 1.4457, 'grad_norm': 2.9491841793060303, 'learning_rate': 3.978417950180643e-05, 'epoch': 0.6132344552196235}
Step 2160: {'loss': 1.5153, 'grad_norm': 3.9279916286468506, 'learning_rate': 3.973664194713824e-05, 'epoch': 0.6160867084997148}
Step 2170: {'loss': 1.5459, 'grad_norm': 2.7057764530181885, 'learning_rate': 3.9689104392470057e-05, 'epoch': 0.618938961779806}
Step 2180: {'loss': 1.5182, 'grad_norm': 2.943591356277466, 'learning_rate': 3.9641566837801864e-05, 'epoch': 0.6217912150598973}
Step 2190: {'loss': 1.5466, 'grad_norm': 3.3740131855010986, 'learning_rate': 3.959402928313368e-05, 'epoch': 0.6246434683399886}
Step 2200: {'loss': 1.3521, 'grad_norm': 2.6291418075561523, 'learning_rate': 3.954649172846549e-05, 'epoch': 0.6274957216200798}
Step 2210: {'loss': 1.4264, 'grad_norm': 3.0443787574768066, 'learning_rate': 3.949895417379731e-05, 'epoch': 0.6303479749001711}
Step 2220: {'loss': 1.4862, 'grad_norm': 3.1021065711975098, 'learning_rate': 3.9451416619129114e-05, 'epoch': 0.6332002281802624}
Step 2230: {'loss': 1.3253, 'grad_norm': 3.280233383178711, 'learning_rate': 3.940387906446092e-05, 'epoch': 0.6360524814603536}
Step 2240: {'loss': 1.5754, 'grad_norm': 2.6848232746124268, 'learning_rate': 3.9356341509792736e-05, 'epoch': 0.638904734740445}
Step 2250: {'loss': 1.3032, 'grad_norm': 3.0504961013793945, 'learning_rate': 3.930880395512455e-05, 'epoch': 0.6417569880205363}
Step 2260: {'loss': 1.4143, 'grad_norm': 3.02388858795166, 'learning_rate': 3.9261266400456364e-05, 'epoch': 0.6446092413006275}
Step 2270: {'loss': 1.4425, 'grad_norm': 3.067750930786133, 'learning_rate': 3.921372884578817e-05, 'epoch': 0.6474614945807188}
Step 2280: {'loss': 1.4904, 'grad_norm': 2.811913251876831, 'learning_rate': 3.9166191291119986e-05, 'epoch': 0.6503137478608101}
Step 2290: {'loss': 1.3463, 'grad_norm': 3.234977960586548, 'learning_rate': 3.91186537364518e-05, 'epoch': 0.6531660011409013}
Step 2300: {'loss': 1.4319, 'grad_norm': 3.453425645828247, 'learning_rate': 3.9071116181783614e-05, 'epoch': 0.6560182544209926}
Step 2310: {'loss': 1.5045, 'grad_norm': 3.002321720123291, 'learning_rate': 3.902357862711543e-05, 'epoch': 0.6588705077010839}
Step 2320: {'loss': 1.4631, 'grad_norm': 3.031794309616089, 'learning_rate': 3.8976041072447236e-05, 'epoch': 0.6617227609811751}
Step 2330: {'loss': 1.5181, 'grad_norm': 3.6825637817382812, 'learning_rate': 3.892850351777905e-05, 'epoch': 0.6645750142612664}
Step 2340: {'loss': 1.4593, 'grad_norm': 3.0559475421905518, 'learning_rate': 3.888096596311086e-05, 'epoch': 0.6674272675413577}
Step 2350: {'loss': 1.45, 'grad_norm': 2.5561435222625732, 'learning_rate': 3.883342840844267e-05, 'epoch': 0.6702795208214489}
Step 2360: {'loss': 1.3732, 'grad_norm': 2.8890304565429688, 'learning_rate': 3.878589085377448e-05, 'epoch': 0.6731317741015402}
Step 2370: {'loss': 1.4823, 'grad_norm': 2.829070568084717, 'learning_rate': 3.8738353299106293e-05, 'epoch': 0.6759840273816314}
Step 2380: {'loss': 1.4881, 'grad_norm': 2.7087011337280273, 'learning_rate': 3.869081574443811e-05, 'epoch': 0.6788362806617227}
Step 2390: {'loss': 1.4543, 'grad_norm': 2.5724563598632812, 'learning_rate': 3.864327818976992e-05, 'epoch': 0.6816885339418141}
Step 2400: {'loss': 1.4824, 'grad_norm': 3.0188231468200684, 'learning_rate': 3.8595740635101736e-05, 'epoch': 0.6845407872219053}
Step 2410: {'loss': 1.3658, 'grad_norm': 2.392190933227539, 'learning_rate': 3.8548203080433543e-05, 'epoch': 0.6873930405019966}
Step 2420: {'loss': 1.4548, 'grad_norm': 4.360576152801514, 'learning_rate': 3.850066552576536e-05, 'epoch': 0.6902452937820879}
Step 2430: {'loss': 1.4601, 'grad_norm': 2.650916814804077, 'learning_rate': 3.845312797109717e-05, 'epoch': 0.6930975470621791}
Step 2440: {'loss': 1.3665, 'grad_norm': 2.710193634033203, 'learning_rate': 3.840559041642898e-05, 'epoch': 0.6959498003422704}
Step 2450: {'loss': 1.5149, 'grad_norm': 3.110095500946045, 'learning_rate': 3.8358052861760794e-05, 'epoch': 0.6988020536223617}
Step 2460: {'loss': 1.3838, 'grad_norm': 2.6397204399108887, 'learning_rate': 3.83105153070926e-05, 'epoch': 0.7016543069024529}
Step 2470: {'loss': 1.4751, 'grad_norm': 3.1146819591522217, 'learning_rate': 3.8262977752424415e-05, 'epoch': 0.7045065601825442}
Step 2480: {'loss': 1.4151, 'grad_norm': 2.398543119430542, 'learning_rate': 3.821544019775623e-05, 'epoch': 0.7073588134626355}
Step 2490: {'loss': 1.5245, 'grad_norm': 2.460160493850708, 'learning_rate': 3.8167902643088044e-05, 'epoch': 0.7102110667427267}
Step 2500: {'loss': 1.4475, 'grad_norm': 2.4329843521118164, 'learning_rate': 3.812036508841985e-05, 'epoch': 0.713063320022818}
Step 2510: {'loss': 1.4203, 'grad_norm': 2.827270030975342, 'learning_rate': 3.8072827533751665e-05, 'epoch': 0.7159155733029093}
Step 2520: {'loss': 1.4288, 'grad_norm': 3.0862877368927, 'learning_rate': 3.802528997908348e-05, 'epoch': 0.7187678265830005}
Step 2530: {'loss': 1.5204, 'grad_norm': 2.926259994506836, 'learning_rate': 3.7977752424415294e-05, 'epoch': 0.7216200798630918}
Step 2540: {'loss': 1.3758, 'grad_norm': 2.804462194442749, 'learning_rate': 3.79302148697471e-05, 'epoch': 0.7244723331431832}
Step 2550: {'loss': 1.4729, 'grad_norm': 3.521737813949585, 'learning_rate': 3.7882677315078915e-05, 'epoch': 0.7273245864232744}
Step 2560: {'loss': 1.3065, 'grad_norm': 3.1914596557617188, 'learning_rate': 3.783513976041072e-05, 'epoch': 0.7301768397033657}
Step 2570: {'loss': 1.4338, 'grad_norm': 3.8387467861175537, 'learning_rate': 3.778760220574254e-05, 'epoch': 0.733029092983457}
Step 2580: {'loss': 1.3899, 'grad_norm': 3.027364492416382, 'learning_rate': 3.774006465107435e-05, 'epoch': 0.7358813462635482}
Step 2590: {'loss': 1.4265, 'grad_norm': 2.8583483695983887, 'learning_rate': 3.769252709640616e-05, 'epoch': 0.7387335995436395}
Step 2600: {'loss': 1.4433, 'grad_norm': 2.9647469520568848, 'learning_rate': 3.764498954173797e-05, 'epoch': 0.7415858528237308}
Step 2610: {'loss': 1.3739, 'grad_norm': 2.8756659030914307, 'learning_rate': 3.759745198706979e-05, 'epoch': 0.744438106103822}
Step 2620: {'loss': 1.3811, 'grad_norm': 3.1895642280578613, 'learning_rate': 3.75499144324016e-05, 'epoch': 0.7472903593839133}
Step 2630: {'loss': 1.3854, 'grad_norm': 3.16045880317688, 'learning_rate': 3.7502376877733415e-05, 'epoch': 0.7501426126640046}
Step 2640: {'loss': 1.3906, 'grad_norm': 3.287538528442383, 'learning_rate': 3.745483932306522e-05, 'epoch': 0.7529948659440958}
Step 2650: {'loss': 1.5356, 'grad_norm': 2.3229644298553467, 'learning_rate': 3.740730176839704e-05, 'epoch': 0.7558471192241871}
Step 2660: {'loss': 1.4207, 'grad_norm': 3.462834358215332, 'learning_rate': 3.735976421372885e-05, 'epoch': 0.7586993725042784}
Step 2670: {'loss': 1.3049, 'grad_norm': 2.0865371227264404, 'learning_rate': 3.731222665906066e-05, 'epoch': 0.7615516257843696}
Step 2680: {'loss': 1.5812, 'grad_norm': 3.203509569168091, 'learning_rate': 3.7264689104392466e-05, 'epoch': 0.764403879064461}
Step 2690: {'loss': 1.464, 'grad_norm': 3.364448070526123, 'learning_rate': 3.721715154972428e-05, 'epoch': 0.7672561323445521}
Step 2700: {'loss': 1.3447, 'grad_norm': 3.7845542430877686, 'learning_rate': 3.7169613995056095e-05, 'epoch': 0.7701083856246435}
Step 2710: {'loss': 1.3554, 'grad_norm': 2.4962656497955322, 'learning_rate': 3.712207644038791e-05, 'epoch': 0.7729606389047348}
Step 2720: {'loss': 1.4318, 'grad_norm': 2.7820804119110107, 'learning_rate': 3.707453888571972e-05, 'epoch': 0.775812892184826}
Step 2730: {'loss': 1.4909, 'grad_norm': 2.354478120803833, 'learning_rate': 3.702700133105153e-05, 'epoch': 0.7786651454649173}
Step 2740: {'loss': 1.2871, 'grad_norm': 2.452082395553589, 'learning_rate': 3.6979463776383345e-05, 'epoch': 0.7815173987450086}
Step 2750: {'loss': 1.5018, 'grad_norm': 3.153261423110962, 'learning_rate': 3.693192622171516e-05, 'epoch': 0.7843696520250998}
Step 2760: {'loss': 1.3551, 'grad_norm': 2.7141854763031006, 'learning_rate': 3.688438866704697e-05, 'epoch': 0.7872219053051911}
Step 2770: {'loss': 1.3494, 'grad_norm': 2.305936098098755, 'learning_rate': 3.683685111237878e-05, 'epoch': 0.7900741585852824}
Step 2780: {'loss': 1.5284, 'grad_norm': 2.8536081314086914, 'learning_rate': 3.6789313557710595e-05, 'epoch': 0.7929264118653736}
Step 2790: {'loss': 1.3965, 'grad_norm': 3.2256786823272705, 'learning_rate': 3.67417760030424e-05, 'epoch': 0.7957786651454649}
Step 2800: {'loss': 1.4903, 'grad_norm': 3.6984660625457764, 'learning_rate': 3.6694238448374216e-05, 'epoch': 0.7986309184255562}
Step 2810: {'loss': 1.4087, 'grad_norm': 2.769005060195923, 'learning_rate': 3.664670089370603e-05, 'epoch': 0.8014831717056474}
Step 2820: {'loss': 1.4389, 'grad_norm': 2.835050344467163, 'learning_rate': 3.659916333903784e-05, 'epoch': 0.8043354249857387}
Step 2830: {'loss': 1.4502, 'grad_norm': 2.7500007152557373, 'learning_rate': 3.655162578436965e-05, 'epoch': 0.80718767826583}
Step 2840: {'loss': 1.3541, 'grad_norm': 2.6841304302215576, 'learning_rate': 3.6504088229701467e-05, 'epoch': 0.8100399315459212}
Step 2850: {'loss': 1.5542, 'grad_norm': 2.465369939804077, 'learning_rate': 3.645655067503328e-05, 'epoch': 0.8128921848260126}
Step 2860: {'loss': 1.4135, 'grad_norm': 2.9616565704345703, 'learning_rate': 3.640901312036509e-05, 'epoch': 0.8157444381061039}
Step 2870: {'loss': 1.5163, 'grad_norm': 3.186145782470703, 'learning_rate': 3.63614755656969e-05, 'epoch': 0.8185966913861951}
Step 2880: {'loss': 1.3121, 'grad_norm': 2.501772165298462, 'learning_rate': 3.6313938011028717e-05, 'epoch': 0.8214489446662864}
Step 2890: {'loss': 1.336, 'grad_norm': 2.2983086109161377, 'learning_rate': 3.626640045636053e-05, 'epoch': 0.8243011979463777}
Step 2900: {'loss': 1.3896, 'grad_norm': 2.799046039581299, 'learning_rate': 3.621886290169234e-05, 'epoch': 0.8271534512264689}
Step 2910: {'loss': 1.3886, 'grad_norm': 2.632204055786133, 'learning_rate': 3.6171325347024146e-05, 'epoch': 0.8300057045065602}
Step 2920: {'loss': 1.4756, 'grad_norm': 3.836721897125244, 'learning_rate': 3.612378779235596e-05, 'epoch': 0.8328579577866515}
Step 2930: {'loss': 1.2848, 'grad_norm': 2.7629787921905518, 'learning_rate': 3.6076250237687774e-05, 'epoch': 0.8357102110667427}
Step 2940: {'loss': 1.4054, 'grad_norm': 3.2979235649108887, 'learning_rate': 3.602871268301959e-05, 'epoch': 0.838562464346834}
Step 2950: {'loss': 1.4177, 'grad_norm': 3.1492831707000732, 'learning_rate': 3.59811751283514e-05, 'epoch': 0.8414147176269253}
Step 2960: {'loss': 1.4335, 'grad_norm': 2.701650381088257, 'learning_rate': 3.593363757368321e-05, 'epoch': 0.8442669709070165}
Step 2970: {'loss': 1.3953, 'grad_norm': 2.8276355266571045, 'learning_rate': 3.5886100019015024e-05, 'epoch': 0.8471192241871078}
Step 2980: {'loss': 1.5648, 'grad_norm': 3.0070579051971436, 'learning_rate': 3.583856246434684e-05, 'epoch': 0.849971477467199}
Step 2990: {'loss': 1.3525, 'grad_norm': 2.472914695739746, 'learning_rate': 3.579102490967865e-05, 'epoch': 0.8528237307472903}
Step 3000: {'loss': 1.4653, 'grad_norm': 3.452327013015747, 'learning_rate': 3.574348735501046e-05, 'epoch': 0.8556759840273817}
Step 3010: {'loss': 1.2509, 'grad_norm': 3.2121684551239014, 'learning_rate': 3.5695949800342274e-05, 'epoch': 0.8585282373074729}
Step 3020: {'loss': 1.4022, 'grad_norm': 2.763606309890747, 'learning_rate': 3.564841224567408e-05, 'epoch': 0.8613804905875642}
Step 3030: {'loss': 1.3718, 'grad_norm': 3.0051817893981934, 'learning_rate': 3.5600874691005896e-05, 'epoch': 0.8642327438676555}
Step 3040: {'loss': 1.3961, 'grad_norm': 3.066343307495117, 'learning_rate': 3.555333713633771e-05, 'epoch': 0.8670849971477467}
Step 3050: {'loss': 1.5011, 'grad_norm': 2.5535762310028076, 'learning_rate': 3.550579958166952e-05, 'epoch': 0.869937250427838}
Step 3060: {'loss': 1.3816, 'grad_norm': 3.259103298187256, 'learning_rate': 3.545826202700133e-05, 'epoch': 0.8727895037079293}
Step 3070: {'loss': 1.4256, 'grad_norm': 2.6014046669006348, 'learning_rate': 3.5410724472333146e-05, 'epoch': 0.8756417569880205}
Step 3080: {'loss': 1.5401, 'grad_norm': 3.6996607780456543, 'learning_rate': 3.536318691766496e-05, 'epoch': 0.8784940102681118}
Step 3090: {'loss': 1.5465, 'grad_norm': 3.5841846466064453, 'learning_rate': 3.531564936299677e-05, 'epoch': 0.8813462635482031}
Step 3100: {'loss': 1.3396, 'grad_norm': 3.20108699798584, 'learning_rate': 3.526811180832858e-05, 'epoch': 0.8841985168282943}
Step 3110: {'loss': 1.4537, 'grad_norm': 3.2051854133605957, 'learning_rate': 3.5220574253660396e-05, 'epoch': 0.8870507701083856}
Step 3120: {'loss': 1.4297, 'grad_norm': 3.1673648357391357, 'learning_rate': 3.5173036698992203e-05, 'epoch': 0.8899030233884769}
Step 3130: {'loss': 1.3792, 'grad_norm': 2.780371904373169, 'learning_rate': 3.512549914432402e-05, 'epoch': 0.8927552766685681}
Step 3140: {'loss': 1.2793, 'grad_norm': 2.5558364391326904, 'learning_rate': 3.5077961589655825e-05, 'epoch': 0.8956075299486594}
Step 3150: {'loss': 1.452, 'grad_norm': 2.890190362930298, 'learning_rate': 3.503042403498764e-05, 'epoch': 0.8984597832287508}
Step 3160: {'loss': 1.4079, 'grad_norm': 2.639936685562134, 'learning_rate': 3.4982886480319454e-05, 'epoch': 0.901312036508842}
Step 3170: {'loss': 1.3898, 'grad_norm': 2.5683400630950928, 'learning_rate': 3.493534892565127e-05, 'epoch': 0.9041642897889333}
Step 3180: {'loss': 1.2786, 'grad_norm': 2.9554224014282227, 'learning_rate': 3.4887811370983075e-05, 'epoch': 0.9070165430690246}
Step 3190: {'loss': 1.4321, 'grad_norm': 3.15506649017334, 'learning_rate': 3.484027381631489e-05, 'epoch': 0.9098687963491158}
Step 3200: {'loss': 1.3092, 'grad_norm': 2.4752604961395264, 'learning_rate': 3.4792736261646704e-05, 'epoch': 0.9127210496292071}
Step 3210: {'loss': 1.391, 'grad_norm': 2.6912412643432617, 'learning_rate': 3.474519870697852e-05, 'epoch': 0.9155733029092984}
Step 3220: {'loss': 1.3214, 'grad_norm': 2.8914763927459717, 'learning_rate': 3.469766115231033e-05, 'epoch': 0.9184255561893896}
Step 3230: {'loss': 1.4249, 'grad_norm': 2.4984018802642822, 'learning_rate': 3.465012359764214e-05, 'epoch': 0.9212778094694809}
Step 3240: {'loss': 1.3912, 'grad_norm': 2.880552291870117, 'learning_rate': 3.460258604297395e-05, 'epoch': 0.9241300627495722}
Step 3250: {'loss': 1.3638, 'grad_norm': 2.9074506759643555, 'learning_rate': 3.455504848830576e-05, 'epoch': 0.9269823160296634}
Step 3260: {'loss': 1.3025, 'grad_norm': 2.861541509628296, 'learning_rate': 3.4507510933637575e-05, 'epoch': 0.9298345693097547}
Step 3270: {'loss': 1.4445, 'grad_norm': 3.00142502784729, 'learning_rate': 3.445997337896939e-05, 'epoch': 0.932686822589846}
Step 3280: {'loss': 1.4261, 'grad_norm': 2.6384284496307373, 'learning_rate': 3.44124358243012e-05, 'epoch': 0.9355390758699372}
Step 3290: {'loss': 1.3743, 'grad_norm': 2.758936643600464, 'learning_rate': 3.436489826963301e-05, 'epoch': 0.9383913291500285}
Step 3300: {'loss': 1.2844, 'grad_norm': 3.4342830181121826, 'learning_rate': 3.4317360714964825e-05, 'epoch': 0.9412435824301197}
Step 3310: {'loss': 1.3729, 'grad_norm': 2.849882125854492, 'learning_rate': 3.426982316029664e-05, 'epoch': 0.944095835710211}
Step 3320: {'loss': 1.4198, 'grad_norm': 2.7189762592315674, 'learning_rate': 3.422228560562845e-05, 'epoch': 0.9469480889903024}
Step 3330: {'loss': 1.3132, 'grad_norm': 3.054403781890869, 'learning_rate': 3.417474805096026e-05, 'epoch': 0.9498003422703936}
Step 3340: {'loss': 1.5348, 'grad_norm': 3.1907217502593994, 'learning_rate': 3.4127210496292076e-05, 'epoch': 0.9526525955504849}
Step 3350: {'loss': 1.3517, 'grad_norm': 2.7602334022521973, 'learning_rate': 3.407967294162388e-05, 'epoch': 0.9555048488305762}
Step 3360: {'loss': 1.3247, 'grad_norm': 2.8474628925323486, 'learning_rate': 3.40321353869557e-05, 'epoch': 0.9583571021106674}
Step 3370: {'loss': 1.2996, 'grad_norm': 2.870157241821289, 'learning_rate': 3.3984597832287505e-05, 'epoch': 0.9612093553907587}
Step 3380: {'loss': 1.4445, 'grad_norm': 3.23683500289917, 'learning_rate': 3.393706027761932e-05, 'epoch': 0.96406160867085}
Step 3390: {'loss': 1.4073, 'grad_norm': 2.947981357574463, 'learning_rate': 3.388952272295113e-05, 'epoch': 0.9669138619509412}
Step 3400: {'loss': 1.3393, 'grad_norm': 3.0590691566467285, 'learning_rate': 3.384198516828295e-05, 'epoch': 0.9697661152310325}
Step 3410: {'loss': 1.4023, 'grad_norm': 2.328054428100586, 'learning_rate': 3.3794447613614755e-05, 'epoch': 0.9726183685111238}
Step 3420: {'loss': 1.2631, 'grad_norm': 2.692218542098999, 'learning_rate': 3.374691005894657e-05, 'epoch': 0.975470621791215}
Step 3430: {'loss': 1.3193, 'grad_norm': 3.628089427947998, 'learning_rate': 3.369937250427838e-05, 'epoch': 0.9783228750713063}
Step 3440: {'loss': 1.4094, 'grad_norm': 2.9773709774017334, 'learning_rate': 3.36518349496102e-05, 'epoch': 0.9811751283513976}
Step 3450: {'loss': 1.2747, 'grad_norm': 2.8479459285736084, 'learning_rate': 3.3604297394942005e-05, 'epoch': 0.9840273816314888}
Step 3460: {'loss': 1.3737, 'grad_norm': 2.969708204269409, 'learning_rate': 3.355675984027382e-05, 'epoch': 0.9868796349115802}
Step 3470: {'loss': 1.4496, 'grad_norm': 2.8781187534332275, 'learning_rate': 3.3509222285605626e-05, 'epoch': 0.9897318881916715}
Step 3480: {'loss': 1.5217, 'grad_norm': 2.90327787399292, 'learning_rate': 3.346168473093744e-05, 'epoch': 0.9925841414717627}
Step 3490: {'loss': 1.3751, 'grad_norm': 2.478896141052246, 'learning_rate': 3.3414147176269255e-05, 'epoch': 0.995436394751854}
Step 3500: {'loss': 1.4179, 'grad_norm': 2.565596103668213, 'learning_rate': 3.336660962160106e-05, 'epoch': 0.9982886480319453}
Epoch 1.0 completed in 2302.91s
Step 3506: {'eval_loss': 1.315402865409851, 'eval_runtime': 68.7343, 'eval_samples_per_second': 22.667, 'eval_steps_per_second': 5.674, 'epoch': 1.0}
Step 3510: {'loss': 1.3636, 'grad_norm': 2.807744026184082, 'learning_rate': 3.3319072066932876e-05, 'epoch': 1.0011409013120365}
Step 3520: {'loss': 1.2741, 'grad_norm': 2.6545557975769043, 'learning_rate': 3.327153451226469e-05, 'epoch': 1.0039931545921277}
Step 3530: {'loss': 1.183, 'grad_norm': 2.9829118251800537, 'learning_rate': 3.3223996957596505e-05, 'epoch': 1.006845407872219}
Step 3540: {'loss': 1.2979, 'grad_norm': 3.2369067668914795, 'learning_rate': 3.317645940292832e-05, 'epoch': 1.0096976611523103}
Step 3550: {'loss': 1.3965, 'grad_norm': 2.7539432048797607, 'learning_rate': 3.3128921848260127e-05, 'epoch': 1.0125499144324015}
Step 3560: {'loss': 1.3031, 'grad_norm': 2.7487499713897705, 'learning_rate': 3.308138429359194e-05, 'epoch': 1.015402167712493}
Step 3570: {'loss': 1.388, 'grad_norm': 2.4244518280029297, 'learning_rate': 3.303384673892375e-05, 'epoch': 1.0182544209925841}
Step 3580: {'loss': 1.2241, 'grad_norm': 2.5526154041290283, 'learning_rate': 3.298630918425556e-05, 'epoch': 1.0211066742726753}
Step 3590: {'loss': 1.3, 'grad_norm': 3.218745470046997, 'learning_rate': 3.293877162958738e-05, 'epoch': 1.0239589275527667}
Step 3600: {'loss': 1.2057, 'grad_norm': 2.5148544311523438, 'learning_rate': 3.2891234074919184e-05, 'epoch': 1.026811180832858}
Step 3610: {'loss': 1.2183, 'grad_norm': 2.8510360717773438, 'learning_rate': 3.2843696520251e-05, 'epoch': 1.0296634341129491}
Step 3620: {'loss': 1.328, 'grad_norm': 2.788752794265747, 'learning_rate': 3.279615896558281e-05, 'epoch': 1.0325156873930406}
Step 3630: {'loss': 1.2526, 'grad_norm': 2.3670241832733154, 'learning_rate': 3.274862141091463e-05, 'epoch': 1.0353679406731318}
Step 3640: {'loss': 1.3092, 'grad_norm': 3.4399521350860596, 'learning_rate': 3.2701083856246434e-05, 'epoch': 1.038220193953223}
Step 3650: {'loss': 1.2362, 'grad_norm': 2.8094985485076904, 'learning_rate': 3.265354630157825e-05, 'epoch': 1.0410724472333144}
Step 3660: {'loss': 1.277, 'grad_norm': 3.2209835052490234, 'learning_rate': 3.260600874691006e-05, 'epoch': 1.0439247005134056}
Step 3670: {'loss': 1.2707, 'grad_norm': 2.55599045753479, 'learning_rate': 3.255847119224188e-05, 'epoch': 1.0467769537934968}
Step 3680: {'loss': 1.2991, 'grad_norm': 2.3602263927459717, 'learning_rate': 3.2510933637573684e-05, 'epoch': 1.0496292070735882}
Step 3690: {'loss': 1.2925, 'grad_norm': 2.9725887775421143, 'learning_rate': 3.24633960829055e-05, 'epoch': 1.0524814603536794}
Step 3700: {'loss': 1.1563, 'grad_norm': 2.721543550491333, 'learning_rate': 3.2415858528237306e-05, 'epoch': 1.0553337136337706}
Step 3710: {'loss': 1.2959, 'grad_norm': 2.8506274223327637, 'learning_rate': 3.236832097356912e-05, 'epoch': 1.058185966913862}
Step 3720: {'loss': 1.1732, 'grad_norm': 2.868370771408081, 'learning_rate': 3.2320783418900934e-05, 'epoch': 1.0610382201939532}
Step 3730: {'loss': 1.3981, 'grad_norm': 2.887474775314331, 'learning_rate': 3.227324586423274e-05, 'epoch': 1.0638904734740444}
Step 3740: {'loss': 1.3341, 'grad_norm': 2.896984577178955, 'learning_rate': 3.2225708309564556e-05, 'epoch': 1.0667427267541358}
Step 3750: {'loss': 1.4623, 'grad_norm': 2.861177921295166, 'learning_rate': 3.217817075489637e-05, 'epoch': 1.069594980034227}
Step 3760: {'loss': 1.3943, 'grad_norm': 2.613600730895996, 'learning_rate': 3.2130633200228184e-05, 'epoch': 1.0724472333143182}
Step 3770: {'loss': 1.2416, 'grad_norm': 3.1256916522979736, 'learning_rate': 3.208309564556e-05, 'epoch': 1.0752994865944097}
Step 3780: {'loss': 1.2907, 'grad_norm': 2.6982827186584473, 'learning_rate': 3.2035558090891806e-05, 'epoch': 1.0781517398745009}
Step 3790: {'loss': 1.2812, 'grad_norm': 2.4126172065734863, 'learning_rate': 3.198802053622362e-05, 'epoch': 1.081003993154592}
Step 3800: {'loss': 1.2557, 'grad_norm': 2.6296956539154053, 'learning_rate': 3.194048298155543e-05, 'epoch': 1.0838562464346835}
Step 3810: {'loss': 1.3115, 'grad_norm': 3.643578052520752, 'learning_rate': 3.189294542688724e-05, 'epoch': 1.0867084997147747}
Step 3820: {'loss': 1.2934, 'grad_norm': 2.9967758655548096, 'learning_rate': 3.184540787221905e-05, 'epoch': 1.0895607529948659}
Step 3830: {'loss': 1.2726, 'grad_norm': 2.597977638244629, 'learning_rate': 3.1797870317550864e-05, 'epoch': 1.0924130062749573}
Step 3840: {'loss': 1.2919, 'grad_norm': 2.792243003845215, 'learning_rate': 3.175033276288268e-05, 'epoch': 1.0952652595550485}
Step 3850: {'loss': 1.3849, 'grad_norm': 3.493299961090088, 'learning_rate': 3.170279520821449e-05, 'epoch': 1.0981175128351397}
Step 3860: {'loss': 1.2548, 'grad_norm': 2.776625394821167, 'learning_rate': 3.1655257653546306e-05, 'epoch': 1.1009697661152311}
Step 3870: {'loss': 1.1705, 'grad_norm': 2.8125576972961426, 'learning_rate': 3.1607720098878114e-05, 'epoch': 1.1038220193953223}
Step 3880: {'loss': 1.4202, 'grad_norm': 2.5208117961883545, 'learning_rate': 3.156018254420993e-05, 'epoch': 1.1066742726754135}
Step 3890: {'loss': 1.3174, 'grad_norm': 2.63411283493042, 'learning_rate': 3.151264498954174e-05, 'epoch': 1.109526525955505}
Step 3900: {'loss': 1.3999, 'grad_norm': 2.740633249282837, 'learning_rate': 3.1465107434873556e-05, 'epoch': 1.1123787792355961}
Step 3910: {'loss': 1.2777, 'grad_norm': 2.6550283432006836, 'learning_rate': 3.1417569880205364e-05, 'epoch': 1.1152310325156873}
Step 3920: {'loss': 1.2757, 'grad_norm': 3.070497512817383, 'learning_rate': 3.137003232553717e-05, 'epoch': 1.1180832857957788}
Step 3930: {'loss': 1.2623, 'grad_norm': 2.4591524600982666, 'learning_rate': 3.1322494770868985e-05, 'epoch': 1.12093553907587}
Step 3940: {'loss': 1.2734, 'grad_norm': 2.680185079574585, 'learning_rate': 3.12749572162008e-05, 'epoch': 1.1237877923559612}
Step 3950: {'loss': 1.2724, 'grad_norm': 2.3234312534332275, 'learning_rate': 3.1227419661532614e-05, 'epoch': 1.1266400456360526}
Step 3960: {'loss': 1.2419, 'grad_norm': 2.596252202987671, 'learning_rate': 3.117988210686442e-05, 'epoch': 1.1294922989161438}
Step 3970: {'loss': 1.1537, 'grad_norm': 2.7307589054107666, 'learning_rate': 3.1132344552196235e-05, 'epoch': 1.132344552196235}
Step 3980: {'loss': 1.509, 'grad_norm': 2.8721935749053955, 'learning_rate': 3.108480699752805e-05, 'epoch': 1.1351968054763262}
Step 3990: {'loss': 1.284, 'grad_norm': 3.120635747909546, 'learning_rate': 3.1037269442859864e-05, 'epoch': 1.1380490587564176}
Step 4000: {'loss': 1.3511, 'grad_norm': 3.738776206970215, 'learning_rate': 3.098973188819168e-05, 'epoch': 1.1409013120365088}
Step 4010: {'loss': 1.3307, 'grad_norm': 2.635843276977539, 'learning_rate': 3.0942194333523485e-05, 'epoch': 1.1437535653166}
Step 4020: {'loss': 1.4348, 'grad_norm': 2.9348692893981934, 'learning_rate': 3.08946567788553e-05, 'epoch': 1.1466058185966914}
Step 4030: {'loss': 1.29, 'grad_norm': 2.602858304977417, 'learning_rate': 3.084711922418711e-05, 'epoch': 1.1494580718767826}
Step 4040: {'loss': 1.3309, 'grad_norm': 3.347668170928955, 'learning_rate': 3.079958166951892e-05, 'epoch': 1.1523103251568738}
Step 4050: {'loss': 1.2278, 'grad_norm': 2.7987236976623535, 'learning_rate': 3.075204411485073e-05, 'epoch': 1.1551625784369652}
Step 4060: {'loss': 1.4008, 'grad_norm': 3.093461275100708, 'learning_rate': 3.070450656018254e-05, 'epoch': 1.1580148317170564}
Step 4070: {'loss': 1.2207, 'grad_norm': 2.4452545642852783, 'learning_rate': 3.065696900551436e-05, 'epoch': 1.1608670849971476}
Step 4080: {'loss': 1.3144, 'grad_norm': 2.908942937850952, 'learning_rate': 3.060943145084617e-05, 'epoch': 1.163719338277239}
Step 4090: {'loss': 1.2455, 'grad_norm': 3.3458950519561768, 'learning_rate': 3.0561893896177986e-05, 'epoch': 1.1665715915573303}
Step 4100: {'loss': 1.253, 'grad_norm': 3.4526543617248535, 'learning_rate': 3.0514356341509793e-05, 'epoch': 1.1694238448374215}
Step 4110: {'loss': 1.412, 'grad_norm': 3.114028215408325, 'learning_rate': 3.0466818786841604e-05, 'epoch': 1.1722760981175129}
Step 4120: {'loss': 1.3252, 'grad_norm': 3.503807544708252, 'learning_rate': 3.0419281232173418e-05, 'epoch': 1.175128351397604}
Step 4130: {'loss': 1.3245, 'grad_norm': 2.554741382598877, 'learning_rate': 3.0371743677505232e-05, 'epoch': 1.1779806046776953}
Step 4140: {'loss': 1.3729, 'grad_norm': 2.748643398284912, 'learning_rate': 3.032420612283704e-05, 'epoch': 1.1808328579577867}
Step 4150: {'loss': 1.419, 'grad_norm': 3.356785297393799, 'learning_rate': 3.0276668568168854e-05, 'epoch': 1.183685111237878}
Step 4160: {'loss': 1.2666, 'grad_norm': 2.8164451122283936, 'learning_rate': 3.0229131013500668e-05, 'epoch': 1.186537364517969}
Step 4170: {'loss': 1.2248, 'grad_norm': 3.0932798385620117, 'learning_rate': 3.018159345883248e-05, 'epoch': 1.1893896177980605}
Step 4180: {'loss': 1.2487, 'grad_norm': 3.277355432510376, 'learning_rate': 3.0134055904164293e-05, 'epoch': 1.1922418710781517}
Step 4190: {'loss': 1.2822, 'grad_norm': 2.7260398864746094, 'learning_rate': 3.00865183494961e-05, 'epoch': 1.195094124358243}
Step 4200: {'loss': 1.2922, 'grad_norm': 2.5165467262268066, 'learning_rate': 3.0038980794827915e-05, 'epoch': 1.1979463776383343}
Step 4210: {'loss': 1.3052, 'grad_norm': 3.668642044067383, 'learning_rate': 2.999144324015973e-05, 'epoch': 1.2007986309184255}
Step 4220: {'loss': 1.278, 'grad_norm': 2.7686734199523926, 'learning_rate': 2.994390568549154e-05, 'epoch': 1.2036508841985167}
Step 4230: {'loss': 1.2894, 'grad_norm': 3.125389337539673, 'learning_rate': 2.9896368130823354e-05, 'epoch': 1.2065031374786082}
Step 4240: {'loss': 1.3373, 'grad_norm': 3.0046932697296143, 'learning_rate': 2.984883057615516e-05, 'epoch': 1.2093553907586994}
Step 4250: {'loss': 1.2767, 'grad_norm': 2.681081533432007, 'learning_rate': 2.9801293021486976e-05, 'epoch': 1.2122076440387906}
Step 4260: {'loss': 1.3859, 'grad_norm': 2.7210421562194824, 'learning_rate': 2.975375546681879e-05, 'epoch': 1.215059897318882}
Step 4270: {'loss': 1.2815, 'grad_norm': 3.658559560775757, 'learning_rate': 2.97062179121506e-05, 'epoch': 1.2179121505989732}
Step 4280: {'loss': 1.3087, 'grad_norm': 2.8316845893859863, 'learning_rate': 2.965868035748241e-05, 'epoch': 1.2207644038790644}
Step 4290: {'loss': 1.2755, 'grad_norm': 2.2909889221191406, 'learning_rate': 2.9611142802814222e-05, 'epoch': 1.2236166571591558}
Step 4300: {'loss': 1.3331, 'grad_norm': 2.815269947052002, 'learning_rate': 2.9563605248146037e-05, 'epoch': 1.226468910439247}
Step 4310: {'loss': 1.2762, 'grad_norm': 2.8409671783447266, 'learning_rate': 2.951606769347785e-05, 'epoch': 1.2293211637193382}
Step 4320: {'loss': 1.3533, 'grad_norm': 2.8998942375183105, 'learning_rate': 2.9468530138809665e-05, 'epoch': 1.2321734169994296}
Step 4330: {'loss': 1.2309, 'grad_norm': 2.700479507446289, 'learning_rate': 2.9420992584141473e-05, 'epoch': 1.2350256702795208}
Step 4340: {'loss': 1.257, 'grad_norm': 3.243661403656006, 'learning_rate': 2.9373455029473283e-05, 'epoch': 1.237877923559612}
Step 4350: {'loss': 1.2994, 'grad_norm': 2.8713879585266113, 'learning_rate': 2.9325917474805098e-05, 'epoch': 1.2407301768397034}
Step 4360: {'loss': 1.3671, 'grad_norm': 3.237729787826538, 'learning_rate': 2.9278379920136912e-05, 'epoch': 1.2435824301197946}
Step 4370: {'loss': 1.3751, 'grad_norm': 3.0880706310272217, 'learning_rate': 2.923084236546872e-05, 'epoch': 1.2464346833998858}
Step 4380: {'loss': 1.1909, 'grad_norm': 2.8051533699035645, 'learning_rate': 2.9183304810800533e-05, 'epoch': 1.2492869366799773}
Step 4390: {'loss': 1.3101, 'grad_norm': 2.706127405166626, 'learning_rate': 2.9135767256132344e-05, 'epoch': 1.2521391899600685}
Step 4400: {'loss': 1.2281, 'grad_norm': 2.9601666927337646, 'learning_rate': 2.908822970146416e-05, 'epoch': 1.2549914432401597}
Step 4410: {'loss': 1.2242, 'grad_norm': 2.90818452835083, 'learning_rate': 2.9040692146795973e-05, 'epoch': 1.257843696520251}
Step 4420: {'loss': 1.2537, 'grad_norm': 2.6110920906066895, 'learning_rate': 2.899315459212778e-05, 'epoch': 1.2606959498003423}
Step 4430: {'loss': 1.2582, 'grad_norm': 2.4598770141601562, 'learning_rate': 2.8945617037459594e-05, 'epoch': 1.2635482030804335}
Step 4440: {'loss': 1.2642, 'grad_norm': 2.552874803543091, 'learning_rate': 2.889807948279141e-05, 'epoch': 1.266400456360525}
Step 4450: {'loss': 1.2723, 'grad_norm': 3.096334218978882, 'learning_rate': 2.885054192812322e-05, 'epoch': 1.269252709640616}
Step 4460: {'loss': 1.2075, 'grad_norm': 2.5953638553619385, 'learning_rate': 2.8803004373455027e-05, 'epoch': 1.2721049629207073}
Step 4470: {'loss': 1.2587, 'grad_norm': 2.8452606201171875, 'learning_rate': 2.875546681878684e-05, 'epoch': 1.2749572162007987}
Step 4480: {'loss': 1.262, 'grad_norm': 2.5483357906341553, 'learning_rate': 2.8707929264118655e-05, 'epoch': 1.27780946948089}
Step 4490: {'loss': 1.3009, 'grad_norm': 2.731687068939209, 'learning_rate': 2.866039170945047e-05, 'epoch': 1.2806617227609811}
Step 4500: {'loss': 1.2667, 'grad_norm': 2.3366613388061523, 'learning_rate': 2.861285415478228e-05, 'epoch': 1.2835139760410725}
Step 4510: {'loss': 1.3126, 'grad_norm': 2.9177005290985107, 'learning_rate': 2.8565316600114088e-05, 'epoch': 1.2863662293211637}
Step 4520: {'loss': 1.2617, 'grad_norm': 2.789552688598633, 'learning_rate': 2.8517779045445902e-05, 'epoch': 1.289218482601255}
Step 4530: {'loss': 1.2682, 'grad_norm': 2.766801595687866, 'learning_rate': 2.8470241490777716e-05, 'epoch': 1.2920707358813464}
Step 4540: {'loss': 1.2408, 'grad_norm': 3.186549186706543, 'learning_rate': 2.842270393610953e-05, 'epoch': 1.2949229891614376}
Step 4550: {'loss': 1.2998, 'grad_norm': 2.572312355041504, 'learning_rate': 2.837516638144134e-05, 'epoch': 1.2977752424415288}
Step 4560: {'loss': 1.3889, 'grad_norm': 2.896235227584839, 'learning_rate': 2.8327628826773152e-05, 'epoch': 1.3006274957216202}
Step 4570: {'loss': 1.3695, 'grad_norm': 2.8454411029815674, 'learning_rate': 2.8280091272104963e-05, 'epoch': 1.3034797490017114}
Step 4580: {'loss': 1.2484, 'grad_norm': 2.6895031929016113, 'learning_rate': 2.8232553717436777e-05, 'epoch': 1.3063320022818026}
Step 4590: {'loss': 1.3036, 'grad_norm': 2.9109153747558594, 'learning_rate': 2.818501616276859e-05, 'epoch': 1.309184255561894}
Step 4600: {'loss': 1.3583, 'grad_norm': 2.386676073074341, 'learning_rate': 2.81374786081004e-05, 'epoch': 1.3120365088419852}
Step 4610: {'loss': 1.2516, 'grad_norm': 3.0843346118927, 'learning_rate': 2.8089941053432213e-05, 'epoch': 1.3148887621220764}
Step 4620: {'loss': 1.4196, 'grad_norm': 2.822331666946411, 'learning_rate': 2.8042403498764024e-05, 'epoch': 1.3177410154021678}
Step 4630: {'loss': 1.3173, 'grad_norm': 2.46136736869812, 'learning_rate': 2.7994865944095838e-05, 'epoch': 1.320593268682259}
Step 4640: {'loss': 1.3196, 'grad_norm': 2.437859058380127, 'learning_rate': 2.7947328389427652e-05, 'epoch': 1.3234455219623502}
Step 4650: {'loss': 1.276, 'grad_norm': 2.8846235275268555, 'learning_rate': 2.789979083475946e-05, 'epoch': 1.3262977752424416}
Step 4660: {'loss': 1.2332, 'grad_norm': 3.0730507373809814, 'learning_rate': 2.7852253280091274e-05, 'epoch': 1.3291500285225328}
Step 4670: {'loss': 1.2928, 'grad_norm': 2.8444998264312744, 'learning_rate': 2.7804715725423085e-05, 'epoch': 1.332002281802624}
Step 4680: {'loss': 1.2789, 'grad_norm': 3.314100980758667, 'learning_rate': 2.77571781707549e-05, 'epoch': 1.3348545350827155}
Step 4690: {'loss': 1.3099, 'grad_norm': 2.5989837646484375, 'learning_rate': 2.7709640616086706e-05, 'epoch': 1.3377067883628067}
Step 4700: {'loss': 1.3093, 'grad_norm': 3.2832226753234863, 'learning_rate': 2.766210306141852e-05, 'epoch': 1.3405590416428979}
Step 4710: {'loss': 1.2678, 'grad_norm': 3.204491376876831, 'learning_rate': 2.7614565506750335e-05, 'epoch': 1.3434112949229893}
Step 4720: {'loss': 1.3812, 'grad_norm': 3.9466333389282227, 'learning_rate': 2.756702795208215e-05, 'epoch': 1.3462635482030805}
Step 4730: {'loss': 1.3115, 'grad_norm': 2.5112736225128174, 'learning_rate': 2.751949039741396e-05, 'epoch': 1.3491158014831717}
Step 4740: {'loss': 1.3459, 'grad_norm': 2.8058340549468994, 'learning_rate': 2.7471952842745767e-05, 'epoch': 1.351968054763263}
Step 4750: {'loss': 1.3157, 'grad_norm': 2.548205852508545, 'learning_rate': 2.742441528807758e-05, 'epoch': 1.3548203080433543}
Step 4760: {'loss': 1.3664, 'grad_norm': 2.9567484855651855, 'learning_rate': 2.7376877733409396e-05, 'epoch': 1.3576725613234455}
Step 4770: {'loss': 1.3025, 'grad_norm': 2.8476834297180176, 'learning_rate': 2.732934017874121e-05, 'epoch': 1.360524814603537}
Step 4780: {'loss': 1.2637, 'grad_norm': 2.868997812271118, 'learning_rate': 2.7281802624073017e-05, 'epoch': 1.3633770678836281}
Step 4790: {'loss': 1.4258, 'grad_norm': 2.8933701515197754, 'learning_rate': 2.7234265069404828e-05, 'epoch': 1.3662293211637193}
Step 4800: {'loss': 1.1885, 'grad_norm': 2.374732255935669, 'learning_rate': 2.7186727514736642e-05, 'epoch': 1.3690815744438107}
Step 4810: {'loss': 1.2824, 'grad_norm': 2.6399824619293213, 'learning_rate': 2.7139189960068456e-05, 'epoch': 1.371933827723902}
Step 4820: {'loss': 1.3259, 'grad_norm': 2.8271234035491943, 'learning_rate': 2.709165240540027e-05, 'epoch': 1.3747860810039931}
Step 4830: {'loss': 1.2588, 'grad_norm': 3.0365328788757324, 'learning_rate': 2.7044114850732078e-05, 'epoch': 1.3776383342840846}
Step 4840: {'loss': 1.1979, 'grad_norm': 2.6686251163482666, 'learning_rate': 2.6996577296063892e-05, 'epoch': 1.3804905875641758}
Step 4850: {'loss': 1.2671, 'grad_norm': 2.931072473526001, 'learning_rate': 2.6949039741395703e-05, 'epoch': 1.383342840844267}
Step 4860: {'loss': 1.3032, 'grad_norm': 3.320627212524414, 'learning_rate': 2.6901502186727517e-05, 'epoch': 1.3861950941243584}
Step 4870: {'loss': 1.3189, 'grad_norm': 2.957411527633667, 'learning_rate': 2.685396463205933e-05, 'epoch': 1.3890473474044496}
Step 4880: {'loss': 1.3721, 'grad_norm': 2.6881115436553955, 'learning_rate': 2.680642707739114e-05, 'epoch': 1.3918996006845408}
Step 4890: {'loss': 1.446, 'grad_norm': 3.6465559005737305, 'learning_rate': 2.6758889522722953e-05, 'epoch': 1.3947518539646322}
Step 4900: {'loss': 1.2518, 'grad_norm': 2.6714906692504883, 'learning_rate': 2.6711351968054764e-05, 'epoch': 1.3976041072447234}
Step 4910: {'loss': 1.3016, 'grad_norm': 3.142040491104126, 'learning_rate': 2.6663814413386578e-05, 'epoch': 1.4004563605248146}
Step 4920: {'loss': 1.3276, 'grad_norm': 2.6512389183044434, 'learning_rate': 2.6616276858718386e-05, 'epoch': 1.403308613804906}
Step 4930: {'loss': 1.1722, 'grad_norm': 2.36664080619812, 'learning_rate': 2.65687393040502e-05, 'epoch': 1.4061608670849972}
Step 4940: {'loss': 1.3508, 'grad_norm': 2.646028518676758, 'learning_rate': 2.6521201749382014e-05, 'epoch': 1.4090131203650884}
Step 4950: {'loss': 1.2311, 'grad_norm': 2.3987526893615723, 'learning_rate': 2.6473664194713825e-05, 'epoch': 1.4118653736451796}
Step 4960: {'loss': 1.2687, 'grad_norm': 2.8762147426605225, 'learning_rate': 2.642612664004564e-05, 'epoch': 1.414717626925271}
Step 4970: {'loss': 1.3599, 'grad_norm': 2.551440477371216, 'learning_rate': 2.6378589085377447e-05, 'epoch': 1.4175698802053622}
Step 4980: {'loss': 1.261, 'grad_norm': 2.428966999053955, 'learning_rate': 2.633105153070926e-05, 'epoch': 1.4204221334854534}
Step 4990: {'loss': 1.2281, 'grad_norm': 2.9071857929229736, 'learning_rate': 2.6283513976041075e-05, 'epoch': 1.4232743867655449}
Step 5000: {'loss': 1.3243, 'grad_norm': 2.7136332988739014, 'learning_rate': 2.623597642137289e-05, 'epoch': 1.426126640045636}
Step 5010: {'loss': 1.3172, 'grad_norm': 3.6586732864379883, 'learning_rate': 2.6188438866704697e-05, 'epoch': 1.4289788933257273}
Step 5020: {'loss': 1.3258, 'grad_norm': 2.7705769538879395, 'learning_rate': 2.6140901312036508e-05, 'epoch': 1.4318311466058187}
Step 5030: {'loss': 1.242, 'grad_norm': 2.939631700515747, 'learning_rate': 2.6093363757368322e-05, 'epoch': 1.4346833998859099}
Step 5040: {'loss': 1.3791, 'grad_norm': 2.310305595397949, 'learning_rate': 2.6045826202700136e-05, 'epoch': 1.437535653166001}
Step 5050: {'loss': 1.2748, 'grad_norm': 3.736712694168091, 'learning_rate': 2.599828864803195e-05, 'epoch': 1.4403879064460925}
Step 5060: {'loss': 1.3095, 'grad_norm': 2.658806324005127, 'learning_rate': 2.5950751093363758e-05, 'epoch': 1.4432401597261837}
Step 5070: {'loss': 1.1544, 'grad_norm': 2.7155940532684326, 'learning_rate': 2.590321353869557e-05, 'epoch': 1.446092413006275}
Step 5080: {'loss': 1.2819, 'grad_norm': 2.7503724098205566, 'learning_rate': 2.5855675984027383e-05, 'epoch': 1.4489446662863663}
Step 5090: {'loss': 1.288, 'grad_norm': 3.223501443862915, 'learning_rate': 2.5808138429359197e-05, 'epoch': 1.4517969195664575}
Step 5100: {'loss': 1.3646, 'grad_norm': 3.3497655391693115, 'learning_rate': 2.5760600874691004e-05, 'epoch': 1.4546491728465487}
Step 5110: {'loss': 1.1943, 'grad_norm': 2.865539789199829, 'learning_rate': 2.571306332002282e-05, 'epoch': 1.45750142612664}
Step 5120: {'loss': 1.2705, 'grad_norm': 3.060190439224243, 'learning_rate': 2.5665525765354633e-05, 'epoch': 1.4603536794067313}
Step 5130: {'loss': 1.1637, 'grad_norm': 2.900416851043701, 'learning_rate': 2.5617988210686444e-05, 'epoch': 1.4632059326868225}
Step 5140: {'loss': 1.3178, 'grad_norm': 3.148188829421997, 'learning_rate': 2.5570450656018258e-05, 'epoch': 1.4660581859669137}
Step 5150: {'loss': 1.2325, 'grad_norm': 3.7929744720458984, 'learning_rate': 2.5522913101350065e-05, 'epoch': 1.4689104392470052}
Step 5160: {'loss': 1.242, 'grad_norm': 3.102762460708618, 'learning_rate': 2.547537554668188e-05, 'epoch': 1.4717626925270964}
Step 5170: {'loss': 1.3443, 'grad_norm': 3.3466780185699463, 'learning_rate': 2.5427837992013694e-05, 'epoch': 1.4746149458071875}
Step 5180: {'loss': 1.2886, 'grad_norm': 2.9168238639831543, 'learning_rate': 2.5380300437345504e-05, 'epoch': 1.477467199087279}
Step 5190: {'loss': 1.3003, 'grad_norm': 3.044978141784668, 'learning_rate': 2.533276288267732e-05, 'epoch': 1.4803194523673702}
Step 5200: {'loss': 1.1755, 'grad_norm': 3.552821636199951, 'learning_rate': 2.5285225328009126e-05, 'epoch': 1.4831717056474614}
Step 5210: {'loss': 1.3045, 'grad_norm': 3.3488168716430664, 'learning_rate': 2.523768777334094e-05, 'epoch': 1.4860239589275528}
Step 5220: {'loss': 1.208, 'grad_norm': 2.8039722442626953, 'learning_rate': 2.5190150218672755e-05, 'epoch': 1.488876212207644}
Step 5230: {'loss': 1.2907, 'grad_norm': 3.424860715866089, 'learning_rate': 2.5142612664004565e-05, 'epoch': 1.4917284654877352}
Step 5240: {'loss': 1.2482, 'grad_norm': 2.885817766189575, 'learning_rate': 2.5095075109336376e-05, 'epoch': 1.4945807187678266}
Step 5250: {'loss': 1.2922, 'grad_norm': 2.7210075855255127, 'learning_rate': 2.5047537554668187e-05, 'epoch': 1.4974329720479178}
Step 5260: {'loss': 1.2612, 'grad_norm': 2.6324570178985596, 'learning_rate': 2.5e-05, 'epoch': 1.500285225328009}
Step 5270: {'loss': 1.2589, 'grad_norm': 3.1229872703552246, 'learning_rate': 2.4952462445331815e-05, 'epoch': 1.5031374786081004}
Step 5280: {'loss': 1.3308, 'grad_norm': 3.1549201011657715, 'learning_rate': 2.4904924890663626e-05, 'epoch': 1.5059897318881916}
Step 5290: {'loss': 1.1799, 'grad_norm': 2.587939500808716, 'learning_rate': 2.4857387335995437e-05, 'epoch': 1.5088419851682828}
Step 5300: {'loss': 1.3594, 'grad_norm': 2.7965211868286133, 'learning_rate': 2.4809849781327248e-05, 'epoch': 1.5116942384483743}
Step 5310: {'loss': 1.2133, 'grad_norm': 2.473722457885742, 'learning_rate': 2.4762312226659062e-05, 'epoch': 1.5145464917284654}
Step 5320: {'loss': 1.2196, 'grad_norm': 2.9947397708892822, 'learning_rate': 2.4714774671990873e-05, 'epoch': 1.5173987450085566}
Step 5330: {'loss': 1.231, 'grad_norm': 3.0898773670196533, 'learning_rate': 2.4667237117322687e-05, 'epoch': 1.520250998288648}
Step 5340: {'loss': 1.306, 'grad_norm': 3.2130625247955322, 'learning_rate': 2.4619699562654498e-05, 'epoch': 1.5231032515687393}
Step 5350: {'loss': 1.2098, 'grad_norm': 2.741607904434204, 'learning_rate': 2.457216200798631e-05, 'epoch': 1.5259555048488305}
Step 5360: {'loss': 1.3079, 'grad_norm': 2.63185715675354, 'learning_rate': 2.4524624453318123e-05, 'epoch': 1.528807758128922}
Step 5370: {'loss': 1.2963, 'grad_norm': 2.5098633766174316, 'learning_rate': 2.4477086898649934e-05, 'epoch': 1.531660011409013}
Step 5380: {'loss': 1.2904, 'grad_norm': 2.787482261657715, 'learning_rate': 2.4429549343981748e-05, 'epoch': 1.5345122646891043}
Step 5390: {'loss': 1.3455, 'grad_norm': 2.704416036605835, 'learning_rate': 2.438201178931356e-05, 'epoch': 1.5373645179691957}
Step 5400: {'loss': 1.3289, 'grad_norm': 2.3530781269073486, 'learning_rate': 2.433447423464537e-05, 'epoch': 1.540216771249287}
Step 5410: {'loss': 1.1889, 'grad_norm': 2.7240254878997803, 'learning_rate': 2.428693667997718e-05, 'epoch': 1.543069024529378}
Step 5420: {'loss': 1.2164, 'grad_norm': 2.4877564907073975, 'learning_rate': 2.4239399125308995e-05, 'epoch': 1.5459212778094695}
Step 5430: {'loss': 1.2882, 'grad_norm': 3.3804361820220947, 'learning_rate': 2.419186157064081e-05, 'epoch': 1.5487735310895607}
Step 5440: {'loss': 1.2482, 'grad_norm': 2.950068473815918, 'learning_rate': 2.414432401597262e-05, 'epoch': 1.551625784369652}
Step 5450: {'loss': 1.3, 'grad_norm': 2.9469385147094727, 'learning_rate': 2.4096786461304434e-05, 'epoch': 1.5544780376497433}
Step 5460: {'loss': 1.2189, 'grad_norm': 3.389840841293335, 'learning_rate': 2.404924890663624e-05, 'epoch': 1.5573302909298345}
Step 5470: {'loss': 1.3171, 'grad_norm': 2.533521890640259, 'learning_rate': 2.4001711351968056e-05, 'epoch': 1.5601825442099257}
Step 5480: {'loss': 1.3107, 'grad_norm': 2.6357662677764893, 'learning_rate': 2.3954173797299866e-05, 'epoch': 1.5630347974900172}
Step 5490: {'loss': 1.2572, 'grad_norm': 2.9669692516326904, 'learning_rate': 2.390663624263168e-05, 'epoch': 1.5658870507701084}
Step 5500: {'loss': 1.2911, 'grad_norm': 3.0313377380371094, 'learning_rate': 2.3859098687963495e-05, 'epoch': 1.5687393040501996}
Step 5510: {'loss': 1.2496, 'grad_norm': 2.87133526802063, 'learning_rate': 2.3811561133295306e-05, 'epoch': 1.571591557330291}
Step 5520: {'loss': 1.2836, 'grad_norm': 3.0471911430358887, 'learning_rate': 2.3764023578627117e-05, 'epoch': 1.5744438106103822}
Step 5530: {'loss': 1.3181, 'grad_norm': 3.0453879833221436, 'learning_rate': 2.3716486023958927e-05, 'epoch': 1.5772960638904734}
Step 5540: {'loss': 1.3127, 'grad_norm': 3.070533037185669, 'learning_rate': 2.366894846929074e-05, 'epoch': 1.5801483171705648}
Step 5550: {'loss': 1.393, 'grad_norm': 3.3076796531677246, 'learning_rate': 2.3621410914622552e-05, 'epoch': 1.583000570450656}
Step 5560: {'loss': 1.2464, 'grad_norm': 2.9963457584381104, 'learning_rate': 2.3573873359954367e-05, 'epoch': 1.5858528237307472}
Step 5570: {'loss': 1.2424, 'grad_norm': 2.389293909072876, 'learning_rate': 2.3526335805286177e-05, 'epoch': 1.5887050770108386}
Step 5580: {'loss': 1.2029, 'grad_norm': 2.922330141067505, 'learning_rate': 2.3478798250617988e-05, 'epoch': 1.5915573302909298}
Step 5590: {'loss': 1.2466, 'grad_norm': 2.957753896713257, 'learning_rate': 2.3431260695949802e-05, 'epoch': 1.594409583571021}
Step 5600: {'loss': 1.2918, 'grad_norm': 2.911421298980713, 'learning_rate': 2.3383723141281613e-05, 'epoch': 1.5972618368511124}
Step 5610: {'loss': 1.3771, 'grad_norm': 2.5220327377319336, 'learning_rate': 2.3336185586613427e-05, 'epoch': 1.6001140901312036}
Step 5620: {'loss': 1.3112, 'grad_norm': 3.840855598449707, 'learning_rate': 2.328864803194524e-05, 'epoch': 1.6029663434112948}
Step 5630: {'loss': 1.2656, 'grad_norm': 2.900749444961548, 'learning_rate': 2.324111047727705e-05, 'epoch': 1.6058185966913863}
Step 5640: {'loss': 1.3198, 'grad_norm': 2.9320523738861084, 'learning_rate': 2.319357292260886e-05, 'epoch': 1.6086708499714775}
Step 5650: {'loss': 1.3457, 'grad_norm': 3.4983768463134766, 'learning_rate': 2.3146035367940674e-05, 'epoch': 1.6115231032515687}
Step 5660: {'loss': 1.3083, 'grad_norm': 3.123901605606079, 'learning_rate': 2.309849781327249e-05, 'epoch': 1.61437535653166}
Step 5670: {'loss': 1.3015, 'grad_norm': 2.979308605194092, 'learning_rate': 2.30509602586043e-05, 'epoch': 1.6172276098117513}
Step 5680: {'loss': 1.2857, 'grad_norm': 2.895617961883545, 'learning_rate': 2.300342270393611e-05, 'epoch': 1.6200798630918425}
Step 5690: {'loss': 1.2876, 'grad_norm': 2.0439984798431396, 'learning_rate': 2.295588514926792e-05, 'epoch': 1.622932116371934}
Step 5700: {'loss': 1.2919, 'grad_norm': 2.905729055404663, 'learning_rate': 2.2908347594599735e-05, 'epoch': 1.625784369652025}
Step 5710: {'loss': 1.2741, 'grad_norm': 2.7424569129943848, 'learning_rate': 2.2860810039931546e-05, 'epoch': 1.6286366229321163}
Step 5720: {'loss': 1.2055, 'grad_norm': 3.0049872398376465, 'learning_rate': 2.281327248526336e-05, 'epoch': 1.6314888762122077}
Step 5730: {'loss': 1.3615, 'grad_norm': 2.8997678756713867, 'learning_rate': 2.276573493059517e-05, 'epoch': 1.634341129492299}
Step 5740: {'loss': 1.3174, 'grad_norm': 2.533003091812134, 'learning_rate': 2.2718197375926982e-05, 'epoch': 1.6371933827723901}
Step 5750: {'loss': 1.3768, 'grad_norm': 2.7844367027282715, 'learning_rate': 2.2670659821258796e-05, 'epoch': 1.6400456360524815}
Step 5760: {'loss': 1.291, 'grad_norm': 2.6831769943237305, 'learning_rate': 2.2623122266590607e-05, 'epoch': 1.6428978893325727}
Step 5770: {'loss': 1.3039, 'grad_norm': 2.9033684730529785, 'learning_rate': 2.257558471192242e-05, 'epoch': 1.645750142612664}
Step 5780: {'loss': 1.3461, 'grad_norm': 2.3163018226623535, 'learning_rate': 2.2528047157254232e-05, 'epoch': 1.6486023958927554}
Step 5790: {'loss': 1.1678, 'grad_norm': 2.6402409076690674, 'learning_rate': 2.2480509602586046e-05, 'epoch': 1.6514546491728466}
Step 5800: {'loss': 1.3273, 'grad_norm': 3.049480676651001, 'learning_rate': 2.2432972047917853e-05, 'epoch': 1.6543069024529378}
Step 5810: {'loss': 1.23, 'grad_norm': 2.675806999206543, 'learning_rate': 2.2385434493249668e-05, 'epoch': 1.6571591557330292}
Step 5820: {'loss': 1.3562, 'grad_norm': 2.7519173622131348, 'learning_rate': 2.2337896938581482e-05, 'epoch': 1.6600114090131204}
Step 5830: {'loss': 1.3077, 'grad_norm': 3.1381378173828125, 'learning_rate': 2.2290359383913293e-05, 'epoch': 1.6628636622932116}
Step 5840: {'loss': 1.3031, 'grad_norm': 2.5912184715270996, 'learning_rate': 2.2242821829245107e-05, 'epoch': 1.665715915573303}
Step 5850: {'loss': 1.2279, 'grad_norm': 3.229994535446167, 'learning_rate': 2.2195284274576918e-05, 'epoch': 1.6685681688533942}
Step 5860: {'loss': 1.278, 'grad_norm': 3.168797492980957, 'learning_rate': 2.214774671990873e-05, 'epoch': 1.6714204221334854}
Step 5870: {'loss': 1.3423, 'grad_norm': 2.798417091369629, 'learning_rate': 2.210020916524054e-05, 'epoch': 1.6742726754135768}
Step 5880: {'loss': 1.3776, 'grad_norm': 2.7300162315368652, 'learning_rate': 2.2052671610572354e-05, 'epoch': 1.677124928693668}
Step 5890: {'loss': 1.3461, 'grad_norm': 3.4603688716888428, 'learning_rate': 2.2005134055904164e-05, 'epoch': 1.6799771819737592}
Step 5900: {'loss': 1.3021, 'grad_norm': 2.5173208713531494, 'learning_rate': 2.195759650123598e-05, 'epoch': 1.6828294352538506}
Step 5910: {'loss': 1.2488, 'grad_norm': 2.5349700450897217, 'learning_rate': 2.191005894656779e-05, 'epoch': 1.6856816885339418}
Step 5920: {'loss': 1.3185, 'grad_norm': 2.782090425491333, 'learning_rate': 2.18625213918996e-05, 'epoch': 1.688533941814033}
Step 5930: {'loss': 1.3521, 'grad_norm': 2.675492525100708, 'learning_rate': 2.1814983837231415e-05, 'epoch': 1.6913861950941245}
Step 5940: {'loss': 1.4236, 'grad_norm': 3.0553195476531982, 'learning_rate': 2.1767446282563225e-05, 'epoch': 1.6942384483742157}
Step 5950: {'loss': 1.3796, 'grad_norm': 3.3255176544189453, 'learning_rate': 2.171990872789504e-05, 'epoch': 1.6970907016543069}
Step 5960: {'loss': 1.2398, 'grad_norm': 2.6138031482696533, 'learning_rate': 2.167237117322685e-05, 'epoch': 1.6999429549343983}
Step 5970: {'loss': 1.1561, 'grad_norm': 2.727879047393799, 'learning_rate': 2.162483361855866e-05, 'epoch': 1.7027952082144895}
Step 5980: {'loss': 1.3006, 'grad_norm': 3.606942892074585, 'learning_rate': 2.1577296063890475e-05, 'epoch': 1.7056474614945807}
Step 5990: {'loss': 1.2335, 'grad_norm': 3.0408990383148193, 'learning_rate': 2.1529758509222286e-05, 'epoch': 1.708499714774672}
Step 6000: {'loss': 1.3278, 'grad_norm': 2.643319606781006, 'learning_rate': 2.14822209545541e-05, 'epoch': 1.7113519680547633}
Step 6010: {'loss': 1.3843, 'grad_norm': 3.5362205505371094, 'learning_rate': 2.143468339988591e-05, 'epoch': 1.7142042213348545}
Step 6020: {'loss': 1.2814, 'grad_norm': 3.538541555404663, 'learning_rate': 2.1387145845217722e-05, 'epoch': 1.717056474614946}
Step 6030: {'loss': 1.1929, 'grad_norm': 2.278733730316162, 'learning_rate': 2.1339608290549533e-05, 'epoch': 1.7199087278950371}
Step 6040: {'loss': 1.3243, 'grad_norm': 3.0559241771698, 'learning_rate': 2.1292070735881347e-05, 'epoch': 1.7227609811751283}
Step 6050: {'loss': 1.241, 'grad_norm': 2.729731559753418, 'learning_rate': 2.1244533181213158e-05, 'epoch': 1.7256132344552197}
Step 6060: {'loss': 1.332, 'grad_norm': 3.084379196166992, 'learning_rate': 2.1196995626544972e-05, 'epoch': 1.728465487735311}
Step 6070: {'loss': 1.2741, 'grad_norm': 2.7668230533599854, 'learning_rate': 2.1149458071876786e-05, 'epoch': 1.7313177410154021}
Step 6080: {'loss': 1.3447, 'grad_norm': 2.5904641151428223, 'learning_rate': 2.1101920517208594e-05, 'epoch': 1.7341699942954936}
Step 6090: {'loss': 1.3057, 'grad_norm': 2.9350221157073975, 'learning_rate': 2.1054382962540408e-05, 'epoch': 1.7370222475755848}
Step 6100: {'loss': 1.2901, 'grad_norm': 2.803931474685669, 'learning_rate': 2.100684540787222e-05, 'epoch': 1.739874500855676}
Step 6110: {'loss': 1.3075, 'grad_norm': 2.7896900177001953, 'learning_rate': 2.0959307853204033e-05, 'epoch': 1.7427267541357674}
Step 6120: {'loss': 1.2955, 'grad_norm': 3.0183658599853516, 'learning_rate': 2.0911770298535844e-05, 'epoch': 1.7455790074158584}
Step 6130: {'loss': 1.1934, 'grad_norm': 2.741696357727051, 'learning_rate': 2.0864232743867658e-05, 'epoch': 1.7484312606959498}
Step 6140: {'loss': 1.2002, 'grad_norm': 2.4508790969848633, 'learning_rate': 2.081669518919947e-05, 'epoch': 1.7512835139760412}
Step 6150: {'loss': 1.3006, 'grad_norm': 2.954927921295166, 'learning_rate': 2.076915763453128e-05, 'epoch': 1.7541357672561322}
Step 6160: {'loss': 1.2733, 'grad_norm': 2.752387046813965, 'learning_rate': 2.0721620079863094e-05, 'epoch': 1.7569880205362236}
Step 6170: {'loss': 1.2042, 'grad_norm': 2.403428554534912, 'learning_rate': 2.0674082525194905e-05, 'epoch': 1.759840273816315}
Step 6180: {'loss': 1.1774, 'grad_norm': 2.3075780868530273, 'learning_rate': 2.062654497052672e-05, 'epoch': 1.762692527096406}
Step 6190: {'loss': 1.231, 'grad_norm': 2.5425667762756348, 'learning_rate': 2.057900741585853e-05, 'epoch': 1.7655447803764974}
Step 6200: {'loss': 1.3002, 'grad_norm': 2.7312557697296143, 'learning_rate': 2.053146986119034e-05, 'epoch': 1.7683970336565888}
Step 6210: {'loss': 1.2304, 'grad_norm': 3.691347122192383, 'learning_rate': 2.048393230652215e-05, 'epoch': 1.7712492869366798}
Step 6220: {'loss': 1.2868, 'grad_norm': 2.952435255050659, 'learning_rate': 2.0436394751853966e-05, 'epoch': 1.7741015402167712}
Step 6230: {'loss': 1.2416, 'grad_norm': 3.062465190887451, 'learning_rate': 2.038885719718578e-05, 'epoch': 1.7769537934968627}
Step 6240: {'loss': 1.3318, 'grad_norm': 3.377946376800537, 'learning_rate': 2.034131964251759e-05, 'epoch': 1.7798060467769536}
Step 6250: {'loss': 1.2048, 'grad_norm': 3.643399715423584, 'learning_rate': 2.02937820878494e-05, 'epoch': 1.782658300057045}
Step 6260: {'loss': 1.2565, 'grad_norm': 2.6683599948883057, 'learning_rate': 2.0246244533181212e-05, 'epoch': 1.7855105533371365}
Step 6270: {'loss': 1.2474, 'grad_norm': 2.4910006523132324, 'learning_rate': 2.0198706978513027e-05, 'epoch': 1.7883628066172275}
Step 6280: {'loss': 1.3448, 'grad_norm': 2.700813055038452, 'learning_rate': 2.0151169423844837e-05, 'epoch': 1.7912150598973189}
Step 6290: {'loss': 1.3582, 'grad_norm': 3.097194194793701, 'learning_rate': 2.010363186917665e-05, 'epoch': 1.7940673131774103}
Step 6300: {'loss': 1.2523, 'grad_norm': 2.6168899536132812, 'learning_rate': 2.0056094314508462e-05, 'epoch': 1.7969195664575013}
Step 6310: {'loss': 1.3911, 'grad_norm': 2.8101725578308105, 'learning_rate': 2.0008556759840273e-05, 'epoch': 1.7997718197375927}
Step 6320: {'loss': 1.3539, 'grad_norm': 3.5069403648376465, 'learning_rate': 1.9961019205172088e-05, 'epoch': 1.8026240730176841}
Step 6330: {'loss': 1.2643, 'grad_norm': 2.678872585296631, 'learning_rate': 1.99134816505039e-05, 'epoch': 1.805476326297775}
Step 6340: {'loss': 1.2469, 'grad_norm': 3.27946400642395, 'learning_rate': 1.9865944095835713e-05, 'epoch': 1.8083285795778665}
Step 6350: {'loss': 1.3058, 'grad_norm': 2.9309000968933105, 'learning_rate': 1.9818406541167523e-05, 'epoch': 1.811180832857958}
Step 6360: {'loss': 1.2155, 'grad_norm': 2.8219828605651855, 'learning_rate': 1.9770868986499334e-05, 'epoch': 1.814033086138049}
Step 6370: {'loss': 1.2504, 'grad_norm': 2.964764356613159, 'learning_rate': 1.9723331431831145e-05, 'epoch': 1.8168853394181403}
Step 6380: {'loss': 1.3866, 'grad_norm': 3.1483733654022217, 'learning_rate': 1.967579387716296e-05, 'epoch': 1.8197375926982318}
Step 6390: {'loss': 1.3056, 'grad_norm': 3.068323850631714, 'learning_rate': 1.9628256322494773e-05, 'epoch': 1.8225898459783227}
Step 6400: {'loss': 1.3852, 'grad_norm': 3.1634809970855713, 'learning_rate': 1.9580718767826584e-05, 'epoch': 1.8254420992584142}
Step 6410: {'loss': 1.2861, 'grad_norm': 3.1614513397216797, 'learning_rate': 1.95331812131584e-05, 'epoch': 1.8282943525385056}
Step 6420: {'loss': 1.2916, 'grad_norm': 3.065558910369873, 'learning_rate': 1.9485643658490206e-05, 'epoch': 1.8311466058185966}
Step 6430: {'loss': 1.2033, 'grad_norm': 2.945634365081787, 'learning_rate': 1.943810610382202e-05, 'epoch': 1.833998859098688}
Step 6440: {'loss': 1.3128, 'grad_norm': 2.3316924571990967, 'learning_rate': 1.939056854915383e-05, 'epoch': 1.8368511123787792}
Step 6450: {'loss': 1.3845, 'grad_norm': 3.076113700866699, 'learning_rate': 1.9343030994485645e-05, 'epoch': 1.8397033656588704}
Step 6460: {'loss': 1.2719, 'grad_norm': 2.7175557613372803, 'learning_rate': 1.929549343981746e-05, 'epoch': 1.8425556189389618}
Step 6470: {'loss': 1.2912, 'grad_norm': 2.7227165699005127, 'learning_rate': 1.924795588514927e-05, 'epoch': 1.845407872219053}
Step 6480: {'loss': 1.313, 'grad_norm': 2.69781231880188, 'learning_rate': 1.920041833048108e-05, 'epoch': 1.8482601254991442}
Step 6490: {'loss': 1.2523, 'grad_norm': 2.861936569213867, 'learning_rate': 1.9152880775812892e-05, 'epoch': 1.8511123787792356}
Step 6500: {'loss': 1.2549, 'grad_norm': 3.4916036128997803, 'learning_rate': 1.9105343221144706e-05, 'epoch': 1.8539646320593268}
Step 6510: {'loss': 1.3284, 'grad_norm': 2.7722132205963135, 'learning_rate': 1.9057805666476517e-05, 'epoch': 1.856816885339418}
Step 6520: {'loss': 1.2822, 'grad_norm': 2.508821725845337, 'learning_rate': 1.901026811180833e-05, 'epoch': 1.8596691386195094}
Step 6530: {'loss': 1.2518, 'grad_norm': 2.566467523574829, 'learning_rate': 1.8962730557140142e-05, 'epoch': 1.8625213918996006}
Step 6540: {'loss': 1.2018, 'grad_norm': 2.2716503143310547, 'learning_rate': 1.8915193002471953e-05, 'epoch': 1.8653736451796918}
Step 6550: {'loss': 1.2336, 'grad_norm': 2.595942735671997, 'learning_rate': 1.8867655447803767e-05, 'epoch': 1.8682258984597833}
Step 6560: {'loss': 1.3781, 'grad_norm': 2.8152854442596436, 'learning_rate': 1.8820117893135578e-05, 'epoch': 1.8710781517398745}
Step 6570: {'loss': 1.3447, 'grad_norm': 3.1535043716430664, 'learning_rate': 1.8772580338467392e-05, 'epoch': 1.8739304050199657}
Step 6580: {'loss': 1.2509, 'grad_norm': 2.8499584197998047, 'learning_rate': 1.8725042783799203e-05, 'epoch': 1.876782658300057}
Step 6590: {'loss': 1.3617, 'grad_norm': 3.312133550643921, 'learning_rate': 1.8677505229131014e-05, 'epoch': 1.8796349115801483}
Step 6600: {'loss': 1.2902, 'grad_norm': 3.44793438911438, 'learning_rate': 1.8629967674462825e-05, 'epoch': 1.8824871648602395}
Step 6610: {'loss': 1.2518, 'grad_norm': 2.9948458671569824, 'learning_rate': 1.858243011979464e-05, 'epoch': 1.885339418140331}
Step 6620: {'loss': 1.1138, 'grad_norm': 2.9008800983428955, 'learning_rate': 1.8534892565126453e-05, 'epoch': 1.888191671420422}
Step 6630: {'loss': 1.178, 'grad_norm': 2.8731517791748047, 'learning_rate': 1.8487355010458264e-05, 'epoch': 1.8910439247005133}
Step 6640: {'loss': 1.1429, 'grad_norm': 2.664658546447754, 'learning_rate': 1.8439817455790075e-05, 'epoch': 1.8938961779806047}
Step 6650: {'loss': 1.2732, 'grad_norm': 2.6074957847595215, 'learning_rate': 1.8392279901121885e-05, 'epoch': 1.896748431260696}
Step 6660: {'loss': 1.3536, 'grad_norm': 2.428614377975464, 'learning_rate': 1.83447423464537e-05, 'epoch': 1.8996006845407871}
Step 6670: {'loss': 1.2662, 'grad_norm': 2.5739402770996094, 'learning_rate': 1.829720479178551e-05, 'epoch': 1.9024529378208785}
Step 6680: {'loss': 1.2421, 'grad_norm': 2.8706142902374268, 'learning_rate': 1.8249667237117325e-05, 'epoch': 1.9053051911009697}
Step 6690: {'loss': 1.1995, 'grad_norm': 2.7887892723083496, 'learning_rate': 1.8202129682449135e-05, 'epoch': 1.908157444381061}
Step 6700: {'loss': 1.3115, 'grad_norm': 2.7766785621643066, 'learning_rate': 1.8154592127780946e-05, 'epoch': 1.9110096976611524}
Step 6710: {'loss': 1.2725, 'grad_norm': 4.214258193969727, 'learning_rate': 1.810705457311276e-05, 'epoch': 1.9138619509412436}
Step 6720: {'loss': 1.2877, 'grad_norm': 2.817530393600464, 'learning_rate': 1.805951701844457e-05, 'epoch': 1.9167142042213348}
Step 6730: {'loss': 1.2659, 'grad_norm': 2.8166847229003906, 'learning_rate': 1.8011979463776386e-05, 'epoch': 1.9195664575014262}
Step 6740: {'loss': 1.3212, 'grad_norm': 2.713019609451294, 'learning_rate': 1.7964441909108196e-05, 'epoch': 1.9224187107815174}
Step 6750: {'loss': 1.2008, 'grad_norm': 2.941528558731079, 'learning_rate': 1.791690435444001e-05, 'epoch': 1.9252709640616086}
Step 6760: {'loss': 1.1946, 'grad_norm': 2.751309871673584, 'learning_rate': 1.7869366799771818e-05, 'epoch': 1.9281232173417}
Step 6770: {'loss': 1.1652, 'grad_norm': 2.692856550216675, 'learning_rate': 1.7821829245103632e-05, 'epoch': 1.9309754706217912}
Step 6780: {'loss': 1.1449, 'grad_norm': 2.2885758876800537, 'learning_rate': 1.7774291690435446e-05, 'epoch': 1.9338277239018824}
Step 6790: {'loss': 1.3086, 'grad_norm': 3.4554779529571533, 'learning_rate': 1.7726754135767257e-05, 'epoch': 1.9366799771819738}
Step 6800: {'loss': 1.3378, 'grad_norm': 8.584959030151367, 'learning_rate': 1.767921658109907e-05, 'epoch': 1.939532230462065}
Step 6810: {'loss': 1.2287, 'grad_norm': 4.218532085418701, 'learning_rate': 1.7631679026430882e-05, 'epoch': 1.9423844837421562}
Step 6820: {'loss': 1.2469, 'grad_norm': 3.037543535232544, 'learning_rate': 1.7584141471762693e-05, 'epoch': 1.9452367370222476}
Step 6830: {'loss': 1.3099, 'grad_norm': 2.7920968532562256, 'learning_rate': 1.7536603917094504e-05, 'epoch': 1.9480889903023388}
Step 6840: {'loss': 1.2626, 'grad_norm': 2.795043706893921, 'learning_rate': 1.7489066362426318e-05, 'epoch': 1.95094124358243}
Step 6850: {'loss': 1.2345, 'grad_norm': 3.3172149658203125, 'learning_rate': 1.744152880775813e-05, 'epoch': 1.9537934968625215}
Step 6860: {'loss': 1.2333, 'grad_norm': 3.0985517501831055, 'learning_rate': 1.7393991253089943e-05, 'epoch': 1.9566457501426127}
Step 6870: {'loss': 1.2963, 'grad_norm': 2.8268208503723145, 'learning_rate': 1.7346453698421754e-05, 'epoch': 1.9594980034227039}
Step 6880: {'loss': 1.2111, 'grad_norm': 2.1379554271698, 'learning_rate': 1.7298916143753565e-05, 'epoch': 1.9623502567027953}
Step 6890: {'loss': 1.275, 'grad_norm': 3.1708929538726807, 'learning_rate': 1.725137858908538e-05, 'epoch': 1.9652025099828865}
Step 6900: {'loss': 1.2722, 'grad_norm': 2.792537212371826, 'learning_rate': 1.720384103441719e-05, 'epoch': 1.9680547632629777}
Step 6910: {'loss': 1.3861, 'grad_norm': 3.6215720176696777, 'learning_rate': 1.7156303479749004e-05, 'epoch': 1.970907016543069}
Step 6920: {'loss': 1.3096, 'grad_norm': 2.6152212619781494, 'learning_rate': 1.7108765925080815e-05, 'epoch': 1.9737592698231603}
Step 6930: {'loss': 1.4157, 'grad_norm': 3.1601719856262207, 'learning_rate': 1.7061228370412626e-05, 'epoch': 1.9766115231032515}
Step 6940: {'loss': 1.2805, 'grad_norm': 3.058135509490967, 'learning_rate': 1.701369081574444e-05, 'epoch': 1.979463776383343}
Step 6950: {'loss': 1.2327, 'grad_norm': 3.2697482109069824, 'learning_rate': 1.696615326107625e-05, 'epoch': 1.9823160296634341}
Step 6960: {'loss': 1.2876, 'grad_norm': 3.1693243980407715, 'learning_rate': 1.6918615706408065e-05, 'epoch': 1.9851682829435253}
Step 6970: {'loss': 1.2795, 'grad_norm': 2.6343185901641846, 'learning_rate': 1.6871078151739876e-05, 'epoch': 1.9880205362236167}
Step 6980: {'loss': 1.2196, 'grad_norm': 3.081129312515259, 'learning_rate': 1.6823540597071687e-05, 'epoch': 1.990872789503708}
Step 6990: {'loss': 1.176, 'grad_norm': 2.868889331817627, 'learning_rate': 1.6776003042403497e-05, 'epoch': 1.9937250427837991}
Step 7000: {'loss': 1.1683, 'grad_norm': 2.3825061321258545, 'learning_rate': 1.6728465487735312e-05, 'epoch': 1.9965772960638906}
Step 7010: {'loss': 1.195, 'grad_norm': 2.939770221710205, 'learning_rate': 1.6680927933067123e-05, 'epoch': 1.9994295493439818}
Epoch 2.0 completed in 4778.12s
Step 7012: {'eval_loss': 1.274171233177185, 'eval_runtime': 68.8492, 'eval_samples_per_second': 22.629, 'eval_steps_per_second': 5.665, 'epoch': 2.0}
Step 7020: {'loss': 1.1473, 'grad_norm': 2.4906954765319824, 'learning_rate': 1.6633390378398937e-05, 'epoch': 2.002281802624073}
Step 7030: {'loss': 1.1933, 'grad_norm': 3.122622489929199, 'learning_rate': 1.6585852823730748e-05, 'epoch': 2.0051340559041644}
Step 7040: {'loss': 1.21, 'grad_norm': 2.391857862472534, 'learning_rate': 1.653831526906256e-05, 'epoch': 2.0079863091842554}
Step 7050: {'loss': 1.2178, 'grad_norm': 2.634387969970703, 'learning_rate': 1.6490777714394373e-05, 'epoch': 2.010838562464347}
Step 7060: {'loss': 1.3623, 'grad_norm': 2.0929455757141113, 'learning_rate': 1.6443240159726183e-05, 'epoch': 2.013690815744438}
Step 7070: {'loss': 1.263, 'grad_norm': 2.3751220703125, 'learning_rate': 1.6395702605057998e-05, 'epoch': 2.016543069024529}
Step 7080: {'loss': 1.3075, 'grad_norm': 2.9997894763946533, 'learning_rate': 1.634816505038981e-05, 'epoch': 2.0193953223046206}
Step 7090: {'loss': 1.0988, 'grad_norm': 2.4877235889434814, 'learning_rate': 1.630062749572162e-05, 'epoch': 2.022247575584712}
Step 7100: {'loss': 1.3096, 'grad_norm': 3.4481983184814453, 'learning_rate': 1.6253089941053433e-05, 'epoch': 2.025099828864803}
Step 7110: {'loss': 1.1709, 'grad_norm': 2.7569050788879395, 'learning_rate': 1.6205552386385244e-05, 'epoch': 2.0279520821448944}
Step 7120: {'loss': 1.1134, 'grad_norm': 2.5853030681610107, 'learning_rate': 1.615801483171706e-05, 'epoch': 2.030804335424986}
Step 7130: {'loss': 1.2914, 'grad_norm': 3.090829849243164, 'learning_rate': 1.611047727704887e-05, 'epoch': 2.033656588705077}
Step 7140: {'loss': 1.1701, 'grad_norm': 2.9939663410186768, 'learning_rate': 1.6062939722380684e-05, 'epoch': 2.0365088419851682}
Step 7150: {'loss': 1.1655, 'grad_norm': 3.220806837081909, 'learning_rate': 1.601540216771249e-05, 'epoch': 2.0393610952652597}
Step 7160: {'loss': 0.9472, 'grad_norm': 2.729810953140259, 'learning_rate': 1.5967864613044305e-05, 'epoch': 2.0422133485453506}
Step 7170: {'loss': 1.2561, 'grad_norm': 2.7684073448181152, 'learning_rate': 1.5920327058376116e-05, 'epoch': 2.045065601825442}
Step 7180: {'loss': 1.2385, 'grad_norm': 2.6590068340301514, 'learning_rate': 1.587278950370793e-05, 'epoch': 2.0479178551055335}
Step 7190: {'loss': 1.1723, 'grad_norm': 2.6407761573791504, 'learning_rate': 1.5825251949039744e-05, 'epoch': 2.0507701083856245}
Step 7200: {'loss': 1.0196, 'grad_norm': 2.59952712059021, 'learning_rate': 1.5777714394371555e-05, 'epoch': 2.053622361665716}
Step 7210: {'loss': 1.2138, 'grad_norm': 3.3058431148529053, 'learning_rate': 1.5730176839703366e-05, 'epoch': 2.0564746149458073}
Step 7220: {'loss': 1.2449, 'grad_norm': 3.1831631660461426, 'learning_rate': 1.5682639285035177e-05, 'epoch': 2.0593268682258983}
Step 7230: {'loss': 1.2952, 'grad_norm': 2.719191551208496, 'learning_rate': 1.563510173036699e-05, 'epoch': 2.0621791215059897}
Step 7240: {'loss': 1.2712, 'grad_norm': 3.046414852142334, 'learning_rate': 1.5587564175698802e-05, 'epoch': 2.065031374786081}
Step 7250: {'loss': 1.1901, 'grad_norm': 2.842693328857422, 'learning_rate': 1.5540026621030616e-05, 'epoch': 2.067883628066172}
Step 7260: {'loss': 1.237, 'grad_norm': 3.157862901687622, 'learning_rate': 1.5492489066362427e-05, 'epoch': 2.0707358813462635}
Step 7270: {'loss': 1.0956, 'grad_norm': 2.9978184700012207, 'learning_rate': 1.5444951511694238e-05, 'epoch': 2.073588134626355}
Step 7280: {'loss': 1.2597, 'grad_norm': 3.1407737731933594, 'learning_rate': 1.5397413957026052e-05, 'epoch': 2.076440387906446}
Step 7290: {'loss': 1.1956, 'grad_norm': 2.9716477394104004, 'learning_rate': 1.5349876402357863e-05, 'epoch': 2.0792926411865373}
Step 7300: {'loss': 1.161, 'grad_norm': 2.770174503326416, 'learning_rate': 1.5302338847689677e-05, 'epoch': 2.0821448944666288}
Step 7310: {'loss': 1.2074, 'grad_norm': 2.8996999263763428, 'learning_rate': 1.5254801293021486e-05, 'epoch': 2.0849971477467197}
Step 7320: {'loss': 1.1113, 'grad_norm': 3.0571985244750977, 'learning_rate': 1.52072637383533e-05, 'epoch': 2.087849401026811}
Step 7330: {'loss': 1.1668, 'grad_norm': 2.742058753967285, 'learning_rate': 1.5159726183685111e-05, 'epoch': 2.0907016543069026}
Step 7340: {'loss': 1.177, 'grad_norm': 3.57010555267334, 'learning_rate': 1.5112188629016924e-05, 'epoch': 2.0935539075869936}
Step 7350: {'loss': 1.2803, 'grad_norm': 3.4985508918762207, 'learning_rate': 1.5064651074348738e-05, 'epoch': 2.096406160867085}
Step 7360: {'loss': 1.3178, 'grad_norm': 2.669339179992676, 'learning_rate': 1.5017113519680547e-05, 'epoch': 2.0992584141471764}
Step 7370: {'loss': 1.1892, 'grad_norm': 2.8633100986480713, 'learning_rate': 1.4969575965012361e-05, 'epoch': 2.1021106674272674}
Step 7380: {'loss': 1.3491, 'grad_norm': 3.169917583465576, 'learning_rate': 1.4922038410344172e-05, 'epoch': 2.104962920707359}
Step 7390: {'loss': 1.1789, 'grad_norm': 2.5201733112335205, 'learning_rate': 1.4874500855675985e-05, 'epoch': 2.10781517398745}
Step 7400: {'loss': 1.2403, 'grad_norm': 3.1791248321533203, 'learning_rate': 1.4826963301007796e-05, 'epoch': 2.110667427267541}
Step 7410: {'loss': 1.0852, 'grad_norm': 2.7204673290252686, 'learning_rate': 1.477942574633961e-05, 'epoch': 2.1135196805476326}
Step 7420: {'loss': 1.155, 'grad_norm': 2.841782331466675, 'learning_rate': 1.4731888191671422e-05, 'epoch': 2.116371933827724}
Step 7430: {'loss': 1.1151, 'grad_norm': 2.7036592960357666, 'learning_rate': 1.4684350637003233e-05, 'epoch': 2.119224187107815}
Step 7440: {'loss': 1.2392, 'grad_norm': 3.051248788833618, 'learning_rate': 1.4636813082335046e-05, 'epoch': 2.1220764403879064}
Step 7450: {'loss': 1.1248, 'grad_norm': 2.402773141860962, 'learning_rate': 1.4589275527666856e-05, 'epoch': 2.124928693667998}
Step 7460: {'loss': 1.1982, 'grad_norm': 2.3901283740997314, 'learning_rate': 1.454173797299867e-05, 'epoch': 2.127780946948089}
Step 7470: {'loss': 1.1377, 'grad_norm': 2.40920352935791, 'learning_rate': 1.4494200418330481e-05, 'epoch': 2.1306332002281803}
Step 7480: {'loss': 1.077, 'grad_norm': 2.7764651775360107, 'learning_rate': 1.4446662863662294e-05, 'epoch': 2.1334854535082717}
Step 7490: {'loss': 1.2225, 'grad_norm': 2.810690402984619, 'learning_rate': 1.4399125308994105e-05, 'epoch': 2.1363377067883627}
Step 7500: {'loss': 1.3503, 'grad_norm': 3.2663888931274414, 'learning_rate': 1.4351587754325917e-05, 'epoch': 2.139189960068454}
Step 7510: {'loss': 1.2043, 'grad_norm': 2.8456249237060547, 'learning_rate': 1.4304050199657732e-05, 'epoch': 2.1420422133485455}
Step 7520: {'loss': 1.1371, 'grad_norm': 2.4302093982696533, 'learning_rate': 1.4256512644989542e-05, 'epoch': 2.1448944666286365}
Step 7530: {'loss': 1.2599, 'grad_norm': 2.600651979446411, 'learning_rate': 1.4208975090321355e-05, 'epoch': 2.147746719908728}
Step 7540: {'loss': 1.2273, 'grad_norm': 2.5083155632019043, 'learning_rate': 1.4161437535653166e-05, 'epoch': 2.1505989731888193}
Step 7550: {'loss': 1.2697, 'grad_norm': 2.9570953845977783, 'learning_rate': 1.411389998098498e-05, 'epoch': 2.1534512264689103}
Step 7560: {'loss': 1.1905, 'grad_norm': 2.991511583328247, 'learning_rate': 1.4066362426316789e-05, 'epoch': 2.1563034797490017}
Step 7570: {'loss': 1.2368, 'grad_norm': 3.542057991027832, 'learning_rate': 1.4018824871648603e-05, 'epoch': 2.159155733029093}
Step 7580: {'loss': 1.172, 'grad_norm': 2.7645187377929688, 'learning_rate': 1.3971287316980416e-05, 'epoch': 2.162007986309184}
Step 7590: {'loss': 1.1983, 'grad_norm': 2.658653974533081, 'learning_rate': 1.3923749762312227e-05, 'epoch': 2.1648602395892755}
Step 7600: {'loss': 1.1843, 'grad_norm': 2.673802375793457, 'learning_rate': 1.387621220764404e-05, 'epoch': 2.167712492869367}
Step 7610: {'loss': 1.2191, 'grad_norm': 2.9627034664154053, 'learning_rate': 1.3828674652975852e-05, 'epoch': 2.170564746149458}
Step 7620: {'loss': 1.2135, 'grad_norm': 6.958239555358887, 'learning_rate': 1.3781137098307664e-05, 'epoch': 2.1734169994295494}
Step 7630: {'loss': 1.1339, 'grad_norm': 2.5300137996673584, 'learning_rate': 1.3733599543639475e-05, 'epoch': 2.176269252709641}
Step 7640: {'loss': 1.1851, 'grad_norm': 2.7616894245147705, 'learning_rate': 1.3686061988971287e-05, 'epoch': 2.1791215059897318}
Step 7650: {'loss': 1.2436, 'grad_norm': 2.643954038619995, 'learning_rate': 1.3638524434303098e-05, 'epoch': 2.181973759269823}
Step 7660: {'loss': 1.199, 'grad_norm': 3.192534923553467, 'learning_rate': 1.3590986879634913e-05, 'epoch': 2.1848260125499146}
Step 7670: {'loss': 1.214, 'grad_norm': 3.0046520233154297, 'learning_rate': 1.3543449324966725e-05, 'epoch': 2.1876782658300056}
Step 7680: {'loss': 1.0889, 'grad_norm': 3.0436389446258545, 'learning_rate': 1.3495911770298536e-05, 'epoch': 2.190530519110097}
Step 7690: {'loss': 1.2171, 'grad_norm': 3.1475160121917725, 'learning_rate': 1.344837421563035e-05, 'epoch': 2.1933827723901884}
Step 7700: {'loss': 1.2454, 'grad_norm': 3.375537872314453, 'learning_rate': 1.340083666096216e-05, 'epoch': 2.1962350256702794}
Step 7710: {'loss': 1.2236, 'grad_norm': 3.264657974243164, 'learning_rate': 1.3353299106293973e-05, 'epoch': 2.199087278950371}
Step 7720: {'loss': 1.2805, 'grad_norm': 3.466204881668091, 'learning_rate': 1.3305761551625784e-05, 'epoch': 2.2019395322304622}
Step 7730: {'loss': 1.2418, 'grad_norm': 2.522242546081543, 'learning_rate': 1.3258223996957597e-05, 'epoch': 2.204791785510553}
Step 7740: {'loss': 1.2152, 'grad_norm': 2.8492655754089355, 'learning_rate': 1.3210686442289411e-05, 'epoch': 2.2076440387906446}
Step 7750: {'loss': 1.2324, 'grad_norm': 2.9392731189727783, 'learning_rate': 1.3163148887621222e-05, 'epoch': 2.210496292070736}
Step 7760: {'loss': 1.1984, 'grad_norm': 3.3598508834838867, 'learning_rate': 1.3115611332953034e-05, 'epoch': 2.213348545350827}
Step 7770: {'loss': 1.2218, 'grad_norm': 2.7898473739624023, 'learning_rate': 1.3068073778284845e-05, 'epoch': 2.2162007986309185}
Step 7780: {'loss': 1.1886, 'grad_norm': 2.9754600524902344, 'learning_rate': 1.3020536223616658e-05, 'epoch': 2.21905305191101}
Step 7790: {'loss': 1.2707, 'grad_norm': 2.8947830200195312, 'learning_rate': 1.2972998668948468e-05, 'epoch': 2.221905305191101}
Step 7800: {'loss': 1.2545, 'grad_norm': 2.776177406311035, 'learning_rate': 1.2925461114280283e-05, 'epoch': 2.2247575584711923}
Step 7810: {'loss': 1.2018, 'grad_norm': 3.0351884365081787, 'learning_rate': 1.2877923559612094e-05, 'epoch': 2.2276098117512837}
Step 7820: {'loss': 1.176, 'grad_norm': 3.4057374000549316, 'learning_rate': 1.2830386004943906e-05, 'epoch': 2.2304620650313747}
Step 7830: {'loss': 1.1875, 'grad_norm': 3.40472149848938, 'learning_rate': 1.278284845027572e-05, 'epoch': 2.233314318311466}
Step 7840: {'loss': 1.2702, 'grad_norm': 2.4207684993743896, 'learning_rate': 1.273531089560753e-05, 'epoch': 2.2361665715915575}
Step 7850: {'loss': 1.2529, 'grad_norm': 2.942594051361084, 'learning_rate': 1.2687773340939344e-05, 'epoch': 2.2390188248716485}
Step 7860: {'loss': 1.2343, 'grad_norm': 4.821069240570068, 'learning_rate': 1.2640235786271154e-05, 'epoch': 2.24187107815174}
Step 7870: {'loss': 1.2536, 'grad_norm': 3.233098030090332, 'learning_rate': 1.2592698231602967e-05, 'epoch': 2.2447233314318313}
Step 7880: {'loss': 1.2049, 'grad_norm': 2.6595659255981445, 'learning_rate': 1.2545160676934778e-05, 'epoch': 2.2475755847119223}
Step 7890: {'loss': 1.1605, 'grad_norm': 2.85286283493042, 'learning_rate': 1.2497623122266592e-05, 'epoch': 2.2504278379920137}
Step 7900: {'loss': 1.1973, 'grad_norm': 3.574756383895874, 'learning_rate': 1.2450085567598403e-05, 'epoch': 2.253280091272105}
Step 7910: {'loss': 1.2434, 'grad_norm': 2.2547192573547363, 'learning_rate': 1.2402548012930215e-05, 'epoch': 2.256132344552196}
Step 7920: {'loss': 1.1208, 'grad_norm': 2.8157224655151367, 'learning_rate': 1.2355010458262028e-05, 'epoch': 2.2589845978322876}
Step 7930: {'loss': 1.2021, 'grad_norm': 3.79640793800354, 'learning_rate': 1.2307472903593839e-05, 'epoch': 2.2618368511123785}
Step 7940: {'loss': 1.2288, 'grad_norm': 3.275710105895996, 'learning_rate': 1.2259935348925653e-05, 'epoch': 2.26468910439247}
Step 7950: {'loss': 1.1899, 'grad_norm': 2.821568250656128, 'learning_rate': 1.2212397794257464e-05, 'epoch': 2.2675413576725614}
Step 7960: {'loss': 1.1601, 'grad_norm': 2.366159439086914, 'learning_rate': 1.2164860239589276e-05, 'epoch': 2.2703936109526524}
Step 7970: {'loss': 1.1681, 'grad_norm': 2.6660802364349365, 'learning_rate': 1.2117322684921089e-05, 'epoch': 2.2732458642327438}
Step 7980: {'loss': 1.1959, 'grad_norm': 2.8337039947509766, 'learning_rate': 1.20697851302529e-05, 'epoch': 2.276098117512835}
Step 7990: {'loss': 1.2006, 'grad_norm': 3.5763280391693115, 'learning_rate': 1.2022247575584712e-05, 'epoch': 2.278950370792926}
Step 8000: {'loss': 1.2862, 'grad_norm': 2.847062349319458, 'learning_rate': 1.1974710020916525e-05, 'epoch': 2.2818026240730176}
Step 8010: {'loss': 1.3343, 'grad_norm': 2.952622890472412, 'learning_rate': 1.1927172466248335e-05, 'epoch': 2.284654877353109}
Step 8020: {'loss': 1.1456, 'grad_norm': 3.072425127029419, 'learning_rate': 1.187963491158015e-05, 'epoch': 2.2875071306332}
Step 8030: {'loss': 1.1168, 'grad_norm': 2.760258674621582, 'learning_rate': 1.1832097356911962e-05, 'epoch': 2.2903593839132914}
Step 8040: {'loss': 1.2353, 'grad_norm': 2.729459762573242, 'learning_rate': 1.1784559802243773e-05, 'epoch': 2.293211637193383}
Step 8050: {'loss': 1.1925, 'grad_norm': 2.8289968967437744, 'learning_rate': 1.1737022247575586e-05, 'epoch': 2.296063890473474}
Step 8060: {'loss': 1.3016, 'grad_norm': 3.0396535396575928, 'learning_rate': 1.1689484692907398e-05, 'epoch': 2.2989161437535652}
Step 8070: {'loss': 1.2871, 'grad_norm': 3.0133323669433594, 'learning_rate': 1.1641947138239209e-05, 'epoch': 2.3017683970336567}
Step 8080: {'loss': 1.1218, 'grad_norm': 2.8768110275268555, 'learning_rate': 1.1594409583571021e-05, 'epoch': 2.3046206503137476}
Step 8090: {'loss': 1.3595, 'grad_norm': 10.888216018676758, 'learning_rate': 1.1546872028902834e-05, 'epoch': 2.307472903593839}
Step 8100: {'loss': 1.1883, 'grad_norm': 3.01678466796875, 'learning_rate': 1.1499334474234646e-05, 'epoch': 2.3103251568739305}
Step 8110: {'loss': 1.3133, 'grad_norm': 3.158841133117676, 'learning_rate': 1.1451796919566459e-05, 'epoch': 2.3131774101540215}
Step 8120: {'loss': 1.1301, 'grad_norm': 2.894028663635254, 'learning_rate': 1.140425936489827e-05, 'epoch': 2.316029663434113}
Step 8130: {'loss': 1.2122, 'grad_norm': 3.1483466625213623, 'learning_rate': 1.1356721810230082e-05, 'epoch': 2.3188819167142043}
Step 8140: {'loss': 1.0778, 'grad_norm': 2.948263168334961, 'learning_rate': 1.1309184255561895e-05, 'epoch': 2.3217341699942953}
Step 8150: {'loss': 1.1717, 'grad_norm': 2.44340181350708, 'learning_rate': 1.1261646700893706e-05, 'epoch': 2.3245864232743867}
Step 8160: {'loss': 1.192, 'grad_norm': 2.6274445056915283, 'learning_rate': 1.1214109146225518e-05, 'epoch': 2.327438676554478}
Step 8170: {'loss': 1.1745, 'grad_norm': 2.597356081008911, 'learning_rate': 1.116657159155733e-05, 'epoch': 2.330290929834569}
Step 8180: {'loss': 1.2454, 'grad_norm': 3.1727936267852783, 'learning_rate': 1.1119034036889143e-05, 'epoch': 2.3331431831146605}
Step 8190: {'loss': 1.2378, 'grad_norm': 3.08945631980896, 'learning_rate': 1.1071496482220956e-05, 'epoch': 2.335995436394752}
Step 8200: {'loss': 1.1617, 'grad_norm': 2.817656993865967, 'learning_rate': 1.1023958927552768e-05, 'epoch': 2.338847689674843}
Step 8210: {'loss': 1.2966, 'grad_norm': 2.674051523208618, 'learning_rate': 1.0976421372884579e-05, 'epoch': 2.3416999429549343}
Step 8220: {'loss': 1.1068, 'grad_norm': 2.5962579250335693, 'learning_rate': 1.0928883818216392e-05, 'epoch': 2.3445521962350258}
Step 8230: {'loss': 1.1251, 'grad_norm': 2.9545140266418457, 'learning_rate': 1.0881346263548204e-05, 'epoch': 2.3474044495151167}
Step 8240: {'loss': 1.1459, 'grad_norm': 3.0364253520965576, 'learning_rate': 1.0833808708880015e-05, 'epoch': 2.350256702795208}
Step 8250: {'loss': 1.1985, 'grad_norm': 3.008145809173584, 'learning_rate': 1.0786271154211827e-05, 'epoch': 2.3531089560752996}
Step 8260: {'loss': 1.2345, 'grad_norm': 2.83756422996521, 'learning_rate': 1.073873359954364e-05, 'epoch': 2.3559612093553906}
Step 8270: {'loss': 1.224, 'grad_norm': 3.5844008922576904, 'learning_rate': 1.0691196044875452e-05, 'epoch': 2.358813462635482}
Step 8280: {'loss': 1.142, 'grad_norm': 2.5232746601104736, 'learning_rate': 1.0643658490207265e-05, 'epoch': 2.3616657159155734}
Step 8290: {'loss': 1.2649, 'grad_norm': 2.5620357990264893, 'learning_rate': 1.0596120935539076e-05, 'epoch': 2.3645179691956644}
Step 8300: {'loss': 1.1804, 'grad_norm': 2.900789737701416, 'learning_rate': 1.0548583380870888e-05, 'epoch': 2.367370222475756}
Step 8310: {'loss': 1.2305, 'grad_norm': 2.689612627029419, 'learning_rate': 1.05010458262027e-05, 'epoch': 2.370222475755847}
Step 8320: {'loss': 1.1727, 'grad_norm': 2.6731154918670654, 'learning_rate': 1.0453508271534512e-05, 'epoch': 2.373074729035938}
Step 8330: {'loss': 1.3339, 'grad_norm': 2.590895652770996, 'learning_rate': 1.0405970716866324e-05, 'epoch': 2.3759269823160296}
Step 8340: {'loss': 1.2059, 'grad_norm': 2.732182264328003, 'learning_rate': 1.0358433162198138e-05, 'epoch': 2.378779235596121}
Step 8350: {'loss': 1.1928, 'grad_norm': 3.08528995513916, 'learning_rate': 1.031089560752995e-05, 'epoch': 2.381631488876212}
Step 8360: {'loss': 1.1968, 'grad_norm': 3.4666478633880615, 'learning_rate': 1.0263358052861762e-05, 'epoch': 2.3844837421563034}
Step 8370: {'loss': 1.1594, 'grad_norm': 2.516969919204712, 'learning_rate': 1.0215820498193574e-05, 'epoch': 2.387335995436395}
Step 8380: {'loss': 1.32, 'grad_norm': 3.5373096466064453, 'learning_rate': 1.0168282943525385e-05, 'epoch': 2.390188248716486}
Step 8390: {'loss': 1.2374, 'grad_norm': 2.5237419605255127, 'learning_rate': 1.0120745388857198e-05, 'epoch': 2.3930405019965773}
Step 8400: {'loss': 1.231, 'grad_norm': 2.8285930156707764, 'learning_rate': 1.007320783418901e-05, 'epoch': 2.3958927552766687}
Step 8410: {'loss': 1.2142, 'grad_norm': 3.240447521209717, 'learning_rate': 1.0025670279520821e-05, 'epoch': 2.3987450085567597}
Step 8420: {'loss': 1.1702, 'grad_norm': 2.4920096397399902, 'learning_rate': 9.978132724852635e-06, 'epoch': 2.401597261836851}
Step 8430: {'loss': 1.2078, 'grad_norm': 3.3298380374908447, 'learning_rate': 9.930595170184446e-06, 'epoch': 2.4044495151169425}
Step 8440: {'loss': 1.2488, 'grad_norm': 3.1000654697418213, 'learning_rate': 9.883057615516258e-06, 'epoch': 2.4073017683970335}
Step 8450: {'loss': 1.2802, 'grad_norm': 3.644502639770508, 'learning_rate': 9.835520060848071e-06, 'epoch': 2.410154021677125}
Step 8460: {'loss': 1.2538, 'grad_norm': 3.0002427101135254, 'learning_rate': 9.787982506179882e-06, 'epoch': 2.4130062749572163}
Step 8470: {'loss': 1.2086, 'grad_norm': 3.3578226566314697, 'learning_rate': 9.740444951511694e-06, 'epoch': 2.4158585282373073}
Step 8480: {'loss': 1.204, 'grad_norm': 3.1055984497070312, 'learning_rate': 9.692907396843507e-06, 'epoch': 2.4187107815173987}
Step 8490: {'loss': 1.1479, 'grad_norm': 2.706522226333618, 'learning_rate': 9.645369842175318e-06, 'epoch': 2.42156303479749}
Step 8500: {'loss': 1.1892, 'grad_norm': 2.5378453731536865, 'learning_rate': 9.597832287507132e-06, 'epoch': 2.424415288077581}
Step 8510: {'loss': 1.2077, 'grad_norm': 3.2233526706695557, 'learning_rate': 9.550294732838944e-06, 'epoch': 2.4272675413576725}
Step 8520: {'loss': 1.219, 'grad_norm': 2.9308149814605713, 'learning_rate': 9.502757178170755e-06, 'epoch': 2.430119794637764}
Step 8530: {'loss': 1.19, 'grad_norm': 3.0277209281921387, 'learning_rate': 9.455219623502568e-06, 'epoch': 2.432972047917855}
Step 8540: {'loss': 1.2625, 'grad_norm': 3.0769712924957275, 'learning_rate': 9.40768206883438e-06, 'epoch': 2.4358243011979464}
Step 8550: {'loss': 1.2259, 'grad_norm': 2.6246426105499268, 'learning_rate': 9.360144514166191e-06, 'epoch': 2.4386765544780378}
Step 8560: {'loss': 1.292, 'grad_norm': 3.2340569496154785, 'learning_rate': 9.312606959498004e-06, 'epoch': 2.4415288077581287}
Step 8570: {'loss': 1.2879, 'grad_norm': 2.645660400390625, 'learning_rate': 9.265069404829816e-06, 'epoch': 2.44438106103822}
Step 8580: {'loss': 1.1387, 'grad_norm': 2.5889878273010254, 'learning_rate': 9.217531850161629e-06, 'epoch': 2.4472333143183116}
Step 8590: {'loss': 1.295, 'grad_norm': 3.170604705810547, 'learning_rate': 9.169994295493441e-06, 'epoch': 2.4500855675984026}
Step 8600: {'loss': 1.2104, 'grad_norm': 2.8469817638397217, 'learning_rate': 9.122456740825252e-06, 'epoch': 2.452937820878494}
Step 8610: {'loss': 1.1692, 'grad_norm': 2.845475912094116, 'learning_rate': 9.074919186157065e-06, 'epoch': 2.4557900741585854}
Step 8620: {'loss': 1.1475, 'grad_norm': 2.6704978942871094, 'learning_rate': 9.027381631488877e-06, 'epoch': 2.4586423274386764}
Step 8630: {'loss': 1.189, 'grad_norm': 3.262723684310913, 'learning_rate': 8.979844076820688e-06, 'epoch': 2.461494580718768}
Step 8640: {'loss': 1.2071, 'grad_norm': 2.585385322570801, 'learning_rate': 8.9323065221525e-06, 'epoch': 2.4643468339988592}
Step 8650: {'loss': 1.2665, 'grad_norm': 2.7043497562408447, 'learning_rate': 8.884768967484313e-06, 'epoch': 2.46719908727895}
Step 8660: {'loss': 1.1941, 'grad_norm': 3.3348679542541504, 'learning_rate': 8.837231412816125e-06, 'epoch': 2.4700513405590416}
Step 8670: {'loss': 1.0959, 'grad_norm': 3.0462963581085205, 'learning_rate': 8.789693858147938e-06, 'epoch': 2.472903593839133}
Step 8680: {'loss': 1.2152, 'grad_norm': 3.315537929534912, 'learning_rate': 8.742156303479749e-06, 'epoch': 2.475755847119224}
Step 8690: {'loss': 1.1653, 'grad_norm': 2.349080801010132, 'learning_rate': 8.694618748811561e-06, 'epoch': 2.4786081003993155}
Step 8700: {'loss': 1.1358, 'grad_norm': 2.7038557529449463, 'learning_rate': 8.647081194143374e-06, 'epoch': 2.481460353679407}
Step 8710: {'loss': 1.1478, 'grad_norm': 2.6764912605285645, 'learning_rate': 8.599543639475185e-06, 'epoch': 2.484312606959498}
Step 8720: {'loss': 1.3172, 'grad_norm': 3.4388983249664307, 'learning_rate': 8.552006084806997e-06, 'epoch': 2.4871648602395893}
Step 8730: {'loss': 1.1223, 'grad_norm': 2.415893793106079, 'learning_rate': 8.50446853013881e-06, 'epoch': 2.4900171135196807}
Step 8740: {'loss': 1.2892, 'grad_norm': 3.3870315551757812, 'learning_rate': 8.456930975470622e-06, 'epoch': 2.4928693667997717}
Step 8750: {'loss': 1.2269, 'grad_norm': 3.8061859607696533, 'learning_rate': 8.409393420802435e-06, 'epoch': 2.495721620079863}
Step 8760: {'loss': 1.2985, 'grad_norm': 2.566774845123291, 'learning_rate': 8.361855866134247e-06, 'epoch': 2.4985738733599545}
Step 8770: {'loss': 1.3007, 'grad_norm': 2.9444689750671387, 'learning_rate': 8.314318311466058e-06, 'epoch': 2.5014261266400455}
Step 8780: {'loss': 1.2337, 'grad_norm': 3.160236120223999, 'learning_rate': 8.26678075679787e-06, 'epoch': 2.504278379920137}
Step 8790: {'loss': 1.2863, 'grad_norm': 3.172811985015869, 'learning_rate': 8.219243202129683e-06, 'epoch': 2.5071306332002283}
Step 8800: {'loss': 1.1047, 'grad_norm': 3.3113934993743896, 'learning_rate': 8.171705647461494e-06, 'epoch': 2.5099828864803193}
Step 8810: {'loss': 1.2182, 'grad_norm': 2.8135457038879395, 'learning_rate': 8.124168092793306e-06, 'epoch': 2.5128351397604107}
Step 8820: {'loss': 1.1939, 'grad_norm': 2.7367496490478516, 'learning_rate': 8.076630538125119e-06, 'epoch': 2.515687393040502}
Step 8830: {'loss': 1.2127, 'grad_norm': 2.999368906021118, 'learning_rate': 8.029092983456931e-06, 'epoch': 2.518539646320593}
Step 8840: {'loss': 1.221, 'grad_norm': 3.371140241622925, 'learning_rate': 7.981555428788744e-06, 'epoch': 2.5213918996006845}
Step 8850: {'loss': 1.2583, 'grad_norm': 4.3917646408081055, 'learning_rate': 7.934017874120555e-06, 'epoch': 2.524244152880776}
Step 8860: {'loss': 1.1922, 'grad_norm': 2.9871909618377686, 'learning_rate': 7.886480319452367e-06, 'epoch': 2.527096406160867}
Step 8870: {'loss': 1.3045, 'grad_norm': 3.2575957775115967, 'learning_rate': 7.83894276478418e-06, 'epoch': 2.5299486594409584}
Step 8880: {'loss': 1.221, 'grad_norm': 3.0136351585388184, 'learning_rate': 7.79140521011599e-06, 'epoch': 2.53280091272105}
Step 8890: {'loss': 1.2531, 'grad_norm': 2.56807804107666, 'learning_rate': 7.743867655447803e-06, 'epoch': 2.5356531660011408}
Step 8900: {'loss': 1.0902, 'grad_norm': 3.0047097206115723, 'learning_rate': 7.696330100779617e-06, 'epoch': 2.538505419281232}
Step 8910: {'loss': 1.2612, 'grad_norm': 3.0844931602478027, 'learning_rate': 7.648792546111428e-06, 'epoch': 2.5413576725613236}
Step 8920: {'loss': 1.1804, 'grad_norm': 2.900970220565796, 'learning_rate': 7.601254991443241e-06, 'epoch': 2.5442099258414146}
Step 8930: {'loss': 1.2801, 'grad_norm': 3.6039581298828125, 'learning_rate': 7.5537174367750524e-06, 'epoch': 2.547062179121506}
Step 8940: {'loss': 1.2019, 'grad_norm': 2.9937305450439453, 'learning_rate': 7.506179882106865e-06, 'epoch': 2.5499144324015974}
Step 8950: {'loss': 1.1736, 'grad_norm': 3.0491607189178467, 'learning_rate': 7.458642327438677e-06, 'epoch': 2.5527666856816884}
Step 8960: {'loss': 1.2138, 'grad_norm': 2.6237432956695557, 'learning_rate': 7.411104772770488e-06, 'epoch': 2.55561893896178}
Step 8970: {'loss': 1.2097, 'grad_norm': 3.2429099082946777, 'learning_rate': 7.363567218102301e-06, 'epoch': 2.5584711922418713}
Step 8980: {'loss': 1.1703, 'grad_norm': 3.1768252849578857, 'learning_rate': 7.316029663434114e-06, 'epoch': 2.5613234455219622}
Step 8990: {'loss': 1.1154, 'grad_norm': 2.526655435562134, 'learning_rate': 7.268492108765926e-06, 'epoch': 2.5641756988020536}
Step 9000: {'loss': 1.249, 'grad_norm': 2.8556809425354004, 'learning_rate': 7.2209545540977375e-06, 'epoch': 2.567027952082145}
Step 9010: {'loss': 1.197, 'grad_norm': 2.5835061073303223, 'learning_rate': 7.17341699942955e-06, 'epoch': 2.569880205362236}
Step 9020: {'loss': 1.3374, 'grad_norm': 2.533264636993408, 'learning_rate': 7.125879444761362e-06, 'epoch': 2.5727324586423275}
Step 9030: {'loss': 1.2551, 'grad_norm': 2.888415575027466, 'learning_rate': 7.078341890093173e-06, 'epoch': 2.575584711922419}
Step 9040: {'loss': 1.1866, 'grad_norm': 2.8767895698547363, 'learning_rate': 7.030804335424986e-06, 'epoch': 2.57843696520251}
Step 9050: {'loss': 1.2052, 'grad_norm': 3.1864407062530518, 'learning_rate': 6.9832667807567976e-06, 'epoch': 2.5812892184826013}
Step 9060: {'loss': 1.1725, 'grad_norm': 3.6099042892456055, 'learning_rate': 6.935729226088611e-06, 'epoch': 2.5841414717626927}
Step 9070: {'loss': 1.2381, 'grad_norm': 2.8204073905944824, 'learning_rate': 6.888191671420423e-06, 'epoch': 2.5869937250427837}
Step 9080: {'loss': 1.2592, 'grad_norm': 2.3293704986572266, 'learning_rate': 6.840654116752235e-06, 'epoch': 2.589845978322875}
Step 9090: {'loss': 1.1342, 'grad_norm': 3.100691318511963, 'learning_rate': 6.793116562084047e-06, 'epoch': 2.5926982316029665}
Step 9100: {'loss': 1.1817, 'grad_norm': 2.7739851474761963, 'learning_rate': 6.7455790074158585e-06, 'epoch': 2.5955504848830575}
Step 9110: {'loss': 1.1477, 'grad_norm': 3.067812204360962, 'learning_rate': 6.698041452747671e-06, 'epoch': 2.598402738163149}
Step 9120: {'loss': 1.188, 'grad_norm': 3.151163339614868, 'learning_rate': 6.650503898079483e-06, 'epoch': 2.6012549914432403}
Step 9130: {'loss': 1.255, 'grad_norm': 3.426288366317749, 'learning_rate': 6.602966343411294e-06, 'epoch': 2.6041072447233313}
Step 9140: {'loss': 1.1548, 'grad_norm': 2.4070069789886475, 'learning_rate': 6.555428788743108e-06, 'epoch': 2.6069594980034227}
Step 9150: {'loss': 1.1665, 'grad_norm': 2.829420566558838, 'learning_rate': 6.50789123407492e-06, 'epoch': 2.609811751283514}
Step 9160: {'loss': 1.1793, 'grad_norm': 2.974790096282959, 'learning_rate': 6.460353679406732e-06, 'epoch': 2.612664004563605}
Step 9170: {'loss': 1.1322, 'grad_norm': 2.7187252044677734, 'learning_rate': 6.4128161247385436e-06, 'epoch': 2.6155162578436966}
Step 9180: {'loss': 1.1519, 'grad_norm': 2.5867433547973633, 'learning_rate': 6.365278570070356e-06, 'epoch': 2.618368511123788}
Step 9190: {'loss': 1.1402, 'grad_norm': 3.209787368774414, 'learning_rate': 6.317741015402168e-06, 'epoch': 2.621220764403879}
Step 9200: {'loss': 1.1785, 'grad_norm': 2.4984326362609863, 'learning_rate': 6.2702034607339794e-06, 'epoch': 2.6240730176839704}
Step 9210: {'loss': 1.2971, 'grad_norm': 3.3942861557006836, 'learning_rate': 6.222665906065792e-06, 'epoch': 2.626925270964062}
Step 9220: {'loss': 1.1579, 'grad_norm': 2.7832071781158447, 'learning_rate': 6.1751283513976045e-06, 'epoch': 2.629777524244153}
Step 9230: {'loss': 1.3705, 'grad_norm': 2.8957433700561523, 'learning_rate': 6.127590796729416e-06, 'epoch': 2.632629777524244}
Step 9240: {'loss': 1.233, 'grad_norm': 2.4824106693267822, 'learning_rate': 6.080053242061229e-06, 'epoch': 2.6354820308043356}
Step 9250: {'loss': 1.2887, 'grad_norm': 2.996234655380249, 'learning_rate': 6.032515687393041e-06, 'epoch': 2.6383342840844266}
Step 9260: {'loss': 1.1379, 'grad_norm': 2.8425121307373047, 'learning_rate': 5.984978132724853e-06, 'epoch': 2.641186537364518}
Step 9270: {'loss': 1.2223, 'grad_norm': 2.938511848449707, 'learning_rate': 5.9374405780566645e-06, 'epoch': 2.6440387906446094}
Step 9280: {'loss': 1.2431, 'grad_norm': 2.761599540710449, 'learning_rate': 5.889903023388477e-06, 'epoch': 2.6468910439247004}
Step 9290: {'loss': 1.1556, 'grad_norm': 3.0163040161132812, 'learning_rate': 5.8423654687202896e-06, 'epoch': 2.649743297204792}
Step 9300: {'loss': 1.2188, 'grad_norm': 2.608438491821289, 'learning_rate': 5.794827914052101e-06, 'epoch': 2.6525955504848833}
Step 9310: {'loss': 1.1475, 'grad_norm': 2.5785207748413086, 'learning_rate': 5.747290359383914e-06, 'epoch': 2.6554478037649742}
Step 9320: {'loss': 1.2778, 'grad_norm': 2.6823890209198, 'learning_rate': 5.699752804715726e-06, 'epoch': 2.6583000570450657}
Step 9330: {'loss': 1.2087, 'grad_norm': 3.3632283210754395, 'learning_rate': 5.652215250047538e-06, 'epoch': 2.661152310325157}
Step 9340: {'loss': 1.2167, 'grad_norm': 3.267812728881836, 'learning_rate': 5.60467769537935e-06, 'epoch': 2.664004563605248}
Step 9350: {'loss': 1.1668, 'grad_norm': 2.2126874923706055, 'learning_rate': 5.557140140711162e-06, 'epoch': 2.6668568168853395}
Step 9360: {'loss': 1.2288, 'grad_norm': 2.967109441757202, 'learning_rate': 5.509602586042975e-06, 'epoch': 2.669709070165431}
Step 9370: {'loss': 1.1338, 'grad_norm': 2.6872034072875977, 'learning_rate': 5.462065031374786e-06, 'epoch': 2.672561323445522}
Step 9380: {'loss': 1.1565, 'grad_norm': 2.793504238128662, 'learning_rate': 5.414527476706598e-06, 'epoch': 2.6754135767256133}
Step 9390: {'loss': 1.1823, 'grad_norm': 3.0220816135406494, 'learning_rate': 5.366989922038411e-06, 'epoch': 2.6782658300057047}
Step 9400: {'loss': 1.1985, 'grad_norm': 3.1614553928375244, 'learning_rate': 5.319452367370223e-06, 'epoch': 2.6811180832857957}
Step 9410: {'loss': 1.1698, 'grad_norm': 3.512859582901001, 'learning_rate': 5.271914812702035e-06, 'epoch': 2.683970336565887}
Step 9420: {'loss': 1.1429, 'grad_norm': 2.424196720123291, 'learning_rate': 5.224377258033847e-06, 'epoch': 2.6868225898459785}
Step 9430: {'loss': 1.2239, 'grad_norm': 3.4589812755584717, 'learning_rate': 5.17683970336566e-06, 'epoch': 2.6896748431260695}
Step 9440: {'loss': 1.203, 'grad_norm': 2.847727060317993, 'learning_rate': 5.129302148697471e-06, 'epoch': 2.692527096406161}
Step 9450: {'loss': 1.1324, 'grad_norm': 2.400947093963623, 'learning_rate': 5.081764594029283e-06, 'epoch': 2.6953793496862524}
Step 9460: {'loss': 1.2362, 'grad_norm': 2.9588284492492676, 'learning_rate': 5.034227039361096e-06, 'epoch': 2.6982316029663433}
Step 9470: {'loss': 1.2592, 'grad_norm': 2.735609531402588, 'learning_rate': 4.986689484692908e-06, 'epoch': 2.7010838562464348}
Step 9480: {'loss': 1.1779, 'grad_norm': 2.9011151790618896, 'learning_rate': 4.93915193002472e-06, 'epoch': 2.703936109526526}
Step 9490: {'loss': 1.285, 'grad_norm': 2.788404703140259, 'learning_rate': 4.891614375356532e-06, 'epoch': 2.706788362806617}
Step 9500: {'loss': 1.2386, 'grad_norm': 2.7161436080932617, 'learning_rate': 4.844076820688344e-06, 'epoch': 2.7096406160867086}
Step 9510: {'loss': 1.1927, 'grad_norm': 3.1040091514587402, 'learning_rate': 4.7965392660201565e-06, 'epoch': 2.7124928693668}
Step 9520: {'loss': 1.265, 'grad_norm': 2.619983434677124, 'learning_rate': 4.749001711351968e-06, 'epoch': 2.715345122646891}
Step 9530: {'loss': 1.1608, 'grad_norm': 2.8473868370056152, 'learning_rate': 4.701464156683781e-06, 'epoch': 2.7181973759269824}
Step 9540: {'loss': 1.1325, 'grad_norm': 2.4612298011779785, 'learning_rate': 4.653926602015592e-06, 'epoch': 2.721049629207074}
Step 9550: {'loss': 1.1725, 'grad_norm': 2.5193233489990234, 'learning_rate': 4.606389047347405e-06, 'epoch': 2.723901882487165}
Step 9560: {'loss': 1.2403, 'grad_norm': 2.7506484985351562, 'learning_rate': 4.5588514926792166e-06, 'epoch': 2.7267541357672562}
Step 9570: {'loss': 1.2346, 'grad_norm': 2.965278387069702, 'learning_rate': 4.511313938011029e-06, 'epoch': 2.7296063890473476}
Step 9580: {'loss': 1.225, 'grad_norm': 3.536289930343628, 'learning_rate': 4.463776383342841e-06, 'epoch': 2.7324586423274386}
Step 9590: {'loss': 1.1838, 'grad_norm': 3.256165027618408, 'learning_rate': 4.416238828674653e-06, 'epoch': 2.73531089560753}
Step 9600: {'loss': 1.2729, 'grad_norm': 2.587402582168579, 'learning_rate': 4.368701274006466e-06, 'epoch': 2.7381631488876215}
Step 9610: {'loss': 1.1569, 'grad_norm': 3.122372627258301, 'learning_rate': 4.3211637193382775e-06, 'epoch': 2.7410154021677124}
Step 9620: {'loss': 1.0447, 'grad_norm': 2.5386581420898438, 'learning_rate': 4.273626164670089e-06, 'epoch': 2.743867655447804}
Step 9630: {'loss': 1.0717, 'grad_norm': 2.5665926933288574, 'learning_rate': 4.226088610001902e-06, 'epoch': 2.7467199087278953}
Step 9640: {'loss': 1.2149, 'grad_norm': 2.526510000228882, 'learning_rate': 4.178551055333714e-06, 'epoch': 2.7495721620079863}
Step 9650: {'loss': 1.197, 'grad_norm': 4.298973083496094, 'learning_rate': 4.131013500665526e-06, 'epoch': 2.7524244152880777}
Step 9660: {'loss': 1.2621, 'grad_norm': 3.4617550373077393, 'learning_rate': 4.0834759459973375e-06, 'epoch': 2.755276668568169}
Step 9670: {'loss': 1.0525, 'grad_norm': 2.4880220890045166, 'learning_rate': 4.035938391329151e-06, 'epoch': 2.75812892184826}
Step 9680: {'loss': 1.1253, 'grad_norm': 2.8687543869018555, 'learning_rate': 3.9884008366609625e-06, 'epoch': 2.7609811751283515}
Step 9690: {'loss': 1.1492, 'grad_norm': 2.9317667484283447, 'learning_rate': 3.940863281992774e-06, 'epoch': 2.763833428408443}
Step 9700: {'loss': 1.2431, 'grad_norm': 3.045710563659668, 'learning_rate': 3.893325727324587e-06, 'epoch': 2.766685681688534}
Step 9710: {'loss': 1.1286, 'grad_norm': 3.212459087371826, 'learning_rate': 3.845788172656399e-06, 'epoch': 2.7695379349686253}
Step 9720: {'loss': 1.3279, 'grad_norm': 3.5174331665039062, 'learning_rate': 3.798250617988211e-06, 'epoch': 2.7723901882487167}
Step 9730: {'loss': 1.1694, 'grad_norm': 3.1610212326049805, 'learning_rate': 3.750713063320023e-06, 'epoch': 2.7752424415288077}
Step 9740: {'loss': 1.3197, 'grad_norm': 3.1020867824554443, 'learning_rate': 3.703175508651835e-06, 'epoch': 2.778094694808899}
Step 9750: {'loss': 1.1713, 'grad_norm': 3.549041986465454, 'learning_rate': 3.6556379539836476e-06, 'epoch': 2.7809469480889906}
Step 9760: {'loss': 1.2229, 'grad_norm': 3.395453453063965, 'learning_rate': 3.6081003993154597e-06, 'epoch': 2.7837992013690815}
Step 9770: {'loss': 1.1667, 'grad_norm': 2.9524624347686768, 'learning_rate': 3.5605628446472714e-06, 'epoch': 2.786651454649173}
Step 9780: {'loss': 1.331, 'grad_norm': 3.3834891319274902, 'learning_rate': 3.5130252899790835e-06, 'epoch': 2.7895037079292644}
Step 9790: {'loss': 1.1404, 'grad_norm': 2.8215060234069824, 'learning_rate': 3.465487735310896e-06, 'epoch': 2.7923559612093554}
Step 9800: {'loss': 1.1802, 'grad_norm': 3.1385226249694824, 'learning_rate': 3.417950180642708e-06, 'epoch': 2.795208214489447}
Step 9810: {'loss': 1.2307, 'grad_norm': 3.165700912475586, 'learning_rate': 3.37041262597452e-06, 'epoch': 2.798060467769538}
Step 9820: {'loss': 1.2408, 'grad_norm': 3.014508008956909, 'learning_rate': 3.322875071306332e-06, 'epoch': 2.800912721049629}
Step 9830: {'loss': 1.1081, 'grad_norm': 2.741699457168579, 'learning_rate': 3.275337516638145e-06, 'epoch': 2.8037649743297206}
Step 9840: {'loss': 1.1106, 'grad_norm': 2.6179680824279785, 'learning_rate': 3.2277999619699565e-06, 'epoch': 2.806617227609812}
Step 9850: {'loss': 1.0736, 'grad_norm': 2.7495429515838623, 'learning_rate': 3.1802624073017686e-06, 'epoch': 2.809469480889903}
Step 9860: {'loss': 1.1262, 'grad_norm': 3.2878010272979736, 'learning_rate': 3.1327248526335807e-06, 'epoch': 2.8123217341699944}
Step 9870: {'loss': 1.1763, 'grad_norm': 3.112027406692505, 'learning_rate': 3.0851872979653928e-06, 'epoch': 2.8151739874500854}
Step 9880: {'loss': 1.3276, 'grad_norm': 3.226531982421875, 'learning_rate': 3.0376497432972053e-06, 'epoch': 2.818026240730177}
Step 9890: {'loss': 1.1409, 'grad_norm': 4.250788688659668, 'learning_rate': 2.990112188629017e-06, 'epoch': 2.8208784940102682}
Step 9900: {'loss': 1.2222, 'grad_norm': 3.713608503341675, 'learning_rate': 2.9425746339608295e-06, 'epoch': 2.823730747290359}
Step 9910: {'loss': 1.1217, 'grad_norm': 2.7881534099578857, 'learning_rate': 2.895037079292641e-06, 'epoch': 2.8265830005704506}
Step 9920: {'loss': 1.1908, 'grad_norm': 3.406378984451294, 'learning_rate': 2.8474995246244537e-06, 'epoch': 2.829435253850542}
Step 9930: {'loss': 1.2887, 'grad_norm': 2.846160650253296, 'learning_rate': 2.7999619699562653e-06, 'epoch': 2.832287507130633}
Step 9940: {'loss': 1.174, 'grad_norm': 3.0537407398223877, 'learning_rate': 2.752424415288078e-06, 'epoch': 2.8351397604107245}
Step 9950: {'loss': 1.2078, 'grad_norm': 3.6104350090026855, 'learning_rate': 2.70488686061989e-06, 'epoch': 2.837992013690816}
Step 9960: {'loss': 1.1245, 'grad_norm': 4.680168628692627, 'learning_rate': 2.657349305951702e-06, 'epoch': 2.840844266970907}
Step 9970: {'loss': 1.1601, 'grad_norm': 2.9360551834106445, 'learning_rate': 2.609811751283514e-06, 'epoch': 2.8436965202509983}
Step 9980: {'loss': 1.1197, 'grad_norm': 3.254506826400757, 'learning_rate': 2.5622741966153262e-06, 'epoch': 2.8465487735310897}
Step 9990: {'loss': 1.2522, 'grad_norm': 2.8111517429351807, 'learning_rate': 2.5147366419471383e-06, 'epoch': 2.8494010268111807}
Step 10000: {'loss': 1.2796, 'grad_norm': 3.0872480869293213, 'learning_rate': 2.4671990872789504e-06, 'epoch': 2.852253280091272}
Step 10010: {'loss': 1.0605, 'grad_norm': 2.5147998332977295, 'learning_rate': 2.4196615326107625e-06, 'epoch': 2.8551055333713635}
Step 10020: {'loss': 1.2289, 'grad_norm': 2.610297203063965, 'learning_rate': 2.372123977942575e-06, 'epoch': 2.8579577866514545}
Step 10030: {'loss': 1.2398, 'grad_norm': 3.021034002304077, 'learning_rate': 2.3245864232743867e-06, 'epoch': 2.860810039931546}
Step 10040: {'loss': 1.1491, 'grad_norm': 3.4590961933135986, 'learning_rate': 2.2770488686061992e-06, 'epoch': 2.8636622932116373}
Step 10050: {'loss': 1.235, 'grad_norm': 3.0052311420440674, 'learning_rate': 2.229511313938011e-06, 'epoch': 2.8665145464917283}
Step 10060: {'loss': 1.2171, 'grad_norm': 2.6189143657684326, 'learning_rate': 2.1819737592698234e-06, 'epoch': 2.8693667997718197}
Step 10070: {'loss': 1.3055, 'grad_norm': 2.9783382415771484, 'learning_rate': 2.1344362046016355e-06, 'epoch': 2.872219053051911}
Step 10080: {'loss': 1.1453, 'grad_norm': 2.9962849617004395, 'learning_rate': 2.0868986499334476e-06, 'epoch': 2.875071306332002}
Step 10090: {'loss': 1.2348, 'grad_norm': 2.7765679359436035, 'learning_rate': 2.0393610952652597e-06, 'epoch': 2.8779235596120936}
Step 10100: {'loss': 1.1877, 'grad_norm': 2.7195589542388916, 'learning_rate': 1.991823540597072e-06, 'epoch': 2.880775812892185}
Step 10110: {'loss': 1.1792, 'grad_norm': 2.957412004470825, 'learning_rate': 1.944285985928884e-06, 'epoch': 2.883628066172276}
Step 10120: {'loss': 1.2615, 'grad_norm': 3.124119520187378, 'learning_rate': 1.8967484312606962e-06, 'epoch': 2.8864803194523674}
Step 10130: {'loss': 1.2373, 'grad_norm': 2.938680648803711, 'learning_rate': 1.849210876592508e-06, 'epoch': 2.889332572732459}
Step 10140: {'loss': 1.1389, 'grad_norm': 2.892840623855591, 'learning_rate': 1.8016733219243204e-06, 'epoch': 2.89218482601255}
Step 10150: {'loss': 1.191, 'grad_norm': 2.4995734691619873, 'learning_rate': 1.7541357672561323e-06, 'epoch': 2.895037079292641}
Step 10160: {'loss': 1.3671, 'grad_norm': 2.89280104637146, 'learning_rate': 1.7065982125879446e-06, 'epoch': 2.8978893325727326}
Step 10170: {'loss': 1.2841, 'grad_norm': 3.399779796600342, 'learning_rate': 1.6590606579197567e-06, 'epoch': 2.9007415858528236}
Step 10180: {'loss': 1.1705, 'grad_norm': 2.957777500152588, 'learning_rate': 1.611523103251569e-06, 'epoch': 2.903593839132915}
Step 10190: {'loss': 1.1282, 'grad_norm': 2.7684218883514404, 'learning_rate': 1.5639855485833809e-06, 'epoch': 2.906446092413006}
Step 10200: {'loss': 1.2139, 'grad_norm': 2.782137870788574, 'learning_rate': 1.516447993915193e-06, 'epoch': 2.9092983456930974}
Step 10210: {'loss': 1.2187, 'grad_norm': 3.15582537651062, 'learning_rate': 1.468910439247005e-06, 'epoch': 2.912150598973189}
Step 10220: {'loss': 1.1277, 'grad_norm': 2.86041259765625, 'learning_rate': 1.4213728845788174e-06, 'epoch': 2.91500285225328}
Step 10230: {'loss': 1.2129, 'grad_norm': 2.7043957710266113, 'learning_rate': 1.3738353299106295e-06, 'epoch': 2.9178551055333712}
Step 10240: {'loss': 1.2846, 'grad_norm': 2.936668872833252, 'learning_rate': 1.3262977752424416e-06, 'epoch': 2.9207073588134627}
Step 10250: {'loss': 1.3056, 'grad_norm': 2.8004257678985596, 'learning_rate': 1.2787602205742537e-06, 'epoch': 2.9235596120935536}
Step 10260: {'loss': 1.134, 'grad_norm': 3.210751533508301, 'learning_rate': 1.2312226659060658e-06, 'epoch': 2.926411865373645}
Step 10270: {'loss': 1.2924, 'grad_norm': 3.1425790786743164, 'learning_rate': 1.1836851112378779e-06, 'epoch': 2.9292641186537365}
Step 10280: {'loss': 1.2108, 'grad_norm': 2.6004621982574463, 'learning_rate': 1.1361475565696902e-06, 'epoch': 2.9321163719338275}
Step 10290: {'loss': 1.1348, 'grad_norm': 2.7077484130859375, 'learning_rate': 1.0886100019015023e-06, 'epoch': 2.934968625213919}
Step 10300: {'loss': 1.2041, 'grad_norm': 2.8547656536102295, 'learning_rate': 1.0410724472333144e-06, 'epoch': 2.9378208784940103}
Step 10310: {'loss': 1.2477, 'grad_norm': 2.943934917449951, 'learning_rate': 9.935348925651264e-07, 'epoch': 2.9406731317741013}
Step 10320: {'loss': 1.1722, 'grad_norm': 2.784719228744507, 'learning_rate': 9.459973378969385e-07, 'epoch': 2.9435253850541927}
Step 10330: {'loss': 1.1901, 'grad_norm': 2.7677481174468994, 'learning_rate': 8.984597832287507e-07, 'epoch': 2.946377638334284}
Step 10340: {'loss': 1.2372, 'grad_norm': 2.9188175201416016, 'learning_rate': 8.509222285605628e-07, 'epoch': 2.949229891614375}
Step 10350: {'loss': 1.324, 'grad_norm': 3.0544188022613525, 'learning_rate': 8.033846738923749e-07, 'epoch': 2.9520821448944665}
Step 10360: {'loss': 1.184, 'grad_norm': 2.7733302116394043, 'learning_rate': 7.558471192241871e-07, 'epoch': 2.954934398174558}
Step 10370: {'loss': 1.197, 'grad_norm': 2.2452874183654785, 'learning_rate': 7.083095645559993e-07, 'epoch': 2.957786651454649}
Step 10380: {'loss': 1.2342, 'grad_norm': 2.9540228843688965, 'learning_rate': 6.607720098878114e-07, 'epoch': 2.9606389047347403}
Step 10390: {'loss': 1.1962, 'grad_norm': 2.9708523750305176, 'learning_rate': 6.132344552196235e-07, 'epoch': 2.9634911580148318}
Step 10400: {'loss': 1.3034, 'grad_norm': 3.0975677967071533, 'learning_rate': 5.656969005514357e-07, 'epoch': 2.9663434112949227}
Step 10410: {'loss': 1.0983, 'grad_norm': 2.7289109230041504, 'learning_rate': 5.181593458832478e-07, 'epoch': 2.969195664575014}
Step 10420: {'loss': 1.3076, 'grad_norm': 3.1311838626861572, 'learning_rate': 4.7062179121505997e-07, 'epoch': 2.9720479178551056}
Step 10430: {'loss': 1.3648, 'grad_norm': 3.4728569984436035, 'learning_rate': 4.2308423654687207e-07, 'epoch': 2.9749001711351966}
Step 10440: {'loss': 1.2276, 'grad_norm': 3.45552921295166, 'learning_rate': 3.7554668187868416e-07, 'epoch': 2.977752424415288}
Step 10450: {'loss': 1.2757, 'grad_norm': 3.141314744949341, 'learning_rate': 3.280091272104963e-07, 'epoch': 2.9806046776953794}
Step 10460: {'loss': 1.1939, 'grad_norm': 3.0153937339782715, 'learning_rate': 2.804715725423084e-07, 'epoch': 2.9834569309754704}
Step 10470: {'loss': 1.1964, 'grad_norm': 3.2051072120666504, 'learning_rate': 2.3293401787412055e-07, 'epoch': 2.986309184255562}
Step 10480: {'loss': 1.1395, 'grad_norm': 3.6005699634552, 'learning_rate': 1.853964632059327e-07, 'epoch': 2.989161437535653}
Step 10490: {'loss': 1.3346, 'grad_norm': 3.058157444000244, 'learning_rate': 1.3785890853774482e-07, 'epoch': 2.992013690815744}
Step 10500: {'loss': 1.2279, 'grad_norm': 2.3984763622283936, 'learning_rate': 9.032135386955696e-08, 'epoch': 2.9948659440958356}
Step 10510: {'loss': 1.2404, 'grad_norm': 3.114846706390381, 'learning_rate': 4.2783799201369086e-08, 'epoch': 2.997718197375927}
Epoch 3.0 completed in 7253.09s
Step 10518: {'eval_loss': 1.2608414888381958, 'eval_runtime': 69.0663, 'eval_samples_per_second': 22.558, 'eval_steps_per_second': 5.647, 'epoch': 3.0}
Step 10518: {'train_runtime': 7322.9603, 'train_samples_per_second': 5.744, 'train_steps_per_second': 1.436, 'total_flos': 1373963638800384.0, 'train_loss': 1.351598309935624, 'epoch': 3.0}
=== Training completed in 7322.96s ===

Step 10518: {'eval_loss': 1.2608414888381958, 'eval_runtime': 68.7904, 'eval_samples_per_second': 22.649, 'eval_steps_per_second': 5.669, 'epoch': 3.0}
