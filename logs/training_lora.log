=== Training started at 2025-12-11 01:18:04 ===
Step 10: {'loss': 3.9368, 'grad_norm': 1.5187633037567139, 'learning_rate': 0.0004986666666666667, 'epoch': 0.008888888888888889}
Step 20: {'loss': 3.1443, 'grad_norm': 1.921229362487793, 'learning_rate': 0.0004971851851851851, 'epoch': 0.017777777777777778}
Step 30: {'loss': 2.6153, 'grad_norm': 2.305107831954956, 'learning_rate': 0.0004957037037037037, 'epoch': 0.02666666666666667}
Step 40: {'loss': 2.2869, 'grad_norm': 2.3213038444519043, 'learning_rate': 0.0004942222222222223, 'epoch': 0.035555555555555556}
Step 50: {'loss': 2.0649, 'grad_norm': 1.9509708881378174, 'learning_rate': 0.0004927407407407407, 'epoch': 0.044444444444444446}
Step 60: {'loss': 1.9049, 'grad_norm': 2.3734796047210693, 'learning_rate': 0.0004912592592592593, 'epoch': 0.05333333333333334}
Step 70: {'loss': 1.6621, 'grad_norm': 2.317460298538208, 'learning_rate': 0.0004897777777777778, 'epoch': 0.06222222222222222}
Step 80: {'loss': 1.733, 'grad_norm': 2.615285873413086, 'learning_rate': 0.0004882962962962962, 'epoch': 0.07111111111111111}
Step 90: {'loss': 1.5233, 'grad_norm': 1.8052304983139038, 'learning_rate': 0.0004868148148148148, 'epoch': 0.08}
Step 100: {'loss': 1.4869, 'grad_norm': 1.5270277261734009, 'learning_rate': 0.00048533333333333333, 'epoch': 0.08888888888888889}
Step 110: {'loss': 1.5019, 'grad_norm': 1.5532441139221191, 'learning_rate': 0.0004838518518518519, 'epoch': 0.09777777777777778}
Step 120: {'loss': 1.4751, 'grad_norm': 1.4558286666870117, 'learning_rate': 0.0004823703703703704, 'epoch': 0.10666666666666667}
Step 130: {'loss': 1.3302, 'grad_norm': 1.4693628549575806, 'learning_rate': 0.0004808888888888889, 'epoch': 0.11555555555555555}
Step 140: {'loss': 1.428, 'grad_norm': 1.5813913345336914, 'learning_rate': 0.0004794074074074074, 'epoch': 0.12444444444444444}
Step 150: {'loss': 1.3144, 'grad_norm': 2.211899518966675, 'learning_rate': 0.0004779259259259259, 'epoch': 0.13333333333333333}
Step 160: {'loss': 1.3447, 'grad_norm': 1.2929925918579102, 'learning_rate': 0.0004764444444444445, 'epoch': 0.14222222222222222}
Step 170: {'loss': 1.4144, 'grad_norm': 1.2812739610671997, 'learning_rate': 0.000474962962962963, 'epoch': 0.1511111111111111}
Step 180: {'loss': 1.294, 'grad_norm': 1.2392863035202026, 'learning_rate': 0.0004734814814814815, 'epoch': 0.16}
Step 190: {'loss': 1.3312, 'grad_norm': 1.290372610092163, 'learning_rate': 0.000472, 'epoch': 0.1688888888888889}
Step 200: {'loss': 1.3, 'grad_norm': 1.2196334600448608, 'learning_rate': 0.0004705185185185185, 'epoch': 0.17777777777777778}
Step 210: {'loss': 1.2787, 'grad_norm': 3.055051326751709, 'learning_rate': 0.000469037037037037, 'epoch': 0.18666666666666668}
Step 220: {'loss': 1.2029, 'grad_norm': 1.3616713285446167, 'learning_rate': 0.0004675555555555556, 'epoch': 0.19555555555555557}
Step 230: {'loss': 1.2205, 'grad_norm': 1.2705802917480469, 'learning_rate': 0.0004660740740740741, 'epoch': 0.20444444444444446}
Step 240: {'loss': 1.1563, 'grad_norm': 1.7211823463439941, 'learning_rate': 0.0004645925925925926, 'epoch': 0.21333333333333335}
Step 250: {'loss': 1.2203, 'grad_norm': 1.0735671520233154, 'learning_rate': 0.0004631111111111111, 'epoch': 0.2222222222222222}
Step 260: {'loss': 1.3152, 'grad_norm': 0.9877790212631226, 'learning_rate': 0.0004616296296296296, 'epoch': 0.2311111111111111}
Step 270: {'loss': 1.1717, 'grad_norm': 1.2614772319793701, 'learning_rate': 0.00046014814814814817, 'epoch': 0.24}
Step 280: {'loss': 1.2477, 'grad_norm': 1.620925784111023, 'learning_rate': 0.0004586666666666667, 'epoch': 0.24888888888888888}
Step 290: {'loss': 1.2853, 'grad_norm': 1.2909538745880127, 'learning_rate': 0.0004571851851851852, 'epoch': 0.2577777777777778}
Step 300: {'loss': 1.3062, 'grad_norm': 0.7778041362762451, 'learning_rate': 0.0004557037037037037, 'epoch': 0.26666666666666666}
Step 310: {'loss': 1.2622, 'grad_norm': 0.8654189705848694, 'learning_rate': 0.00045422222222222223, 'epoch': 0.27555555555555555}
Step 320: {'loss': 1.2371, 'grad_norm': 0.8031435608863831, 'learning_rate': 0.00045274074074074075, 'epoch': 0.28444444444444444}
Step 330: {'loss': 1.2426, 'grad_norm': 0.8505838513374329, 'learning_rate': 0.00045125925925925927, 'epoch': 0.29333333333333333}
Step 340: {'loss': 1.1678, 'grad_norm': 0.9757272601127625, 'learning_rate': 0.0004497777777777778, 'epoch': 0.3022222222222222}
Step 350: {'loss': 1.2132, 'grad_norm': 0.8248431086540222, 'learning_rate': 0.0004482962962962963, 'epoch': 0.3111111111111111}
Step 360: {'loss': 1.1975, 'grad_norm': 0.7876424789428711, 'learning_rate': 0.0004468148148148148, 'epoch': 0.32}
Step 370: {'loss': 1.215, 'grad_norm': 0.8138487935066223, 'learning_rate': 0.0004453333333333334, 'epoch': 0.3288888888888889}
Step 380: {'loss': 1.2204, 'grad_norm': 0.7376484274864197, 'learning_rate': 0.00044385185185185185, 'epoch': 0.3377777777777778}
Step 390: {'loss': 1.1954, 'grad_norm': 0.8737006187438965, 'learning_rate': 0.00044237037037037037, 'epoch': 0.3466666666666667}
Step 400: {'loss': 1.1913, 'grad_norm': 0.8186482191085815, 'learning_rate': 0.0004408888888888889, 'epoch': 0.35555555555555557}
Step 410: {'loss': 1.2417, 'grad_norm': 0.634283721446991, 'learning_rate': 0.0004394074074074074, 'epoch': 0.36444444444444446}
Step 420: {'loss': 1.2731, 'grad_norm': 0.7062112092971802, 'learning_rate': 0.00043792592592592597, 'epoch': 0.37333333333333335}
Step 430: {'loss': 1.2829, 'grad_norm': 0.6213342547416687, 'learning_rate': 0.0004364444444444445, 'epoch': 0.38222222222222224}
Step 440: {'loss': 1.2747, 'grad_norm': 0.7779673933982849, 'learning_rate': 0.00043496296296296295, 'epoch': 0.39111111111111113}
Step 450: {'loss': 1.2157, 'grad_norm': 1.2159532308578491, 'learning_rate': 0.00043348148148148147, 'epoch': 0.4}
Step 460: {'loss': 1.0859, 'grad_norm': 1.441068410873413, 'learning_rate': 0.000432, 'epoch': 0.4088888888888889}
Step 470: {'loss': 1.1663, 'grad_norm': 0.7452889680862427, 'learning_rate': 0.00043051851851851855, 'epoch': 0.4177777777777778}
Step 480: {'loss': 1.1535, 'grad_norm': 0.7527121305465698, 'learning_rate': 0.00042903703703703707, 'epoch': 0.4266666666666667}
Step 490: {'loss': 1.2078, 'grad_norm': 0.8365089893341064, 'learning_rate': 0.0004275555555555556, 'epoch': 0.43555555555555553}
Step 500: {'loss': 1.1452, 'grad_norm': 0.7059004902839661, 'learning_rate': 0.00042607407407407405, 'epoch': 0.4444444444444444}
Step 510: {'loss': 1.1663, 'grad_norm': 0.7257433533668518, 'learning_rate': 0.00042459259259259257, 'epoch': 0.4533333333333333}
Step 520: {'loss': 1.2917, 'grad_norm': 0.5301205515861511, 'learning_rate': 0.00042311111111111114, 'epoch': 0.4622222222222222}
Step 530: {'loss': 1.158, 'grad_norm': 0.977413535118103, 'learning_rate': 0.00042162962962962965, 'epoch': 0.4711111111111111}
Step 540: {'loss': 1.1937, 'grad_norm': 0.6786495447158813, 'learning_rate': 0.00042014814814814817, 'epoch': 0.48}
Step 550: {'loss': 1.1087, 'grad_norm': 0.6363920569419861, 'learning_rate': 0.0004186666666666667, 'epoch': 0.4888888888888889}
Step 560: {'loss': 1.1304, 'grad_norm': 0.5211119055747986, 'learning_rate': 0.00041718518518518515, 'epoch': 0.49777777777777776}
Step 570: {'loss': 1.1485, 'grad_norm': 0.7371568083763123, 'learning_rate': 0.0004157037037037037, 'epoch': 0.5066666666666667}
Step 580: {'loss': 1.169, 'grad_norm': 1.5489081144332886, 'learning_rate': 0.00041422222222222224, 'epoch': 0.5155555555555555}
Step 590: {'loss': 1.1631, 'grad_norm': 0.6287077069282532, 'learning_rate': 0.00041274074074074075, 'epoch': 0.5244444444444445}
Step 600: {'loss': 1.2162, 'grad_norm': 0.6904585957527161, 'learning_rate': 0.00041125925925925927, 'epoch': 0.5333333333333333}
Step 610: {'loss': 1.1873, 'grad_norm': 0.6785797476768494, 'learning_rate': 0.0004097777777777778, 'epoch': 0.5422222222222223}
Step 620: {'loss': 1.2015, 'grad_norm': 0.6805096864700317, 'learning_rate': 0.00040829629629629636, 'epoch': 0.5511111111111111}
Step 630: {'loss': 1.123, 'grad_norm': 0.6129673719406128, 'learning_rate': 0.0004068148148148148, 'epoch': 0.56}
Step 640: {'loss': 1.1775, 'grad_norm': 0.6517429351806641, 'learning_rate': 0.00040533333333333334, 'epoch': 0.5688888888888889}
Step 650: {'loss': 1.1514, 'grad_norm': 0.6019984483718872, 'learning_rate': 0.00040385185185185185, 'epoch': 0.5777777777777777}
Step 660: {'loss': 1.1994, 'grad_norm': 0.6058477759361267, 'learning_rate': 0.00040237037037037037, 'epoch': 0.5866666666666667}
Step 670: {'loss': 1.2826, 'grad_norm': 0.5686356425285339, 'learning_rate': 0.0004008888888888889, 'epoch': 0.5955555555555555}
Step 680: {'loss': 1.1475, 'grad_norm': 0.6086797714233398, 'learning_rate': 0.00039940740740740746, 'epoch': 0.6044444444444445}
Step 690: {'loss': 1.0684, 'grad_norm': 0.6372100114822388, 'learning_rate': 0.0003979259259259259, 'epoch': 0.6133333333333333}
Step 700: {'loss': 1.0895, 'grad_norm': 0.7869500517845154, 'learning_rate': 0.00039644444444444444, 'epoch': 0.6222222222222222}
Step 710: {'loss': 1.1646, 'grad_norm': 0.6334000825881958, 'learning_rate': 0.00039496296296296295, 'epoch': 0.6311111111111111}
Step 720: {'loss': 1.1768, 'grad_norm': 0.6485788822174072, 'learning_rate': 0.00039348148148148147, 'epoch': 0.64}
Step 730: {'loss': 1.2095, 'grad_norm': 0.6536946296691895, 'learning_rate': 0.00039200000000000004, 'epoch': 0.6488888888888888}
Step 740: {'loss': 1.2323, 'grad_norm': 0.6300553679466248, 'learning_rate': 0.00039051851851851856, 'epoch': 0.6577777777777778}
Step 750: {'loss': 1.0679, 'grad_norm': 0.722392737865448, 'learning_rate': 0.000389037037037037, 'epoch': 0.6666666666666666}
Step 760: {'loss': 1.0543, 'grad_norm': 0.7854971885681152, 'learning_rate': 0.00038755555555555554, 'epoch': 0.6755555555555556}
Step 770: {'loss': 1.2542, 'grad_norm': 0.7750962972640991, 'learning_rate': 0.00038607407407407405, 'epoch': 0.6844444444444444}
Step 780: {'loss': 1.1739, 'grad_norm': 0.47406357526779175, 'learning_rate': 0.0003845925925925926, 'epoch': 0.6933333333333334}
Step 790: {'loss': 1.2307, 'grad_norm': 0.5022684931755066, 'learning_rate': 0.00038311111111111114, 'epoch': 0.7022222222222222}
Step 800: {'loss': 1.1042, 'grad_norm': 0.7657656073570251, 'learning_rate': 0.00038162962962962966, 'epoch': 0.7111111111111111}
Step 810: {'loss': 1.1224, 'grad_norm': 0.6849040389060974, 'learning_rate': 0.0003801481481481481, 'epoch': 0.72}
Step 820: {'loss': 1.1383, 'grad_norm': 1.6529905796051025, 'learning_rate': 0.00037866666666666664, 'epoch': 0.7288888888888889}
Step 830: {'loss': 1.2053, 'grad_norm': 0.5837745666503906, 'learning_rate': 0.0003771851851851852, 'epoch': 0.7377777777777778}
Step 840: {'loss': 1.2896, 'grad_norm': 0.669634222984314, 'learning_rate': 0.0003757037037037037, 'epoch': 0.7466666666666667}
Step 850: {'loss': 1.1861, 'grad_norm': 0.6705278158187866, 'learning_rate': 0.00037422222222222224, 'epoch': 0.7555555555555555}
Step 860: {'loss': 1.1581, 'grad_norm': 0.535019040107727, 'learning_rate': 0.00037274074074074076, 'epoch': 0.7644444444444445}
Step 870: {'loss': 1.1762, 'grad_norm': 0.5988060235977173, 'learning_rate': 0.0003712592592592592, 'epoch': 0.7733333333333333}
Step 880: {'loss': 1.2316, 'grad_norm': 0.6131390929222107, 'learning_rate': 0.0003697777777777778, 'epoch': 0.7822222222222223}
Step 890: {'loss': 1.177, 'grad_norm': 0.5181124806404114, 'learning_rate': 0.0003682962962962963, 'epoch': 0.7911111111111111}
Step 900: {'loss': 1.1298, 'grad_norm': 0.5533860325813293, 'learning_rate': 0.0003668148148148148, 'epoch': 0.8}
Step 910: {'loss': 1.1069, 'grad_norm': 0.5172684788703918, 'learning_rate': 0.00036533333333333334, 'epoch': 0.8088888888888889}
Step 920: {'loss': 1.2217, 'grad_norm': 0.5237042903900146, 'learning_rate': 0.00036385185185185186, 'epoch': 0.8177777777777778}
Step 930: {'loss': 1.1777, 'grad_norm': 0.7458787560462952, 'learning_rate': 0.0003623703703703704, 'epoch': 0.8266666666666667}
Step 940: {'loss': 1.0925, 'grad_norm': 0.6666601300239563, 'learning_rate': 0.0003608888888888889, 'epoch': 0.8355555555555556}
Step 950: {'loss': 1.1596, 'grad_norm': 0.7144873142242432, 'learning_rate': 0.0003594074074074074, 'epoch': 0.8444444444444444}
Step 960: {'loss': 1.1071, 'grad_norm': 0.4606259763240814, 'learning_rate': 0.0003579259259259259, 'epoch': 0.8533333333333334}
Step 970: {'loss': 1.1341, 'grad_norm': 0.6030312180519104, 'learning_rate': 0.00035644444444444444, 'epoch': 0.8622222222222222}
Step 980: {'loss': 1.2136, 'grad_norm': 0.7384686470031738, 'learning_rate': 0.000354962962962963, 'epoch': 0.8711111111111111}
Step 990: {'loss': 1.2467, 'grad_norm': 0.6202930212020874, 'learning_rate': 0.00035348148148148153, 'epoch': 0.88}
Step 1000: {'loss': 1.1625, 'grad_norm': 0.44700565934181213, 'learning_rate': 0.000352, 'epoch': 0.8888888888888888}
Step 1010: {'loss': 1.1086, 'grad_norm': 0.5988543033599854, 'learning_rate': 0.0003505185185185185, 'epoch': 0.8977777777777778}
Step 1020: {'loss': 1.1323, 'grad_norm': 0.5897933840751648, 'learning_rate': 0.000349037037037037, 'epoch': 0.9066666666666666}
Step 1030: {'loss': 1.1402, 'grad_norm': 0.580268144607544, 'learning_rate': 0.0003475555555555556, 'epoch': 0.9155555555555556}
Step 1040: {'loss': 1.1327, 'grad_norm': 0.5769871473312378, 'learning_rate': 0.0003460740740740741, 'epoch': 0.9244444444444444}
Step 1050: {'loss': 1.1233, 'grad_norm': 0.7005117535591125, 'learning_rate': 0.00034459259259259263, 'epoch': 0.9333333333333333}
Step 1060: {'loss': 1.1827, 'grad_norm': 0.6006100177764893, 'learning_rate': 0.0003431111111111111, 'epoch': 0.9422222222222222}
Step 1070: {'loss': 1.2191, 'grad_norm': 0.6751521825790405, 'learning_rate': 0.0003416296296296296, 'epoch': 0.9511111111111111}
Step 1080: {'loss': 1.0829, 'grad_norm': 0.7048794031143188, 'learning_rate': 0.0003401481481481481, 'epoch': 0.96}
Step 1090: {'loss': 1.1516, 'grad_norm': 0.6126890182495117, 'learning_rate': 0.0003386666666666667, 'epoch': 0.9688888888888889}
Step 1100: {'loss': 1.1777, 'grad_norm': 0.7539003491401672, 'learning_rate': 0.0003371851851851852, 'epoch': 0.9777777777777777}
Step 1110: {'loss': 1.0662, 'grad_norm': 0.48331406712532043, 'learning_rate': 0.00033570370370370373, 'epoch': 0.9866666666666667}
Step 1120: {'loss': 1.2569, 'grad_norm': 0.5954473614692688, 'learning_rate': 0.0003342222222222222, 'epoch': 0.9955555555555555}
Epoch 1.0 completed in 6730.00s
Step 1125: {'eval_loss': 1.0919054746627808, 'eval_runtime': 199.6305, 'eval_samples_per_second': 2.505, 'eval_steps_per_second': 0.626, 'epoch': 1.0}
Step 1130: {'loss': 1.1927, 'grad_norm': 0.5857274532318115, 'learning_rate': 0.0003327407407407407, 'epoch': 1.0044444444444445}
Step 1140: {'loss': 1.0529, 'grad_norm': 0.5546720027923584, 'learning_rate': 0.0003312592592592593, 'epoch': 1.0133333333333334}
Step 1150: {'loss': 1.0913, 'grad_norm': 0.6525000333786011, 'learning_rate': 0.0003297777777777778, 'epoch': 1.0222222222222221}
Step 1160: {'loss': 1.1673, 'grad_norm': 0.7043499946594238, 'learning_rate': 0.0003282962962962963, 'epoch': 1.031111111111111}
Step 1170: {'loss': 1.1528, 'grad_norm': 0.7834587097167969, 'learning_rate': 0.00032681481481481483, 'epoch': 1.04}
Step 1180: {'loss': 1.1226, 'grad_norm': 0.5575093030929565, 'learning_rate': 0.0003253333333333333, 'epoch': 1.048888888888889}
Step 1190: {'loss': 1.2683, 'grad_norm': 0.5291751623153687, 'learning_rate': 0.00032385185185185186, 'epoch': 1.0577777777777777}
Step 1200: {'loss': 1.0864, 'grad_norm': 0.6800472140312195, 'learning_rate': 0.0003223703703703704, 'epoch': 1.0666666666666667}
Step 1210: {'loss': 1.1074, 'grad_norm': 0.695279061794281, 'learning_rate': 0.0003208888888888889, 'epoch': 1.0755555555555556}
Step 1220: {'loss': 0.9898, 'grad_norm': 0.47909462451934814, 'learning_rate': 0.0003194074074074074, 'epoch': 1.0844444444444445}
Step 1230: {'loss': 1.0952, 'grad_norm': 0.8694149851799011, 'learning_rate': 0.00031792592592592593, 'epoch': 1.0933333333333333}
Step 1240: {'loss': 1.0373, 'grad_norm': 0.4686163663864136, 'learning_rate': 0.00031644444444444444, 'epoch': 1.1022222222222222}
Step 1250: {'loss': 1.1748, 'grad_norm': 0.6689775586128235, 'learning_rate': 0.00031496296296296296, 'epoch': 1.1111111111111112}
Step 1260: {'loss': 1.1062, 'grad_norm': 0.6401200890541077, 'learning_rate': 0.0003134814814814815, 'epoch': 1.12}
Step 1270: {'loss': 1.0501, 'grad_norm': 0.5773656368255615, 'learning_rate': 0.000312, 'epoch': 1.1288888888888888}
Step 1280: {'loss': 1.0694, 'grad_norm': 0.51015704870224, 'learning_rate': 0.0003105185185185185, 'epoch': 1.1377777777777778}
Step 1290: {'loss': 1.0448, 'grad_norm': 0.48990440368652344, 'learning_rate': 0.0003090370370370371, 'epoch': 1.1466666666666667}
Step 1300: {'loss': 1.0786, 'grad_norm': 0.546768069267273, 'learning_rate': 0.0003075555555555556, 'epoch': 1.1555555555555554}
Step 1310: {'loss': 1.0317, 'grad_norm': 0.5559491515159607, 'learning_rate': 0.00030607407407407406, 'epoch': 1.1644444444444444}
Step 1320: {'loss': 1.0161, 'grad_norm': 0.7596527934074402, 'learning_rate': 0.0003045925925925926, 'epoch': 1.1733333333333333}
Step 1330: {'loss': 1.1657, 'grad_norm': 0.5110783576965332, 'learning_rate': 0.0003031111111111111, 'epoch': 1.1822222222222223}
Step 1340: {'loss': 1.1589, 'grad_norm': 0.5233128070831299, 'learning_rate': 0.00030162962962962967, 'epoch': 1.1911111111111112}
Step 1350: {'loss': 1.185, 'grad_norm': 0.5702590942382812, 'learning_rate': 0.0003001481481481482, 'epoch': 1.2}
Step 1360: {'loss': 1.1692, 'grad_norm': 0.7140635251998901, 'learning_rate': 0.0002986666666666667, 'epoch': 1.208888888888889}
Step 1370: {'loss': 1.092, 'grad_norm': 0.5328340530395508, 'learning_rate': 0.00029718518518518516, 'epoch': 1.2177777777777778}
Step 1380: {'loss': 1.221, 'grad_norm': 0.5409652590751648, 'learning_rate': 0.0002957037037037037, 'epoch': 1.2266666666666666}
Step 1390: {'loss': 1.1499, 'grad_norm': 0.5060092210769653, 'learning_rate': 0.00029422222222222225, 'epoch': 1.2355555555555555}
Step 1400: {'loss': 1.1197, 'grad_norm': 0.5932028889656067, 'learning_rate': 0.00029274074074074076, 'epoch': 1.2444444444444445}
Step 1410: {'loss': 1.0779, 'grad_norm': 0.5041168928146362, 'learning_rate': 0.0002912592592592593, 'epoch': 1.2533333333333334}
Step 1420: {'loss': 1.0998, 'grad_norm': 0.6368439197540283, 'learning_rate': 0.0002897777777777778, 'epoch': 1.2622222222222224}
Step 1430: {'loss': 1.1632, 'grad_norm': 0.4713922142982483, 'learning_rate': 0.00028829629629629626, 'epoch': 1.271111111111111}
Step 1440: {'loss': 1.1109, 'grad_norm': 0.5890001654624939, 'learning_rate': 0.00028681481481481483, 'epoch': 1.28}
Step 1450: {'loss': 1.2131, 'grad_norm': 0.530438244342804, 'learning_rate': 0.00028533333333333335, 'epoch': 1.2888888888888888}
Step 1460: {'loss': 1.1717, 'grad_norm': 0.5979163646697998, 'learning_rate': 0.00028385185185185186, 'epoch': 1.2977777777777777}
Step 1470: {'loss': 1.1568, 'grad_norm': 0.6881266236305237, 'learning_rate': 0.0002823703703703704, 'epoch': 1.3066666666666666}
Step 1480: {'loss': 1.0154, 'grad_norm': 0.9160618185997009, 'learning_rate': 0.0002808888888888889, 'epoch': 1.3155555555555556}
Step 1490: {'loss': 1.1507, 'grad_norm': 0.43670889735221863, 'learning_rate': 0.0002794074074074074, 'epoch': 1.3244444444444445}
Step 1500: {'loss': 1.0362, 'grad_norm': 0.7389567494392395, 'learning_rate': 0.00027792592592592593, 'epoch': 1.3333333333333333}
Step 1510: {'loss': 1.0859, 'grad_norm': 0.6583133339881897, 'learning_rate': 0.00027644444444444445, 'epoch': 1.3422222222222222}
Step 1520: {'loss': 1.119, 'grad_norm': 0.5426837801933289, 'learning_rate': 0.00027496296296296296, 'epoch': 1.3511111111111112}
Step 1530: {'loss': 1.0752, 'grad_norm': 0.6030901670455933, 'learning_rate': 0.0002734814814814815, 'epoch': 1.3599999999999999}
Step 1540: {'loss': 1.1001, 'grad_norm': 0.5580886006355286, 'learning_rate': 0.00027200000000000005, 'epoch': 1.3688888888888888}
Step 1550: {'loss': 1.1476, 'grad_norm': 0.6018284559249878, 'learning_rate': 0.0002705185185185185, 'epoch': 1.3777777777777778}
Step 1560: {'loss': 1.0431, 'grad_norm': 0.42984360456466675, 'learning_rate': 0.00026903703703703703, 'epoch': 1.3866666666666667}
Step 1570: {'loss': 1.0465, 'grad_norm': 0.5865603685379028, 'learning_rate': 0.00026755555555555555, 'epoch': 1.3955555555555557}
Step 1580: {'loss': 1.1787, 'grad_norm': 0.5680853128433228, 'learning_rate': 0.00026607407407407406, 'epoch': 1.4044444444444444}
Step 1590: {'loss': 1.2633, 'grad_norm': 0.6158344745635986, 'learning_rate': 0.0002645925925925926, 'epoch': 1.4133333333333333}
Step 1600: {'loss': 1.0402, 'grad_norm': 0.4892382323741913, 'learning_rate': 0.00026311111111111115, 'epoch': 1.4222222222222223}
Step 1610: {'loss': 1.0877, 'grad_norm': 0.47734013199806213, 'learning_rate': 0.00026162962962962967, 'epoch': 1.431111111111111}
Step 1620: {'loss': 1.1879, 'grad_norm': 0.6548910140991211, 'learning_rate': 0.00026014814814814813, 'epoch': 1.44}
Step 1630: {'loss': 0.988, 'grad_norm': 0.5617366433143616, 'learning_rate': 0.00025866666666666665, 'epoch': 1.448888888888889}
Step 1640: {'loss': 1.1538, 'grad_norm': 0.49439308047294617, 'learning_rate': 0.00025718518518518516, 'epoch': 1.4577777777777778}
Step 1650: {'loss': 1.0825, 'grad_norm': 0.48714956641197205, 'learning_rate': 0.00025570370370370374, 'epoch': 1.4666666666666668}
Step 1660: {'loss': 1.1061, 'grad_norm': 0.6010589599609375, 'learning_rate': 0.00025422222222222225, 'epoch': 1.4755555555555555}
Step 1670: {'loss': 1.1278, 'grad_norm': 0.5102576017379761, 'learning_rate': 0.00025274074074074077, 'epoch': 1.4844444444444445}
Step 1680: {'loss': 1.1455, 'grad_norm': 0.5471464395523071, 'learning_rate': 0.00025125925925925923, 'epoch': 1.4933333333333334}
Step 1690: {'loss': 1.184, 'grad_norm': 0.5986040234565735, 'learning_rate': 0.00024977777777777775, 'epoch': 1.5022222222222221}
Step 1700: {'loss': 1.0858, 'grad_norm': 0.5470849871635437, 'learning_rate': 0.0002482962962962963, 'epoch': 1.511111111111111}
Step 1710: {'loss': 1.1011, 'grad_norm': 0.5843448042869568, 'learning_rate': 0.00024681481481481483, 'epoch': 1.52}
Step 1720: {'loss': 1.2405, 'grad_norm': 0.49272069334983826, 'learning_rate': 0.00024533333333333335, 'epoch': 1.528888888888889}
Step 1730: {'loss': 1.1208, 'grad_norm': 0.9995911121368408, 'learning_rate': 0.00024385185185185184, 'epoch': 1.537777777777778}
Step 1740: {'loss': 1.2249, 'grad_norm': 0.5691743493080139, 'learning_rate': 0.00024237037037037038, 'epoch': 1.5466666666666666}
Step 1750: {'loss': 1.0595, 'grad_norm': 0.6285366415977478, 'learning_rate': 0.0002408888888888889, 'epoch': 1.5555555555555556}
Step 1760: {'loss': 1.132, 'grad_norm': 0.4639955461025238, 'learning_rate': 0.00023940740740740742, 'epoch': 1.5644444444444443}
Step 1770: {'loss': 1.1883, 'grad_norm': 0.5935875773429871, 'learning_rate': 0.00023792592592592593, 'epoch': 1.5733333333333333}
Step 1780: {'loss': 1.1009, 'grad_norm': 0.5214412212371826, 'learning_rate': 0.00023644444444444445, 'epoch': 1.5822222222222222}
Step 1790: {'loss': 1.1377, 'grad_norm': 0.5033148527145386, 'learning_rate': 0.00023496296296296297, 'epoch': 1.5911111111111111}
Step 1800: {'loss': 1.1583, 'grad_norm': 0.5178150534629822, 'learning_rate': 0.00023348148148148148, 'epoch': 1.6}
Step 1810: {'loss': 1.1016, 'grad_norm': 0.5044939517974854, 'learning_rate': 0.00023200000000000003, 'epoch': 1.608888888888889}
Step 1820: {'loss': 1.1484, 'grad_norm': 0.5484291315078735, 'learning_rate': 0.00023051851851851852, 'epoch': 1.6177777777777778}
Step 1830: {'loss': 1.0443, 'grad_norm': 0.507917046546936, 'learning_rate': 0.00022903703703703703, 'epoch': 1.6266666666666667}
Step 1840: {'loss': 1.0092, 'grad_norm': 0.7357301712036133, 'learning_rate': 0.00022755555555555558, 'epoch': 1.6355555555555554}
Step 1850: {'loss': 1.1758, 'grad_norm': 0.6351987719535828, 'learning_rate': 0.00022607407407407407, 'epoch': 1.6444444444444444}
Step 1860: {'loss': 1.1307, 'grad_norm': 0.556939423084259, 'learning_rate': 0.0002245925925925926, 'epoch': 1.6533333333333333}
Step 1870: {'loss': 1.1498, 'grad_norm': 0.5200446248054504, 'learning_rate': 0.00022311111111111113, 'epoch': 1.6622222222222223}
Step 1880: {'loss': 1.3261, 'grad_norm': 0.5707795023918152, 'learning_rate': 0.00022162962962962962, 'epoch': 1.6711111111111112}
Step 1890: {'loss': 1.1473, 'grad_norm': 0.4825136959552765, 'learning_rate': 0.00022014814814814816, 'epoch': 1.6800000000000002}
Step 1900: {'loss': 1.0952, 'grad_norm': 0.3986184597015381, 'learning_rate': 0.00021866666666666668, 'epoch': 1.6888888888888889}
Step 1910: {'loss': 1.1021, 'grad_norm': 0.4997529983520508, 'learning_rate': 0.00021718518518518517, 'epoch': 1.6977777777777778}
Step 1920: {'loss': 1.0819, 'grad_norm': 0.5749768614768982, 'learning_rate': 0.0002157037037037037, 'epoch': 1.7066666666666666}
Step 1930: {'loss': 1.1135, 'grad_norm': 0.603564441204071, 'learning_rate': 0.00021422222222222223, 'epoch': 1.7155555555555555}
Step 1940: {'loss': 1.0652, 'grad_norm': 0.6559131145477295, 'learning_rate': 0.00021274074074074074, 'epoch': 1.7244444444444444}
Step 1950: {'loss': 1.1425, 'grad_norm': 0.564391016960144, 'learning_rate': 0.00021125925925925926, 'epoch': 1.7333333333333334}
Step 1960: {'loss': 1.1953, 'grad_norm': 0.5798231959342957, 'learning_rate': 0.00020977777777777778, 'epoch': 1.7422222222222223}
Step 1970: {'loss': 1.0372, 'grad_norm': 0.53835129737854, 'learning_rate': 0.0002082962962962963, 'epoch': 1.751111111111111}
Step 1980: {'loss': 1.0766, 'grad_norm': 0.5053632259368896, 'learning_rate': 0.0002068148148148148, 'epoch': 1.76}
Step 1990: {'loss': 1.0907, 'grad_norm': 0.5175740122795105, 'learning_rate': 0.00020533333333333336, 'epoch': 1.7688888888888887}
Step 2000: {'loss': 1.2044, 'grad_norm': 0.7435340285301208, 'learning_rate': 0.00020385185185185184, 'epoch': 1.7777777777777777}
Step 2010: {'loss': 1.1594, 'grad_norm': 0.437649130821228, 'learning_rate': 0.00020237037037037036, 'epoch': 1.7866666666666666}
Step 2020: {'loss': 1.1081, 'grad_norm': 0.8713433146476746, 'learning_rate': 0.0002008888888888889, 'epoch': 1.7955555555555556}
Step 2030: {'loss': 1.1068, 'grad_norm': 0.5616909861564636, 'learning_rate': 0.0001994074074074074, 'epoch': 1.8044444444444445}
Step 2040: {'loss': 1.0756, 'grad_norm': 0.5746089220046997, 'learning_rate': 0.00019792592592592594, 'epoch': 1.8133333333333335}
Step 2050: {'loss': 1.1072, 'grad_norm': 0.6109442710876465, 'learning_rate': 0.00019644444444444445, 'epoch': 1.8222222222222222}
Step 2060: {'loss': 1.053, 'grad_norm': 0.5731450915336609, 'learning_rate': 0.00019496296296296297, 'epoch': 1.8311111111111111}
Step 2070: {'loss': 1.0821, 'grad_norm': 0.4407055079936981, 'learning_rate': 0.0001934814814814815, 'epoch': 1.8399999999999999}
Step 2080: {'loss': 1.0755, 'grad_norm': 0.5106014609336853, 'learning_rate': 0.000192, 'epoch': 1.8488888888888888}
Step 2090: {'loss': 1.0424, 'grad_norm': 0.47617673873901367, 'learning_rate': 0.00019051851851851855, 'epoch': 1.8577777777777778}
Step 2100: {'loss': 1.0015, 'grad_norm': 0.6389382481575012, 'learning_rate': 0.00018903703703703704, 'epoch': 1.8666666666666667}
Step 2110: {'loss': 1.0433, 'grad_norm': 0.489827424287796, 'learning_rate': 0.00018755555555555555, 'epoch': 1.8755555555555556}
Step 2120: {'loss': 1.0933, 'grad_norm': 0.6705574989318848, 'learning_rate': 0.0001860740740740741, 'epoch': 1.8844444444444446}
Step 2130: {'loss': 1.0724, 'grad_norm': 0.596930205821991, 'learning_rate': 0.0001845925925925926, 'epoch': 1.8933333333333333}
Step 2140: {'loss': 1.1109, 'grad_norm': 0.5638452172279358, 'learning_rate': 0.0001831111111111111, 'epoch': 1.9022222222222223}
Step 2150: {'loss': 1.0796, 'grad_norm': 0.7453539967536926, 'learning_rate': 0.00018162962962962965, 'epoch': 1.911111111111111}
Step 2160: {'loss': 1.1464, 'grad_norm': 0.8145200610160828, 'learning_rate': 0.00018014814814814814, 'epoch': 1.92}
Step 2170: {'loss': 1.2015, 'grad_norm': 2.7914652824401855, 'learning_rate': 0.00017866666666666668, 'epoch': 1.9288888888888889}
Step 2180: {'loss': 1.0795, 'grad_norm': 0.5456627011299133, 'learning_rate': 0.0001771851851851852, 'epoch': 1.9377777777777778}
Step 2190: {'loss': 1.0901, 'grad_norm': 0.4805034399032593, 'learning_rate': 0.0001757037037037037, 'epoch': 1.9466666666666668}
Step 2200: {'loss': 0.9715, 'grad_norm': 0.5649537444114685, 'learning_rate': 0.00017422222222222223, 'epoch': 1.9555555555555557}
Step 2210: {'loss': 1.1469, 'grad_norm': 0.7601501941680908, 'learning_rate': 0.00017274074074074075, 'epoch': 1.9644444444444444}
Step 2220: {'loss': 1.1976, 'grad_norm': 0.5776546001434326, 'learning_rate': 0.00017125925925925926, 'epoch': 1.9733333333333334}
Step 2230: {'loss': 1.1702, 'grad_norm': 0.5453412532806396, 'learning_rate': 0.00016977777777777778, 'epoch': 1.982222222222222}
Step 2240: {'loss': 1.0716, 'grad_norm': 0.5589375495910645, 'learning_rate': 0.0001682962962962963, 'epoch': 1.991111111111111}
Step 2250: {'loss': 1.0525, 'grad_norm': 3.624436616897583, 'learning_rate': 0.00016681481481481481, 'epoch': 2.0}
Epoch 2.0 completed in 13730.70s
Step 2250: {'eval_loss': 1.0739431381225586, 'eval_runtime': 198.9884, 'eval_samples_per_second': 2.513, 'eval_steps_per_second': 0.628, 'epoch': 2.0}
Step 2260: {'loss': 1.111, 'grad_norm': 1.0874533653259277, 'learning_rate': 0.00016533333333333333, 'epoch': 2.008888888888889}
Step 2270: {'loss': 1.1301, 'grad_norm': 0.5227503180503845, 'learning_rate': 0.00016385185185185188, 'epoch': 2.017777777777778}
Step 2280: {'loss': 1.1522, 'grad_norm': 0.4779472053050995, 'learning_rate': 0.00016237037037037036, 'epoch': 2.026666666666667}
Step 2290: {'loss': 1.1258, 'grad_norm': 0.5312354564666748, 'learning_rate': 0.00016088888888888888, 'epoch': 2.0355555555555553}
Step 2300: {'loss': 0.9967, 'grad_norm': 0.5639252066612244, 'learning_rate': 0.00015940740740740743, 'epoch': 2.0444444444444443}
Step 2310: {'loss': 1.1308, 'grad_norm': 0.5372593402862549, 'learning_rate': 0.00015792592592592591, 'epoch': 2.0533333333333332}
Step 2320: {'loss': 1.0658, 'grad_norm': 0.5447067618370056, 'learning_rate': 0.00015644444444444443, 'epoch': 2.062222222222222}
Step 2330: {'loss': 1.086, 'grad_norm': 0.5782361030578613, 'learning_rate': 0.00015496296296296298, 'epoch': 2.071111111111111}
Step 2340: {'loss': 1.2032, 'grad_norm': 0.5970084071159363, 'learning_rate': 0.00015348148148148146, 'epoch': 2.08}
Step 2350: {'loss': 1.2302, 'grad_norm': 0.5733649730682373, 'learning_rate': 0.000152, 'epoch': 2.088888888888889}
Step 2360: {'loss': 1.0564, 'grad_norm': 0.5030921697616577, 'learning_rate': 0.00015051851851851853, 'epoch': 2.097777777777778}
Step 2370: {'loss': 1.025, 'grad_norm': 0.5848191976547241, 'learning_rate': 0.00014903703703703701, 'epoch': 2.1066666666666665}
Step 2380: {'loss': 1.06, 'grad_norm': 0.7461257576942444, 'learning_rate': 0.00014755555555555556, 'epoch': 2.1155555555555554}
Step 2390: {'loss': 1.024, 'grad_norm': 0.5483561754226685, 'learning_rate': 0.00014607407407407407, 'epoch': 2.1244444444444444}
Step 2400: {'loss': 1.1224, 'grad_norm': 0.479114294052124, 'learning_rate': 0.00014459259259259262, 'epoch': 2.1333333333333333}
Step 2410: {'loss': 1.0517, 'grad_norm': 0.4298001527786255, 'learning_rate': 0.0001431111111111111, 'epoch': 2.1422222222222222}
Step 2420: {'loss': 1.1473, 'grad_norm': 0.4977642893791199, 'learning_rate': 0.00014162962962962962, 'epoch': 2.151111111111111}
Step 2430: {'loss': 1.0945, 'grad_norm': 0.47096848487854004, 'learning_rate': 0.00014014814814814817, 'epoch': 2.16}
Step 2440: {'loss': 1.0181, 'grad_norm': 0.5801662802696228, 'learning_rate': 0.00013866666666666666, 'epoch': 2.168888888888889}
Step 2450: {'loss': 1.1535, 'grad_norm': 0.5660377144813538, 'learning_rate': 0.0001371851851851852, 'epoch': 2.1777777777777776}
Step 2460: {'loss': 1.0495, 'grad_norm': 0.6868160963058472, 'learning_rate': 0.00013570370370370372, 'epoch': 2.1866666666666665}
Step 2470: {'loss': 1.054, 'grad_norm': 0.6749099493026733, 'learning_rate': 0.0001342222222222222, 'epoch': 2.1955555555555555}
Step 2480: {'loss': 1.0497, 'grad_norm': 0.5380442142486572, 'learning_rate': 0.00013274074074074075, 'epoch': 2.2044444444444444}
Step 2490: {'loss': 1.087, 'grad_norm': 0.6091986894607544, 'learning_rate': 0.00013125925925925927, 'epoch': 2.2133333333333334}
Step 2500: {'loss': 1.1026, 'grad_norm': 0.5386764407157898, 'learning_rate': 0.00012977777777777779, 'epoch': 2.2222222222222223}
Step 2510: {'loss': 1.0754, 'grad_norm': 0.6106998324394226, 'learning_rate': 0.0001282962962962963, 'epoch': 2.2311111111111113}
Step 2520: {'loss': 1.2028, 'grad_norm': 0.6022965312004089, 'learning_rate': 0.00012681481481481482, 'epoch': 2.24}
Step 2530: {'loss': 1.0879, 'grad_norm': 0.5006139874458313, 'learning_rate': 0.00012533333333333334, 'epoch': 2.2488888888888887}
Step 2540: {'loss': 1.1002, 'grad_norm': 0.5502880215644836, 'learning_rate': 0.00012385185185185185, 'epoch': 2.2577777777777777}
Step 2550: {'loss': 1.1562, 'grad_norm': 0.5531945824623108, 'learning_rate': 0.00012237037037037037, 'epoch': 2.2666666666666666}
Step 2560: {'loss': 1.0499, 'grad_norm': 0.5074765086174011, 'learning_rate': 0.0001208888888888889, 'epoch': 2.2755555555555556}
Step 2570: {'loss': 1.018, 'grad_norm': 0.5578118562698364, 'learning_rate': 0.00011940740740740742, 'epoch': 2.2844444444444445}
Step 2580: {'loss': 1.0742, 'grad_norm': 0.6729099154472351, 'learning_rate': 0.00011792592592592592, 'epoch': 2.2933333333333334}
Step 2590: {'loss': 1.0272, 'grad_norm': 0.6198543906211853, 'learning_rate': 0.00011644444444444445, 'epoch': 2.3022222222222224}
Step 2600: {'loss': 1.0913, 'grad_norm': 0.5708343982696533, 'learning_rate': 0.00011496296296296297, 'epoch': 2.311111111111111}
Step 2610: {'loss': 1.073, 'grad_norm': 0.6519021987915039, 'learning_rate': 0.00011348148148148148, 'epoch': 2.32}
Step 2620: {'loss': 1.0767, 'grad_norm': 0.5277993083000183, 'learning_rate': 0.000112, 'epoch': 2.328888888888889}
Step 2630: {'loss': 1.143, 'grad_norm': 0.6701092720031738, 'learning_rate': 0.00011051851851851851, 'epoch': 2.3377777777777777}
Step 2640: {'loss': 1.033, 'grad_norm': 0.5141459703445435, 'learning_rate': 0.00010903703703703705, 'epoch': 2.3466666666666667}
Step 2650: {'loss': 1.0316, 'grad_norm': 0.5868087410926819, 'learning_rate': 0.00010755555555555556, 'epoch': 2.3555555555555556}
Step 2660: {'loss': 1.0309, 'grad_norm': 0.5288646221160889, 'learning_rate': 0.00010607407407407408, 'epoch': 2.3644444444444446}
Step 2670: {'loss': 1.148, 'grad_norm': 0.791998565196991, 'learning_rate': 0.0001045925925925926, 'epoch': 2.3733333333333335}
Step 2680: {'loss': 1.0681, 'grad_norm': 0.7849249839782715, 'learning_rate': 0.00010311111111111111, 'epoch': 2.3822222222222225}
Step 2690: {'loss': 1.0733, 'grad_norm': 0.5656982064247131, 'learning_rate': 0.00010162962962962963, 'epoch': 2.391111111111111}
Step 2700: {'loss': 1.1246, 'grad_norm': 0.9124969840049744, 'learning_rate': 0.00010014814814814816, 'epoch': 2.4}
Step 2710: {'loss': 0.9886, 'grad_norm': 0.4756508469581604, 'learning_rate': 9.866666666666668e-05, 'epoch': 2.408888888888889}
Step 2720: {'loss': 0.9833, 'grad_norm': 0.461837500333786, 'learning_rate': 9.718518518518518e-05, 'epoch': 2.417777777777778}
Step 2730: {'loss': 1.1153, 'grad_norm': 0.7498663663864136, 'learning_rate': 9.570370370370371e-05, 'epoch': 2.4266666666666667}
Step 2740: {'loss': 1.1085, 'grad_norm': 0.6835487484931946, 'learning_rate': 9.422222222222223e-05, 'epoch': 2.4355555555555557}
Step 2750: {'loss': 1.0182, 'grad_norm': 0.5529657006263733, 'learning_rate': 9.274074074074074e-05, 'epoch': 2.4444444444444446}
Step 2760: {'loss': 1.0899, 'grad_norm': 0.5326440334320068, 'learning_rate': 9.125925925925926e-05, 'epoch': 2.453333333333333}
Step 2770: {'loss': 1.0652, 'grad_norm': 0.46165573596954346, 'learning_rate': 8.977777777777778e-05, 'epoch': 2.462222222222222}
Step 2780: {'loss': 1.1188, 'grad_norm': 0.48573416471481323, 'learning_rate': 8.829629629629629e-05, 'epoch': 2.471111111111111}
Step 2790: {'loss': 1.1112, 'grad_norm': 0.7108746767044067, 'learning_rate': 8.681481481481482e-05, 'epoch': 2.48}
Step 2800: {'loss': 1.0276, 'grad_norm': 0.5713552832603455, 'learning_rate': 8.533333333333334e-05, 'epoch': 2.488888888888889}
Step 2810: {'loss': 1.0716, 'grad_norm': 0.6420604586601257, 'learning_rate': 8.385185185185186e-05, 'epoch': 2.497777777777778}
Step 2820: {'loss': 1.119, 'grad_norm': 0.6489543914794922, 'learning_rate': 8.237037037037037e-05, 'epoch': 2.506666666666667}
Step 2830: {'loss': 1.0249, 'grad_norm': 0.596340000629425, 'learning_rate': 8.088888888888889e-05, 'epoch': 2.5155555555555553}
Step 2840: {'loss': 1.0942, 'grad_norm': 0.6151395440101624, 'learning_rate': 7.940740740740742e-05, 'epoch': 2.5244444444444447}
Step 2850: {'loss': 1.0366, 'grad_norm': 0.5497310757637024, 'learning_rate': 7.792592592592592e-05, 'epoch': 2.533333333333333}
Step 2860: {'loss': 1.1765, 'grad_norm': 0.7693465948104858, 'learning_rate': 7.644444444444444e-05, 'epoch': 2.542222222222222}
Step 2870: {'loss': 1.0551, 'grad_norm': 0.5490357875823975, 'learning_rate': 7.496296296296297e-05, 'epoch': 2.551111111111111}
Step 2880: {'loss': 1.0704, 'grad_norm': 0.5086066722869873, 'learning_rate': 7.348148148148149e-05, 'epoch': 2.56}
Step 2890: {'loss': 1.0969, 'grad_norm': 0.520967423915863, 'learning_rate': 7.2e-05, 'epoch': 2.568888888888889}
Step 2900: {'loss': 1.1103, 'grad_norm': 0.641610860824585, 'learning_rate': 7.051851851851852e-05, 'epoch': 2.5777777777777775}
Step 2910: {'loss': 1.0866, 'grad_norm': 0.7282314300537109, 'learning_rate': 6.903703703703704e-05, 'epoch': 2.586666666666667}
Step 2920: {'loss': 1.1202, 'grad_norm': 0.5099971890449524, 'learning_rate': 6.755555555555555e-05, 'epoch': 2.5955555555555554}
Step 2930: {'loss': 1.0327, 'grad_norm': 0.7217718362808228, 'learning_rate': 6.607407407407408e-05, 'epoch': 2.6044444444444443}
Step 2940: {'loss': 1.0336, 'grad_norm': 0.575038731098175, 'learning_rate': 6.459259259259259e-05, 'epoch': 2.6133333333333333}
Step 2950: {'loss': 1.0202, 'grad_norm': 0.6489378809928894, 'learning_rate': 6.311111111111112e-05, 'epoch': 2.6222222222222222}
Step 2960: {'loss': 1.0733, 'grad_norm': 0.48338964581489563, 'learning_rate': 6.162962962962963e-05, 'epoch': 2.631111111111111}
Step 2970: {'loss': 1.1001, 'grad_norm': 0.5720562934875488, 'learning_rate': 6.014814814814815e-05, 'epoch': 2.64}
Step 2980: {'loss': 1.0754, 'grad_norm': 0.46791934967041016, 'learning_rate': 5.8666666666666665e-05, 'epoch': 2.648888888888889}
Step 2990: {'loss': 1.091, 'grad_norm': 0.6001008152961731, 'learning_rate': 5.718518518518519e-05, 'epoch': 2.6577777777777776}
Step 3000: {'loss': 1.0419, 'grad_norm': 0.41639429330825806, 'learning_rate': 5.5703703703703705e-05, 'epoch': 2.6666666666666665}
Step 3010: {'loss': 1.0795, 'grad_norm': 0.6863303780555725, 'learning_rate': 5.422222222222222e-05, 'epoch': 2.6755555555555555}
Step 3020: {'loss': 0.9643, 'grad_norm': 0.6336906552314758, 'learning_rate': 5.2740740740740745e-05, 'epoch': 2.6844444444444444}
Step 3030: {'loss': 1.0818, 'grad_norm': 0.5301229953765869, 'learning_rate': 5.125925925925926e-05, 'epoch': 2.6933333333333334}
Step 3040: {'loss': 0.9746, 'grad_norm': 0.5033674836158752, 'learning_rate': 4.977777777777778e-05, 'epoch': 2.7022222222222223}
Step 3050: {'loss': 1.0787, 'grad_norm': 0.5363913178443909, 'learning_rate': 4.8296296296296295e-05, 'epoch': 2.7111111111111112}
Step 3060: {'loss': 1.0917, 'grad_norm': 0.5247501730918884, 'learning_rate': 4.681481481481482e-05, 'epoch': 2.7199999999999998}
Step 3070: {'loss': 1.045, 'grad_norm': 0.48699814081192017, 'learning_rate': 4.5333333333333335e-05, 'epoch': 2.728888888888889}
Step 3080: {'loss': 1.1063, 'grad_norm': 0.509915828704834, 'learning_rate': 4.385185185185185e-05, 'epoch': 2.7377777777777776}
Step 3090: {'loss': 0.9201, 'grad_norm': 0.5060719847679138, 'learning_rate': 4.237037037037037e-05, 'epoch': 2.7466666666666666}
Step 3100: {'loss': 1.0631, 'grad_norm': 0.6420701146125793, 'learning_rate': 4.088888888888889e-05, 'epoch': 2.7555555555555555}
Step 3110: {'loss': 1.0317, 'grad_norm': 0.8179448843002319, 'learning_rate': 3.940740740740741e-05, 'epoch': 2.7644444444444445}
Step 3120: {'loss': 0.9887, 'grad_norm': 0.6256828308105469, 'learning_rate': 3.7925925925925925e-05, 'epoch': 2.7733333333333334}
Step 3130: {'loss': 1.0429, 'grad_norm': 0.6071729063987732, 'learning_rate': 3.644444444444445e-05, 'epoch': 2.7822222222222224}
Step 3140: {'loss': 1.0751, 'grad_norm': 0.47799521684646606, 'learning_rate': 3.496296296296296e-05, 'epoch': 2.7911111111111113}
Step 3150: {'loss': 1.0436, 'grad_norm': 0.5529554486274719, 'learning_rate': 3.348148148148148e-05, 'epoch': 2.8}
Step 3160: {'loss': 1.0768, 'grad_norm': 0.48214197158813477, 'learning_rate': 3.2e-05, 'epoch': 2.8088888888888888}
Step 3170: {'loss': 1.2252, 'grad_norm': 0.5851154327392578, 'learning_rate': 3.0518518518518515e-05, 'epoch': 2.8177777777777777}
Step 3180: {'loss': 1.0354, 'grad_norm': 0.5960209965705872, 'learning_rate': 2.903703703703704e-05, 'epoch': 2.8266666666666667}
Step 3190: {'loss': 1.1012, 'grad_norm': 0.686458170413971, 'learning_rate': 2.7555555555555555e-05, 'epoch': 2.8355555555555556}
Step 3200: {'loss': 1.0362, 'grad_norm': 0.5040188431739807, 'learning_rate': 2.6074074074074072e-05, 'epoch': 2.8444444444444446}
Step 3210: {'loss': 1.0865, 'grad_norm': 0.4658525586128235, 'learning_rate': 2.4592592592592592e-05, 'epoch': 2.8533333333333335}
Step 3220: {'loss': 1.0403, 'grad_norm': 0.5894617438316345, 'learning_rate': 2.3111111111111112e-05, 'epoch': 2.862222222222222}
Step 3230: {'loss': 1.0226, 'grad_norm': 0.63136887550354, 'learning_rate': 2.1629629629629632e-05, 'epoch': 2.871111111111111}
Step 3240: {'loss': 1.0502, 'grad_norm': 0.6881959438323975, 'learning_rate': 2.014814814814815e-05, 'epoch': 2.88}
Step 3250: {'loss': 1.1294, 'grad_norm': 0.5593659281730652, 'learning_rate': 1.866666666666667e-05, 'epoch': 2.888888888888889}
Step 3260: {'loss': 1.0332, 'grad_norm': 0.5282066464424133, 'learning_rate': 1.7185185185185185e-05, 'epoch': 2.897777777777778}
Step 3270: {'loss': 0.9851, 'grad_norm': 0.5221757292747498, 'learning_rate': 1.5703703703703702e-05, 'epoch': 2.9066666666666667}
Step 3280: {'loss': 1.0128, 'grad_norm': 0.5853177309036255, 'learning_rate': 1.4222222222222224e-05, 'epoch': 2.9155555555555557}
Step 3290: {'loss': 1.0379, 'grad_norm': 0.6012669205665588, 'learning_rate': 1.274074074074074e-05, 'epoch': 2.924444444444444}
Step 3300: {'loss': 1.0388, 'grad_norm': 0.47396987676620483, 'learning_rate': 1.1259259259259259e-05, 'epoch': 2.9333333333333336}
Step 3310: {'loss': 1.0001, 'grad_norm': 0.5273944139480591, 'learning_rate': 9.777777777777777e-06, 'epoch': 2.942222222222222}
Step 3320: {'loss': 0.941, 'grad_norm': 0.5632011294364929, 'learning_rate': 8.296296296296297e-06, 'epoch': 2.951111111111111}
Step 3330: {'loss': 1.0588, 'grad_norm': 0.5235655903816223, 'learning_rate': 6.814814814814815e-06, 'epoch': 2.96}
Step 3340: {'loss': 1.0076, 'grad_norm': 0.5255048274993896, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.968888888888889}
Step 3350: {'loss': 1.1667, 'grad_norm': 0.5864949226379395, 'learning_rate': 3.851851851851852e-06, 'epoch': 2.977777777777778}
Step 3360: {'loss': 1.1959, 'grad_norm': 0.7806713581085205, 'learning_rate': 2.3703703703703703e-06, 'epoch': 2.986666666666667}
Step 3370: {'loss': 1.1615, 'grad_norm': 0.6726385951042175, 'learning_rate': 8.88888888888889e-07, 'epoch': 2.9955555555555557}
Epoch 3.0 completed in 20727.49s
Step 3375: {'eval_loss': 1.065100908279419, 'eval_runtime': 198.7957, 'eval_samples_per_second': 2.515, 'eval_steps_per_second': 0.629, 'epoch': 3.0}
Step 3375: {'train_runtime': 20926.6301, 'train_samples_per_second': 0.645, 'train_steps_per_second': 0.161, 'total_flos': 6313575186432000.0, 'train_loss': 1.1600980950814705, 'epoch': 3.0}
=== Training completed in 20926.62s ===


=== Training started at 2025-12-11 13:51:58 ===
Step 10: {'loss': 4.411, 'grad_norm': 0.5641936659812927, 'learning_rate': 2.647058823529412e-05, 'epoch': 0.008888888888888889}
Step 20: {'loss': 4.4817, 'grad_norm': 0.6925691366195679, 'learning_rate': 5.588235294117647e-05, 'epoch': 0.017777777777777778}
Step 30: {'loss': 4.3253, 'grad_norm': 1.0211900472640991, 'learning_rate': 8.529411764705883e-05, 'epoch': 0.02666666666666667}
Step 40: {'loss': 4.0459, 'grad_norm': 1.3932708501815796, 'learning_rate': 0.00011470588235294118, 'epoch': 0.035555555555555556}
Step 50: {'loss': 3.6968, 'grad_norm': 1.477966547012329, 'learning_rate': 0.00014411764705882354, 'epoch': 0.044444444444444446}
Step 60: {'loss': 3.2734, 'grad_norm': 1.4650369882583618, 'learning_rate': 0.0001735294117647059, 'epoch': 0.05333333333333334}
Step 70: {'loss': 2.8117, 'grad_norm': 1.8071448802947998, 'learning_rate': 0.0001999083409715857, 'epoch': 0.06222222222222222}
Step 80: {'loss': 2.6359, 'grad_norm': 1.9746365547180176, 'learning_rate': 0.00019899175068744272, 'epoch': 0.07111111111111111}
Step 90: {'loss': 2.2925, 'grad_norm': 1.8710589408874512, 'learning_rate': 0.00019807516040329975, 'epoch': 0.08}
Step 100: {'loss': 2.1277, 'grad_norm': 2.566277027130127, 'learning_rate': 0.00019715857011915676, 'epoch': 0.08888888888888889}
Step 110: {'loss': 2.0242, 'grad_norm': 1.7767398357391357, 'learning_rate': 0.00019624197983501376, 'epoch': 0.09777777777777778}
Step 120: {'loss': 1.9613, 'grad_norm': 1.7427053451538086, 'learning_rate': 0.00019532538955087077, 'epoch': 0.10666666666666667}
Step 130: {'loss': 1.7195, 'grad_norm': 1.8192811012268066, 'learning_rate': 0.00019440879926672778, 'epoch': 0.11555555555555555}
Step 140: {'loss': 1.7781, 'grad_norm': 1.7434602975845337, 'learning_rate': 0.0001934922089825848, 'epoch': 0.12444444444444444}
Step 150: {'loss': 1.6614, 'grad_norm': 1.9713822603225708, 'learning_rate': 0.0001925756186984418, 'epoch': 0.13333333333333333}
Step 160: {'loss': 1.6753, 'grad_norm': 1.6564151048660278, 'learning_rate': 0.0001916590284142988, 'epoch': 0.14222222222222222}
Step 170: {'loss': 1.7374, 'grad_norm': 2.321345567703247, 'learning_rate': 0.0001907424381301558, 'epoch': 0.1511111111111111}
Step 180: {'loss': 1.5554, 'grad_norm': 2.0364749431610107, 'learning_rate': 0.00018982584784601284, 'epoch': 0.16}
Step 190: {'loss': 1.5591, 'grad_norm': 1.9948993921279907, 'learning_rate': 0.00018890925756186985, 'epoch': 0.1688888888888889}
Step 200: {'loss': 1.5538, 'grad_norm': 2.316148519515991, 'learning_rate': 0.00018799266727772686, 'epoch': 0.17777777777777778}
Step 210: {'loss': 1.5074, 'grad_norm': 1.7919988632202148, 'learning_rate': 0.00018707607699358387, 'epoch': 0.18666666666666668}
Step 220: {'loss': 1.4284, 'grad_norm': 1.8705271482467651, 'learning_rate': 0.00018615948670944088, 'epoch': 0.19555555555555557}
Step 230: {'loss': 1.4301, 'grad_norm': 2.3618907928466797, 'learning_rate': 0.0001852428964252979, 'epoch': 0.20444444444444446}
Step 240: {'loss': 1.3245, 'grad_norm': 3.3849315643310547, 'learning_rate': 0.00018432630614115492, 'epoch': 0.21333333333333335}
Step 250: {'loss': 1.3996, 'grad_norm': 1.6957075595855713, 'learning_rate': 0.00018340971585701192, 'epoch': 0.2222222222222222}
Step 260: {'loss': 1.4637, 'grad_norm': 1.5199706554412842, 'learning_rate': 0.00018249312557286893, 'epoch': 0.2311111111111111}
Step 270: {'loss': 1.3383, 'grad_norm': 1.9594483375549316, 'learning_rate': 0.00018157653528872597, 'epoch': 0.24}
Step 280: {'loss': 1.3849, 'grad_norm': 2.2574431896209717, 'learning_rate': 0.00018065994500458297, 'epoch': 0.24888888888888888}
Step 290: {'loss': 1.4093, 'grad_norm': 2.47491455078125, 'learning_rate': 0.00017974335472043998, 'epoch': 0.2577777777777778}
Step 300: {'loss': 1.4329, 'grad_norm': 1.8305530548095703, 'learning_rate': 0.000178826764436297, 'epoch': 0.26666666666666666}
Step 310: {'loss': 1.3817, 'grad_norm': 1.421674132347107, 'learning_rate': 0.000177910174152154, 'epoch': 0.27555555555555555}
Step 320: {'loss': 1.3514, 'grad_norm': 1.762363076210022, 'learning_rate': 0.000176993583868011, 'epoch': 0.28444444444444444}
Step 330: {'loss': 1.3646, 'grad_norm': 1.554275631904602, 'learning_rate': 0.000176076993583868, 'epoch': 0.29333333333333333}
Step 340: {'loss': 1.2844, 'grad_norm': 1.6327555179595947, 'learning_rate': 0.00017516040329972502, 'epoch': 0.3022222222222222}
Step 350: {'loss': 1.3107, 'grad_norm': 1.828285574913025, 'learning_rate': 0.00017424381301558203, 'epoch': 0.3111111111111111}
Step 360: {'loss': 1.3088, 'grad_norm': 1.428769826889038, 'learning_rate': 0.00017332722273143906, 'epoch': 0.32}
Step 370: {'loss': 1.3133, 'grad_norm': 1.4542614221572876, 'learning_rate': 0.00017241063244729607, 'epoch': 0.3288888888888889}
Step 380: {'loss': 1.3066, 'grad_norm': 1.2748510837554932, 'learning_rate': 0.00017149404216315308, 'epoch': 0.3377777777777778}
Step 390: {'loss': 1.2902, 'grad_norm': 1.808764100074768, 'learning_rate': 0.00017057745187901008, 'epoch': 0.3466666666666667}
Step 400: {'loss': 1.2687, 'grad_norm': 1.2736843824386597, 'learning_rate': 0.0001696608615948671, 'epoch': 0.35555555555555557}
Step 410: {'loss': 1.3146, 'grad_norm': 1.123687744140625, 'learning_rate': 0.00016874427131072413, 'epoch': 0.36444444444444446}
Step 420: {'loss': 1.3633, 'grad_norm': 1.2003679275512695, 'learning_rate': 0.00016782768102658113, 'epoch': 0.37333333333333335}
Step 430: {'loss': 1.3643, 'grad_norm': 1.456824779510498, 'learning_rate': 0.00016691109074243814, 'epoch': 0.38222222222222224}
Step 440: {'loss': 1.3574, 'grad_norm': 1.434869647026062, 'learning_rate': 0.00016599450045829515, 'epoch': 0.39111111111111113}
Step 450: {'loss': 1.287, 'grad_norm': 1.4318541288375854, 'learning_rate': 0.00016507791017415218, 'epoch': 0.4}
Step 460: {'loss': 1.169, 'grad_norm': 2.678187131881714, 'learning_rate': 0.0001641613198900092, 'epoch': 0.4088888888888889}
Step 470: {'loss': 1.2429, 'grad_norm': 1.2142229080200195, 'learning_rate': 0.0001632447296058662, 'epoch': 0.4177777777777778}
Step 480: {'loss': 1.2236, 'grad_norm': 1.0943819284439087, 'learning_rate': 0.0001623281393217232, 'epoch': 0.4266666666666667}
Step 490: {'loss': 1.2683, 'grad_norm': 1.1779847145080566, 'learning_rate': 0.0001614115490375802, 'epoch': 0.43555555555555553}
Step 500: {'loss': 1.2123, 'grad_norm': 1.2835993766784668, 'learning_rate': 0.00016049495875343722, 'epoch': 0.4444444444444444}
Step 510: {'loss': 1.2356, 'grad_norm': 1.4174814224243164, 'learning_rate': 0.00015957836846929423, 'epoch': 0.4533333333333333}
Step 520: {'loss': 1.3573, 'grad_norm': 0.9979785084724426, 'learning_rate': 0.00015866177818515124, 'epoch': 0.4622222222222222}
Step 530: {'loss': 1.2145, 'grad_norm': 1.2940034866333008, 'learning_rate': 0.00015774518790100824, 'epoch': 0.4711111111111111}
Step 540: {'loss': 1.2507, 'grad_norm': 1.3864341974258423, 'learning_rate': 0.00015682859761686525, 'epoch': 0.48}
Step 550: {'loss': 1.1611, 'grad_norm': 0.9634988903999329, 'learning_rate': 0.0001559120073327223, 'epoch': 0.4888888888888889}
Step 560: {'loss': 1.1846, 'grad_norm': 1.0735721588134766, 'learning_rate': 0.0001549954170485793, 'epoch': 0.49777777777777776}
Step 570: {'loss': 1.2076, 'grad_norm': 1.6486791372299194, 'learning_rate': 0.0001540788267644363, 'epoch': 0.5066666666666667}
Step 580: {'loss': 1.2281, 'grad_norm': 1.1733356714248657, 'learning_rate': 0.0001531622364802933, 'epoch': 0.5155555555555555}
Step 590: {'loss': 1.2213, 'grad_norm': 0.9426640272140503, 'learning_rate': 0.00015224564619615034, 'epoch': 0.5244444444444445}
Step 600: {'loss': 1.2744, 'grad_norm': 1.1893694400787354, 'learning_rate': 0.00015132905591200735, 'epoch': 0.5333333333333333}
Step 610: {'loss': 1.2483, 'grad_norm': 0.9519032835960388, 'learning_rate': 0.00015041246562786436, 'epoch': 0.5422222222222223}
Step 620: {'loss': 1.2477, 'grad_norm': 0.9027923345565796, 'learning_rate': 0.00014949587534372137, 'epoch': 0.5511111111111111}
Step 630: {'loss': 1.1737, 'grad_norm': 1.129180669784546, 'learning_rate': 0.00014857928505957837, 'epoch': 0.56}
Step 640: {'loss': 1.2244, 'grad_norm': 1.0762901306152344, 'learning_rate': 0.00014766269477543538, 'epoch': 0.5688888888888889}
Step 650: {'loss': 1.2017, 'grad_norm': 0.9017342925071716, 'learning_rate': 0.0001467461044912924, 'epoch': 0.5777777777777777}
Step 660: {'loss': 1.2456, 'grad_norm': 0.8573243618011475, 'learning_rate': 0.0001458295142071494, 'epoch': 0.5866666666666667}
Step 670: {'loss': 1.3223, 'grad_norm': 0.9050369262695312, 'learning_rate': 0.0001449129239230064, 'epoch': 0.5955555555555555}
Step 680: {'loss': 1.1855, 'grad_norm': 0.9587716460227966, 'learning_rate': 0.00014399633363886344, 'epoch': 0.6044444444444445}
Step 690: {'loss': 1.107, 'grad_norm': 1.2675037384033203, 'learning_rate': 0.00014307974335472045, 'epoch': 0.6133333333333333}
Step 700: {'loss': 1.1208, 'grad_norm': 1.44451105594635, 'learning_rate': 0.00014216315307057745, 'epoch': 0.6222222222222222}
Step 710: {'loss': 1.2112, 'grad_norm': 1.5204905271530151, 'learning_rate': 0.00014124656278643446, 'epoch': 0.6311111111111111}
Step 720: {'loss': 1.2175, 'grad_norm': 0.8833625912666321, 'learning_rate': 0.00014032997250229147, 'epoch': 0.64}
Step 730: {'loss': 1.2698, 'grad_norm': 1.0384379625320435, 'learning_rate': 0.0001394133822181485, 'epoch': 0.6488888888888888}
Step 740: {'loss': 1.2783, 'grad_norm': 0.8492764830589294, 'learning_rate': 0.0001384967919340055, 'epoch': 0.6577777777777778}
Step 750: {'loss': 1.1246, 'grad_norm': 0.9141006469726562, 'learning_rate': 0.00013758020164986252, 'epoch': 0.6666666666666666}
Step 760: {'loss': 1.1151, 'grad_norm': 1.4830918312072754, 'learning_rate': 0.00013666361136571953, 'epoch': 0.6755555555555556}
Step 770: {'loss': 1.2948, 'grad_norm': 1.145574927330017, 'learning_rate': 0.00013574702108157653, 'epoch': 0.6844444444444444}
Step 780: {'loss': 1.2043, 'grad_norm': 0.8440316319465637, 'learning_rate': 0.00013483043079743357, 'epoch': 0.6933333333333334}
Step 790: {'loss': 1.275, 'grad_norm': 0.9130533933639526, 'learning_rate': 0.00013391384051329058, 'epoch': 0.7022222222222222}
Step 800: {'loss': 1.1417, 'grad_norm': 1.15695059299469, 'learning_rate': 0.00013299725022914758, 'epoch': 0.7111111111111111}
Step 810: {'loss': 1.164, 'grad_norm': 0.8653585314750671, 'learning_rate': 0.0001320806599450046, 'epoch': 0.72}
Step 820: {'loss': 1.1718, 'grad_norm': 1.3161582946777344, 'learning_rate': 0.0001311640696608616, 'epoch': 0.7288888888888889}
Step 830: {'loss': 1.2473, 'grad_norm': 1.296179175376892, 'learning_rate': 0.0001302474793767186, 'epoch': 0.7377777777777778}
Step 840: {'loss': 1.3358, 'grad_norm': 1.0252166986465454, 'learning_rate': 0.00012933088909257561, 'epoch': 0.7466666666666667}
Step 850: {'loss': 1.2219, 'grad_norm': 1.080420970916748, 'learning_rate': 0.00012841429880843262, 'epoch': 0.7555555555555555}
Step 860: {'loss': 1.2135, 'grad_norm': 1.0543022155761719, 'learning_rate': 0.00012749770852428963, 'epoch': 0.7644444444444445}
Step 870: {'loss': 1.2227, 'grad_norm': 0.908138632774353, 'learning_rate': 0.00012658111824014666, 'epoch': 0.7733333333333333}
Step 880: {'loss': 1.2742, 'grad_norm': 1.011720061302185, 'learning_rate': 0.00012566452795600367, 'epoch': 0.7822222222222223}
Step 890: {'loss': 1.2125, 'grad_norm': 0.7042827606201172, 'learning_rate': 0.00012474793767186068, 'epoch': 0.7911111111111111}
Step 900: {'loss': 1.159, 'grad_norm': 0.9067556262016296, 'learning_rate': 0.0001238313473877177, 'epoch': 0.8}
Step 910: {'loss': 1.1459, 'grad_norm': 0.77986741065979, 'learning_rate': 0.00012291475710357472, 'epoch': 0.8088888888888889}
Step 920: {'loss': 1.2685, 'grad_norm': 0.7537974715232849, 'learning_rate': 0.00012199816681943173, 'epoch': 0.8177777777777778}
Step 930: {'loss': 1.2064, 'grad_norm': 0.8483250737190247, 'learning_rate': 0.00012108157653528874, 'epoch': 0.8266666666666667}
Step 940: {'loss': 1.1341, 'grad_norm': 0.8708512187004089, 'learning_rate': 0.00012016498625114574, 'epoch': 0.8355555555555556}
Step 950: {'loss': 1.1998, 'grad_norm': 1.3452961444854736, 'learning_rate': 0.00011924839596700275, 'epoch': 0.8444444444444444}
Step 960: {'loss': 1.141, 'grad_norm': 0.6617207527160645, 'learning_rate': 0.00011833180568285977, 'epoch': 0.8533333333333334}
Step 970: {'loss': 1.1745, 'grad_norm': 0.7093111872673035, 'learning_rate': 0.00011741521539871678, 'epoch': 0.8622222222222222}
Step 980: {'loss': 1.2365, 'grad_norm': 0.9119622707366943, 'learning_rate': 0.00011649862511457379, 'epoch': 0.8711111111111111}
Step 990: {'loss': 1.2739, 'grad_norm': 0.8298653364181519, 'learning_rate': 0.0001155820348304308, 'epoch': 0.88}
Step 1000: {'loss': 1.2079, 'grad_norm': 0.7153652906417847, 'learning_rate': 0.00011466544454628783, 'epoch': 0.8888888888888888}
Step 1010: {'loss': 1.1366, 'grad_norm': 1.757135033607483, 'learning_rate': 0.00011374885426214484, 'epoch': 0.8977777777777778}
Step 1020: {'loss': 1.1602, 'grad_norm': 0.9258670806884766, 'learning_rate': 0.00011283226397800184, 'epoch': 0.9066666666666666}
Step 1030: {'loss': 1.1596, 'grad_norm': 0.8334272503852844, 'learning_rate': 0.00011191567369385885, 'epoch': 0.9155555555555556}
Step 1040: {'loss': 1.1691, 'grad_norm': 0.8081274032592773, 'learning_rate': 0.00011099908340971586, 'epoch': 0.9244444444444444}
Step 1050: {'loss': 1.1573, 'grad_norm': 0.9281267523765564, 'learning_rate': 0.00011008249312557288, 'epoch': 0.9333333333333333}
Step 1060: {'loss': 1.2139, 'grad_norm': 0.8042176961898804, 'learning_rate': 0.00010916590284142989, 'epoch': 0.9422222222222222}
Step 1070: {'loss': 1.2605, 'grad_norm': 0.9414412975311279, 'learning_rate': 0.0001082493125572869, 'epoch': 0.9511111111111111}
Step 1080: {'loss': 1.1154, 'grad_norm': 0.8115457892417908, 'learning_rate': 0.0001073327222731439, 'epoch': 0.96}
Step 1090: {'loss': 1.184, 'grad_norm': 0.9144962430000305, 'learning_rate': 0.00010641613198900091, 'epoch': 0.9688888888888889}
Step 1100: {'loss': 1.2225, 'grad_norm': 2.3157191276550293, 'learning_rate': 0.00010549954170485795, 'epoch': 0.9777777777777777}
Step 1110: {'loss': 1.1019, 'grad_norm': 0.7295096516609192, 'learning_rate': 0.00010458295142071495, 'epoch': 0.9866666666666667}
Step 1120: {'loss': 1.2923, 'grad_norm': 0.7700325846672058, 'learning_rate': 0.00010366636113657196, 'epoch': 0.9955555555555555}
Epoch 1.0 completed in 7055.56s
Step 1130: {'loss': 1.236, 'grad_norm': 0.8807558417320251, 'learning_rate': 0.00010274977085242896, 'epoch': 1.0044444444444445}
Step 1140: {'loss': 1.0956, 'grad_norm': 0.9239604473114014, 'learning_rate': 0.00010183318056828599, 'epoch': 1.0133333333333334}
Step 1150: {'loss': 1.1328, 'grad_norm': 1.0520743131637573, 'learning_rate': 0.000100916590284143, 'epoch': 1.0222222222222221}
Step 1160: {'loss': 1.2123, 'grad_norm': 0.9294842481613159, 'learning_rate': 0.0001, 'epoch': 1.031111111111111}
Step 1170: {'loss': 1.1984, 'grad_norm': 0.8388755321502686, 'learning_rate': 9.908340971585701e-05, 'epoch': 1.04}
Step 1180: {'loss': 1.1684, 'grad_norm': 0.8190751075744629, 'learning_rate': 9.816681943171403e-05, 'epoch': 1.048888888888889}
Step 1190: {'loss': 1.3314, 'grad_norm': 0.8809822201728821, 'learning_rate': 9.725022914757104e-05, 'epoch': 1.0577777777777777}
Step 1200: {'loss': 1.1231, 'grad_norm': 0.917870283126831, 'learning_rate': 9.633363886342805e-05, 'epoch': 1.0666666666666667}
Step 1210: {'loss': 1.173, 'grad_norm': 1.2823466062545776, 'learning_rate': 9.541704857928506e-05, 'epoch': 1.0755555555555556}
Step 1220: {'loss': 1.0278, 'grad_norm': 0.8130680322647095, 'learning_rate': 9.450045829514208e-05, 'epoch': 1.0844444444444445}
Step 1230: {'loss': 1.1268, 'grad_norm': 0.8831413388252258, 'learning_rate': 9.358386801099908e-05, 'epoch': 1.0933333333333333}
Step 1240: {'loss': 1.0809, 'grad_norm': 0.610246479511261, 'learning_rate': 9.266727772685609e-05, 'epoch': 1.1022222222222222}
Step 1250: {'loss': 1.2227, 'grad_norm': 1.058553695678711, 'learning_rate': 9.175068744271311e-05, 'epoch': 1.1111111111111112}
Step 1260: {'loss': 1.1416, 'grad_norm': 0.9073207974433899, 'learning_rate': 9.083409715857012e-05, 'epoch': 1.12}
Step 1270: {'loss': 1.1142, 'grad_norm': 0.8205253481864929, 'learning_rate': 8.991750687442714e-05, 'epoch': 1.1288888888888888}
Step 1280: {'loss': 1.1148, 'grad_norm': 0.7865174412727356, 'learning_rate': 8.900091659028415e-05, 'epoch': 1.1377777777777778}
Step 1290: {'loss': 1.093, 'grad_norm': 0.6838135719299316, 'learning_rate': 8.808432630614116e-05, 'epoch': 1.1466666666666667}
Step 1300: {'loss': 1.1318, 'grad_norm': 0.8905860781669617, 'learning_rate': 8.716773602199816e-05, 'epoch': 1.1555555555555554}
Step 1310: {'loss': 1.0825, 'grad_norm': 0.6757611632347107, 'learning_rate': 8.625114573785519e-05, 'epoch': 1.1644444444444444}
Step 1320: {'loss': 1.0708, 'grad_norm': 0.6161576509475708, 'learning_rate': 8.53345554537122e-05, 'epoch': 1.1733333333333333}
Step 1330: {'loss': 1.224, 'grad_norm': 0.6106223464012146, 'learning_rate': 8.44179651695692e-05, 'epoch': 1.1822222222222223}
Step 1340: {'loss': 1.1967, 'grad_norm': 0.7142434120178223, 'learning_rate': 8.350137488542622e-05, 'epoch': 1.1911111111111112}
Step 1350: {'loss': 1.2255, 'grad_norm': 1.0701624155044556, 'learning_rate': 8.258478460128323e-05, 'epoch': 1.2}
Step 1360: {'loss': 1.2011, 'grad_norm': 0.8210263848304749, 'learning_rate': 8.166819431714025e-05, 'epoch': 1.208888888888889}
Step 1370: {'loss': 1.1229, 'grad_norm': 0.8007976412773132, 'learning_rate': 8.075160403299726e-05, 'epoch': 1.2177777777777778}
Step 1380: {'loss': 1.2645, 'grad_norm': 0.8887707591056824, 'learning_rate': 7.983501374885427e-05, 'epoch': 1.2266666666666666}
Step 1390: {'loss': 1.183, 'grad_norm': 0.766533374786377, 'learning_rate': 7.891842346471127e-05, 'epoch': 1.2355555555555555}
Step 1400: {'loss': 1.162, 'grad_norm': 0.9055758118629456, 'learning_rate': 7.800183318056828e-05, 'epoch': 1.2444444444444445}
Step 1410: {'loss': 1.1249, 'grad_norm': 0.7075912356376648, 'learning_rate': 7.70852428964253e-05, 'epoch': 1.2533333333333334}
Step 1420: {'loss': 1.1319, 'grad_norm': 0.7523531317710876, 'learning_rate': 7.616865261228231e-05, 'epoch': 1.2622222222222224}
Step 1430: {'loss': 1.2261, 'grad_norm': 0.7515262961387634, 'learning_rate': 7.525206232813933e-05, 'epoch': 1.271111111111111}
Step 1440: {'loss': 1.1566, 'grad_norm': 0.7296952605247498, 'learning_rate': 7.433547204399634e-05, 'epoch': 1.28}
Step 1450: {'loss': 1.2527, 'grad_norm': 0.6850522756576538, 'learning_rate': 7.341888175985336e-05, 'epoch': 1.2888888888888888}
Step 1460: {'loss': 1.2194, 'grad_norm': 0.898430347442627, 'learning_rate': 7.250229147571035e-05, 'epoch': 1.2977777777777777}
Step 1470: {'loss': 1.21, 'grad_norm': 0.7777708172798157, 'learning_rate': 7.158570119156737e-05, 'epoch': 1.3066666666666666}
Step 1480: {'loss': 1.0537, 'grad_norm': 0.5944450497627258, 'learning_rate': 7.066911090742438e-05, 'epoch': 1.3155555555555556}
Step 1490: {'loss': 1.2058, 'grad_norm': 0.8746438026428223, 'learning_rate': 6.975252062328139e-05, 'epoch': 1.3244444444444445}
Step 1500: {'loss': 1.0762, 'grad_norm': 0.7092800140380859, 'learning_rate': 6.883593033913841e-05, 'epoch': 1.3333333333333333}
Step 1510: {'loss': 1.1326, 'grad_norm': 0.6582843065261841, 'learning_rate': 6.791934005499542e-05, 'epoch': 1.3422222222222222}
Step 1520: {'loss': 1.1564, 'grad_norm': 0.7300131916999817, 'learning_rate': 6.700274977085244e-05, 'epoch': 1.3511111111111112}
Step 1530: {'loss': 1.1286, 'grad_norm': 0.9033456444740295, 'learning_rate': 6.608615948670945e-05, 'epoch': 1.3599999999999999}
Step 1540: {'loss': 1.1431, 'grad_norm': 0.6617822647094727, 'learning_rate': 6.516956920256645e-05, 'epoch': 1.3688888888888888}
Step 1550: {'loss': 1.1889, 'grad_norm': 0.8385574221611023, 'learning_rate': 6.425297891842346e-05, 'epoch': 1.3777777777777778}
Step 1560: {'loss': 1.0957, 'grad_norm': 0.6781438589096069, 'learning_rate': 6.333638863428047e-05, 'epoch': 1.3866666666666667}
Step 1570: {'loss': 1.0891, 'grad_norm': 0.7512512803077698, 'learning_rate': 6.241979835013749e-05, 'epoch': 1.3955555555555557}
Step 1580: {'loss': 1.2194, 'grad_norm': 0.7469757795333862, 'learning_rate': 6.15032080659945e-05, 'epoch': 1.4044444444444444}
Step 1590: {'loss': 1.3287, 'grad_norm': 0.7735984921455383, 'learning_rate': 6.058661778185152e-05, 'epoch': 1.4133333333333333}
Step 1600: {'loss': 1.0773, 'grad_norm': 0.6726782321929932, 'learning_rate': 5.967002749770853e-05, 'epoch': 1.4222222222222223}
Step 1610: {'loss': 1.1434, 'grad_norm': 0.5827363133430481, 'learning_rate': 5.875343721356554e-05, 'epoch': 1.431111111111111}
Step 1620: {'loss': 1.2274, 'grad_norm': 0.773137092590332, 'learning_rate': 5.783684692942255e-05, 'epoch': 1.44}
Step 1630: {'loss': 1.0254, 'grad_norm': 0.8423247933387756, 'learning_rate': 5.692025664527957e-05, 'epoch': 1.448888888888889}
Step 1640: {'loss': 1.2022, 'grad_norm': 0.5879942774772644, 'learning_rate': 5.600366636113658e-05, 'epoch': 1.4577777777777778}
Step 1650: {'loss': 1.1105, 'grad_norm': 0.6884462237358093, 'learning_rate': 5.5087076076993585e-05, 'epoch': 1.4666666666666668}
Step 1660: {'loss': 1.1549, 'grad_norm': 0.6782166957855225, 'learning_rate': 5.41704857928506e-05, 'epoch': 1.4755555555555555}
Step 1670: {'loss': 1.1715, 'grad_norm': 0.7188833951950073, 'learning_rate': 5.325389550870761e-05, 'epoch': 1.4844444444444445}
Step 1680: {'loss': 1.1676, 'grad_norm': 1.1101254224777222, 'learning_rate': 5.233730522456463e-05, 'epoch': 1.4933333333333334}
Step 1690: {'loss': 1.2287, 'grad_norm': 0.7188689708709717, 'learning_rate': 5.1420714940421635e-05, 'epoch': 1.5022222222222221}
Step 1700: {'loss': 1.1302, 'grad_norm': 0.8157673478126526, 'learning_rate': 5.050412465627865e-05, 'epoch': 1.511111111111111}
Step 1710: {'loss': 1.1568, 'grad_norm': 0.8126543164253235, 'learning_rate': 4.958753437213566e-05, 'epoch': 1.52}
Step 1720: {'loss': 1.2727, 'grad_norm': 0.7557060122489929, 'learning_rate': 4.867094408799267e-05, 'epoch': 1.528888888888889}
Step 1730: {'loss': 1.1559, 'grad_norm': 0.7435368895530701, 'learning_rate': 4.7754353803849686e-05, 'epoch': 1.537777777777778}
Step 1740: {'loss': 1.2643, 'grad_norm': 1.877492904663086, 'learning_rate': 4.683776351970669e-05, 'epoch': 1.5466666666666666}
Step 1750: {'loss': 1.0951, 'grad_norm': 0.7865771651268005, 'learning_rate': 4.59211732355637e-05, 'epoch': 1.5555555555555556}
Step 1760: {'loss': 1.1694, 'grad_norm': 0.6105818748474121, 'learning_rate': 4.5004582951420715e-05, 'epoch': 1.5644444444444443}
Step 1770: {'loss': 1.2165, 'grad_norm': 0.8988511562347412, 'learning_rate': 4.408799266727773e-05, 'epoch': 1.5733333333333333}
Step 1780: {'loss': 1.1368, 'grad_norm': 0.653029203414917, 'learning_rate': 4.317140238313474e-05, 'epoch': 1.5822222222222222}
Step 1790: {'loss': 1.1788, 'grad_norm': 0.8373101353645325, 'learning_rate': 4.225481209899175e-05, 'epoch': 1.5911111111111111}
Step 1800: {'loss': 1.2082, 'grad_norm': 0.7034522294998169, 'learning_rate': 4.1338221814848766e-05, 'epoch': 1.6}
Step 1810: {'loss': 1.1412, 'grad_norm': 0.7448225021362305, 'learning_rate': 4.042163153070578e-05, 'epoch': 1.608888888888889}
Step 1820: {'loss': 1.2006, 'grad_norm': 0.9257205724716187, 'learning_rate': 3.950504124656279e-05, 'epoch': 1.6177777777777778}
Step 1830: {'loss': 1.0912, 'grad_norm': 0.5780017971992493, 'learning_rate': 3.8588450962419795e-05, 'epoch': 1.6266666666666667}
Step 1840: {'loss': 1.0488, 'grad_norm': 0.7708772420883179, 'learning_rate': 3.767186067827681e-05, 'epoch': 1.6355555555555554}
Step 1850: {'loss': 1.2282, 'grad_norm': 0.8107613325119019, 'learning_rate': 3.6755270394133824e-05, 'epoch': 1.6444444444444444}
Step 1860: {'loss': 1.1825, 'grad_norm': 0.9876782894134521, 'learning_rate': 3.583868010999084e-05, 'epoch': 1.6533333333333333}
Step 1870: {'loss': 1.2004, 'grad_norm': 0.7162197828292847, 'learning_rate': 3.4922089825847846e-05, 'epoch': 1.6622222222222223}
Step 1880: {'loss': 1.363, 'grad_norm': 0.6540087461471558, 'learning_rate': 3.400549954170486e-05, 'epoch': 1.6711111111111112}
Step 1890: {'loss': 1.1839, 'grad_norm': 0.6041759848594666, 'learning_rate': 3.3088909257561874e-05, 'epoch': 1.6800000000000002}
Step 1900: {'loss': 1.1341, 'grad_norm': 0.6254270076751709, 'learning_rate': 3.217231897341889e-05, 'epoch': 1.6888888888888889}
Step 1910: {'loss': 1.1357, 'grad_norm': 0.6187231540679932, 'learning_rate': 3.125572868927589e-05, 'epoch': 1.6977777777777778}
Step 1920: {'loss': 1.1217, 'grad_norm': 0.6950653195381165, 'learning_rate': 3.0339138405132904e-05, 'epoch': 1.7066666666666666}
Step 1930: {'loss': 1.1365, 'grad_norm': 0.8051183819770813, 'learning_rate': 2.9422548120989918e-05, 'epoch': 1.7155555555555555}
Step 1940: {'loss': 1.1087, 'grad_norm': 0.6369321942329407, 'learning_rate': 2.850595783684693e-05, 'epoch': 1.7244444444444444}
Step 1950: {'loss': 1.18, 'grad_norm': 0.7104555368423462, 'learning_rate': 2.7589367552703943e-05, 'epoch': 1.7333333333333334}
Step 1960: {'loss': 1.2487, 'grad_norm': 0.7030423283576965, 'learning_rate': 2.6672777268560954e-05, 'epoch': 1.7422222222222223}
Step 1970: {'loss': 1.0758, 'grad_norm': 0.6101465225219727, 'learning_rate': 2.575618698441797e-05, 'epoch': 1.751111111111111}
Step 1980: {'loss': 1.1214, 'grad_norm': 0.6713480353355408, 'learning_rate': 2.483959670027498e-05, 'epoch': 1.76}
Step 1990: {'loss': 1.1399, 'grad_norm': 0.7257075309753418, 'learning_rate': 2.392300641613199e-05, 'epoch': 1.7688888888888887}
Step 2000: {'loss': 1.2577, 'grad_norm': 0.8914834260940552, 'learning_rate': 2.3006416131989005e-05, 'epoch': 1.7777777777777777}
Step 2010: {'loss': 1.2083, 'grad_norm': 0.5450852513313293, 'learning_rate': 2.2089825847846012e-05, 'epoch': 1.7866666666666666}
Step 2020: {'loss': 1.143, 'grad_norm': 0.90396648645401, 'learning_rate': 2.1173235563703027e-05, 'epoch': 1.7955555555555556}
Step 2030: {'loss': 1.1602, 'grad_norm': 0.6795806288719177, 'learning_rate': 2.0256645279560038e-05, 'epoch': 1.8044444444444445}
Step 2040: {'loss': 1.1157, 'grad_norm': 0.7854484915733337, 'learning_rate': 1.9340054995417052e-05, 'epoch': 1.8133333333333335}
Step 2050: {'loss': 1.1505, 'grad_norm': 1.040418028831482, 'learning_rate': 1.842346471127406e-05, 'epoch': 1.8222222222222222}
Step 2060: {'loss': 1.0934, 'grad_norm': 0.6334657669067383, 'learning_rate': 1.7506874427131074e-05, 'epoch': 1.8311111111111111}
Step 2070: {'loss': 1.1144, 'grad_norm': 0.5647825598716736, 'learning_rate': 1.6590284142988085e-05, 'epoch': 1.8399999999999999}
Step 2080: {'loss': 1.1249, 'grad_norm': 0.6648344397544861, 'learning_rate': 1.56736938588451e-05, 'epoch': 1.8488888888888888}
Step 2090: {'loss': 1.0932, 'grad_norm': 0.9362894296646118, 'learning_rate': 1.4757103574702108e-05, 'epoch': 1.8577777777777778}
Step 2100: {'loss': 1.0406, 'grad_norm': 0.7845157384872437, 'learning_rate': 1.3840513290559121e-05, 'epoch': 1.8666666666666667}
Step 2110: {'loss': 1.0901, 'grad_norm': 1.0069456100463867, 'learning_rate': 1.2923923006416134e-05, 'epoch': 1.8755555555555556}
Step 2120: {'loss': 1.1505, 'grad_norm': 0.7095833420753479, 'learning_rate': 1.2007332722273145e-05, 'epoch': 1.8844444444444446}
Step 2130: {'loss': 1.1268, 'grad_norm': 0.7102500796318054, 'learning_rate': 1.1090742438130157e-05, 'epoch': 1.8933333333333333}
Step 2140: {'loss': 1.1638, 'grad_norm': 0.818331778049469, 'learning_rate': 1.0174152153987168e-05, 'epoch': 1.9022222222222223}
Step 2150: {'loss': 1.1231, 'grad_norm': 0.6782492995262146, 'learning_rate': 9.25756186984418e-06, 'epoch': 1.911111111111111}
Step 2160: {'loss': 1.2062, 'grad_norm': 0.8464624881744385, 'learning_rate': 8.340971585701192e-06, 'epoch': 1.92}
Step 2170: {'loss': 1.2481, 'grad_norm': 0.6784800887107849, 'learning_rate': 7.424381301558204e-06, 'epoch': 1.9288888888888889}
Step 2180: {'loss': 1.1225, 'grad_norm': 0.6876100897789001, 'learning_rate': 6.507791017415216e-06, 'epoch': 1.9377777777777778}
Step 2190: {'loss': 1.1305, 'grad_norm': 0.6190866827964783, 'learning_rate': 5.591200733272228e-06, 'epoch': 1.9466666666666668}
Step 2200: {'loss': 1.0213, 'grad_norm': 0.7760927081108093, 'learning_rate': 4.67461044912924e-06, 'epoch': 1.9555555555555557}
Step 2210: {'loss': 1.1837, 'grad_norm': 0.9082736372947693, 'learning_rate': 3.7580201649862514e-06, 'epoch': 1.9644444444444444}
Step 2220: {'loss': 1.2551, 'grad_norm': 0.779634416103363, 'learning_rate': 2.8414298808432632e-06, 'epoch': 1.9733333333333334}
Step 2230: {'loss': 1.2109, 'grad_norm': 0.6780804991722107, 'learning_rate': 1.924839596700275e-06, 'epoch': 1.982222222222222}
Step 2240: {'loss': 1.1215, 'grad_norm': 1.683760166168213, 'learning_rate': 1.008249312557287e-06, 'epoch': 1.991111111111111}
Step 2250: {'loss': 1.1004, 'grad_norm': 0.7466273903846741, 'learning_rate': 9.165902841429882e-08, 'epoch': 2.0}
Epoch 2.0 completed in 14148.87s
Step 2250: {'train_runtime': 14149.3288, 'train_samples_per_second': 0.636, 'train_steps_per_second': 0.159, 'total_flos': 4238947123200000.0, 'train_loss': 1.3211068869696723, 'epoch': 2.0}
=== Training completed in 14149.33s ===

